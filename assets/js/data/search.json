[ { "title": "Spring Boot自动装配原理", "url": "/posts/AutomaticAssembly/", "categories": "技术科普", "tags": "学习", "date": "2024-01-14 12:17:00 +0000", "snippet": "Spring Boot自动装配原理要理解自动装配，主要是要理解透彻两个配置文件的作用，一个是application.yml，一个是spring.factories。下文，让我们具体来讲讲！Bean自动装载配置的核心问题Spring Boot里面的各种Bean(类对象)能够实现自动装载，自动的装载帮我们减少了XML的配置，和手动编码进行Bean的加载工作。从而极大程度上帮我们减少了配置量和代码...", "content": "Spring Boot自动装配原理要理解自动装配，主要是要理解透彻两个配置文件的作用，一个是application.yml，一个是spring.factories。下文，让我们具体来讲讲！Bean自动装载配置的核心问题Spring Boot里面的各种Bean(类对象)能够实现自动装载，自动的装载帮我们减少了XML的配置，和手动编码进行Bean的加载工作。从而极大程度上帮我们减少了配置量和代码量。要实现Bean的自动装载，需要解决两个问题： 如何保证Bean自动装载的灵活性？这个问题通过配置文件来解决，在配置A情况下去装载BeanY；在配置B情况下去装载BeanZ。（通常情况下配置A和B会有默认值，来决定默认的装载行为，这样就不需要我们配置了，进一步减少配置量） 如何保证Bean装载的顺序性？当BeanA装载完成之后再去装载BeanY，BeanY装载完成之后才去装载BeanX。这个装载顺序问题由@ConditionOnXXXXXXX注解来解决。全局配置文件SpringBoot使用一个全局的配置文件，配置文件名是固定的； application.properties application.yml全局配置文件的作用：修改SpringBoot自动配置的默认值，通过配置来影响SpringBoot自动装载行为。（很关键，需要与下文中的spring.factories配置文件进行区分，只有在yml文件中配置了某个依赖，springboot才会去自动装载相应的配置类）配置装载原理源码解析所有的Spring Boot应用程序都是以SpringApplication.run()作为应用程序入口的。下面我们来一步一步跟踪一下这个函数。run方法传入了SpringApplication对象和一些运行期参数。继续向前跟进，我们发现一个类叫做SpringFactoriesLoader，这里面体现了Spring Boot加载配置文件的核心逻辑。从上图可以看到： 从META-INF/spring.factories文件夹下加载了spring.factories文件资源 然后读取文件中的ClassName作为值放入Properties然后通过反射机制，对spring.factories里面的类资源进行实例化，所以spring.factories文件里面究竟写了什么类？这些类是做什么的？就是我们下一步需要探究的问题了。这张图非常形象，把整个装配的流程都画出来了：等看完接下来细节的讲解，再来看这张图可以说是醍醐灌顶。@EnableAutoConfiguration 作用SpringBoot入口启动类使用了SpringBootApplication，实际上就是开启了自动配置功能@EnableAutoConfiguration。可以把 @SpringBootApplication看作是 @Configuration、@EnableAutoConfiguration、@ComponentScan 注解的集合。根据 SpringBoot 官网，这三个注解的作用分别是： @EnableAutoConfiguration：启用 SpringBoot 的自动配置机制 @Configuration：允许在上下文中注册额外的 bean 或导入其他配置类 @ComponentScan：扫描被@Component (@Service,@Controller)注解的 bean，注解默认会扫描启动类所在的包下所有的类 ，可以自定义不扫描某些 bean。如下图所示，容器中将排除TypeExcludeFilter和AutoConfigurationExcludeFilter。@EnableAutoConfiguration 是实现自动装配的重要注解，我们以这个注解入手。@Target({ElementType.TYPE})@Retention(RetentionPolicy.RUNTIME)@Documented@Inherited@AutoConfigurationPackage //作用：将main包下的所有组件注册到容器中@Import({AutoConfigurationImportSelector.class}) //加载自动装配类 xxxAutoconfigurationpublic @interface EnableAutoConfiguration {String ENABLED_OVERRIDE_PROPERTY = \"spring.boot.enableautoconfiguration\";Class&lt;?&gt;[] exclude() default {};String[] excludeName() default {};}我们现在重点分析下AutoConfigurationImportSelector 类到底做了什么？AutoConfigurationImportSelector:加载自动装配类AutoConfigurationImportSelector类的继承体系如下：public class AutoConfigurationImportSelector implements DeferredImportSelector, BeanClassLoaderAware, ResourceLoaderAware, BeanFactoryAware, EnvironmentAware, Ordered {}public interface DeferredImportSelector extends ImportSelector {}public interface ImportSelector {String[] selectImports(AnnotationMetadata var1);}可以看出，AutoConfigurationImportSelector 类实现了 ImportSelector接口，也就实现了这个接口中的 selectImports方法，该方法主要用于获取所有符合条件的类的全限定类名，这些类需要被加载到 IoC 容器中。这里我们需要重点关注一下getAutoConfigurationEntry()方法，这个方法主要负责加载自动配置类的。调用图：现在我们结合getAutoConfigurationEntry()的源码来详细分析一下： 判断自动装配开关是否打开。默认spring.boot.enableautoconfiguration=true，可在 application.properties 或 application.yml 中设置 获取EnableAutoConfiguration注解中的 exclude 和 excludeName 获取需要自动装配的所有配置类，读取META-INF/spring.factoriesspring-boot/spring-boot-project/spring-boot-autoconfigure/src/main/resources/META-INF/spring.factories从下图可以看到这个文件的配置内容都被我们读取到了。XXXAutoConfiguration的作用就是按需加载组件。所有 Spring Boot Starter 下的META-INF/spring.factories都会被读取到。如果，我们自己要创建一个 Spring Boot Starter，这一步是必不可少的。spring.factories中这么多配置，每次启动都要全部加载么?我们 debug 到后面你会发现，configurations 的值变小了:因为，这一步有经历了一遍筛选，@ConditionalOnXXX 中的所有条件都满足，该类才会生效@Configuration// 检查相关的类：RabbitTemplate 和 Channel是否存在// 存在才会加载@ConditionalOnClass({ RabbitTemplate.class, Channel.class })@EnableConfigurationProperties(RabbitProperties.class)@Import(RabbitAnnotationDrivenConfiguration.class)public class RabbitAutoConfiguration {}//具体的装配检查注解，可以看下方的文档。简单来说：SpringFactoriesLoader会以@EnableAutoConfiguration的包名和类名org.springframework.boot.autoconfigure.EnableAutoConfiguration为Key查找spring.factories文件,并将value中的类名实例化加载到Spring Boot应用中。spring.factories文件中的每一行都是一个自动装配类。Spring boot的自动装配实现原理简述如果上述原理细节无法理解，面试只需要掌握以下这种程度！自动装配可以简单理解为：通过注解或者一些简单的配置就能在 Spring Boot 的帮助下实现某块功能。Springboot 所有自动配置都是在启动时候被扫描并加载：所有的自动配置类都在 spring.factories 里面，但是不一定生效，需要判断条件是否成立，只要导入了对应的starter，就会有对应的启动器，有了启动器，我们的自动装配就会生效，然后就配置成功！补充：Starter是什么？Spring Boot Starter的中文名为”启动器“。“Starter”是Spring Boot中的一个概念，它是一种特殊的依赖包，旨在简化和快速配置特定功能的应用程序。Starter包含了一组预配置的依赖项，使得我们可以很方便地引入和使用某个特定功能或技术栈。Spring Boot的Starter遵循一种命名约定，通常以spring-boot-starter-*的形式命名，例如： spring-boot-starter-web：用于构建Web应用程序的启动器，包含了常用的Web开发所需的依赖项，如Spring MVC、Tomcat等。 spring-boot-starter-data-jpa：用于支持使用JPA进行数据库访问的启动器，包含了JPA、Hibernate、数据库驱动等依赖项。 spring-boot-starter-test：用于编写测试的启动器，包含了JUnit、Mockito等测试框架的依赖项。" }, { "title": "详解 TCP 和 UDP 的区别", "url": "/posts/TCP&UDP/", "categories": "计算机网络", "tags": "学习", "date": "2024-01-14 12:17:00 +0000", "snippet": "详解 TCP 和 UDP 的区别TCP 和 UDP 的区别一直是面试的重点，也是经常被用来拿来各种比较的两个协议。建立连接的差异TCP 建立连接需要经过三次握手，同时 TCP 断开连接需要经过四次挥手，这也表示 TCP 是一种面向连接的协议，这个连接不是用一条网线或者一个管道把两个通信双方绑在一起，而是建立一条虚拟通信管道。TCP 的三次握手流程（客户端向服务器发送建立连接请求）： 服务端...", "content": "详解 TCP 和 UDP 的区别TCP 和 UDP 的区别一直是面试的重点，也是经常被用来拿来各种比较的两个协议。建立连接的差异TCP 建立连接需要经过三次握手，同时 TCP 断开连接需要经过四次挥手，这也表示 TCP 是一种面向连接的协议，这个连接不是用一条网线或者一个管道把两个通信双方绑在一起，而是建立一条虚拟通信管道。TCP 的三次握手流程（客户端向服务器发送建立连接请求）： 服务端进程准备好接收来自外部的 TCP 连接，一般情况下是调用 bind、listen、socket 三个函数完成。这种打开方式被认为是 被动打开(passive open)。然后服务端进程处于 LISTEN 状态，等待客户端连接请求。 客户端通过 connect 发起主动打开(active open)，向服务器发出连接请求，请求中首部同步位 SYN = 1，同时选择一个初始序号 sequence ，简写 seq = x。SYN 报文段不允许携带数据，只消耗一个序号。此时，客户端进入 SYN-SEND 状态。 服务器收到客户端连接后，需要确认客户端的报文段。在确认报文段中，把 SYN 和 ACK 位都置为 1 。确认号是 ack = x + 1，同时也为自己选择一个初始序号 seq = y。请注意，这个报文段也不能携带数据，但同样要消耗掉一个序号。此时，TCP 服务器进入 SYN-RECEIVED(同步收到) 状态。 客户端在收到服务器发出的响应后，需要给出确认连接。确认连接中的 ACK 置为 1 ，序号为 seq = x + 1，确认号为 ack = y + 1。TCP 规定，这个报文段可以携带数据也可以不携带数据，如果不携带数据，那么下一个数据报文段的序号仍是 seq = x + 1。这时，客户端进入 ESTABLISHED (已连接) 状态。 服务器收到客户的确认后，也进入 ESTABLISHED 状态。而 UDP 是面向数据报的协议，所以 UDP 压根不会有连接的概念，也就不会有三次握手建立连接的过程。数据传输结束后，通信双方可以释放连接。数据传输结束后的客户端主机和服务端主机都处于 ESTABLISHED 状态，然后进入释放连接的过程。（客户端主机主动关闭连接）TCP 断开连接需要历经的过程如下 客户端应用程序发出释放连接的报文段，并停止发送数据，主动关闭 TCP 连接。客户端主机发送释放连接的报文段，报文段中首部 FIN 位置为 1 ，不包含数据，序列号位 seq = u，此时客户端主机进入 FIN-WAIT-1(终止等待 1) 阶段。 服务器主机接受到客户端发出的报文段后，即发出确认应答报文，确认应答报文中 ACK = 1，生成自己的序号位 seq = v，ack = u + 1，然后服务器主机就进入 CLOSE-WAIT(关闭等待) 状态，这个时候客户端主机 -&gt; 服务器主机这条方向的连接就释放了，客户端主机没有数据需要发送，此时服务器主机是一种半连接的状态，但是服务器主机仍然可以发送数据。 客户端主机收到服务端主机的确认应答后，即进入 FIN-WAIT-2(终止等待2) 的状态。等待客户端发出连接释放的报文段。 当服务器主机没有数据发送后，应用进程就会通知 TCP 释放连接。这时服务端主机会发出断开连接的报文段，报文段中 ACK = 1，序列号 seq = w，因为在这之间可能已经发送了一些数据，所以 seq 不一定等于 v + 1。ack = u + 1，在发送完断开请求的报文后，服务端主机就进入了 LAST-ACK(最后确认)的阶段。 客户端收到服务端的断开连接请求后，客户端需要作出响应，客户端发出断开连接的报文段，在报文段中，ACK = 1, 序列号 seq = u + 1，因为客户端从连接开始断开后就没有再发送数据，ack = w + 1，然后进入到 TIME-WAIT(时间等待) 状态，请注意，这个时候 TCP 连接还没有释放。必须经过时间等待的设置，也就是 2MSL 后，客户端才会进入 CLOSED 状态，时间 MSL 叫做最长报文段寿命（Maximum Segment Lifetime）。 服务端主要收到了客户端的断开连接确认后，就会进入 CLOSED 状态。因为服务端结束 TCP 连接时间要比客户端早，而整个连接断开过程需要发送四个报文段，因此释放连接的过程也被称为四次挥手。UDP 不存在这条连接，所以它也不需要四次挥手操作。所以总结一点：TCP 是面向连接的，它的数据传输前需要维护一条虚拟连接，数据传输需要在这条虚拟连接上进行，数据传输完毕后需要断开这条连接，而 UDP 传输不是面向连接的，UDP 发送数据不会建立连接，也不会关心接收端的状态。可靠性的差异TCP 和 UDP 一个主要拿来作对比的就是可靠性，TCP 是一种可靠性的传输层协议，UDP 是一种不可靠的传输层协议。TCP 的这种可靠性主要由下面这些特征来保证：通过序列号和应答号实现可靠性计算机网络主机之间的相互通信非常类似于我们日常生活中两个人之间打电话，这种对话通常是一问一答形式，如果你讲了一句话并没有收到任何回应，你通常需要再说一次来确保对方是否听到，如果对方给你回应了一句话，就说明他已经听到你的讲话了，这就是一个完整的通话流程（抛开建立连接不谈，我们着重点放在建立连接之后）。“对方给你的响应” 在计算机网络中被称为确认应答(ACK)，TCP 就是通过 ACK 来实现可靠的数据传输，也就是说，发送方在发出请求之后会等待目标主机的响应，如果没有收到响应，发送方在经过一段时间后就会重传请求。所以，即使在发送过程中产生丢包，TCP 仍然能够通过重传来实现可靠性。上面描述的情况属于发送方请求丢失，还有一种情况属于响应丢失，也就是说请求发送到目标主机后，目标主机会回发 ACK 给请求方，这个 ACK 也有可能丢失，如果 ACK 在链路中丢失，一段时间后请求方没有收到目标主机的 ACK ，仍然会选择重传未收到 ACK 的这个请求。除了消息丢失之外，还存在一种延迟到达的现象，延迟到达指的是发送方发送一个报文段之后，这个报文也许是由于网络抖动或者网络拥堵导致一个报文段迟迟没有到达目标主机，或者目标主机的响应 ACK 迟迟没有到达发送方的现象。这个一段时间判断的标准就是重传时间，一旦过了重传时间发送方会重传报文段，很可能存在重传报文段到达之后，第一次发送的报文段才刚到的情况，这就存在一个问题：目标主机收到了两个相同的报文段。必须选择一个报文段进行丢弃，但是应该选择哪个报文段呢？可以通过序列号（seq）来实现，序列号是按照顺序给发送数据的每一个字节都标上号码的编号。接收端通过查询 TCP 首部中的序列号和数据的长度，将自己下一步应该接收的序列号作为确认应答返送回去。通过序列号和确认应答号，TCP 能够识别是否已经接收数据，又能够判断是否需要接收，从而实现可靠传输。如上图所示，请求按照顺序发送的话是 seq = 1 ，这个请求会把第 1 字节到第 n 字节的数据一起发送过去，等待目标主机一次确认每个字节后，再发送 seq = n + 1 的请求，确认完成后再发送 seq = m + 1 的请求，这样能够保证序列号不会重复。UDP 没有所谓的序列号和确认号，所以不会对数据进行确认，数据丢失后也不会进行重传，所以 UDP 是一种不可靠的协议。如果使用 TCP 和 UDP 来比喻开发人员：TCP 就是那种凡事都要设计好，没设计不会进行开发的工程师，需要把一切因素考虑在内后再开干！所以非常靠谱；而 UDP 就是那种上来直接干干干，接到项目需求马上就开干，也不管设计，也不管技术选型，就是干，这种开发人员非常不靠谱，但是适合快速迭代开发，因为可以马上上手！有序性差异我们上面说到，TCP 会对请求分开发送，每次请求所携带的数据都会被目标主机进行确认，目标主机依次确认每个请求后，就会对请求中的数据进行重组，由于请求是由 seq 的，所以 TCP 在重组这些数据时，也会按照顺序进行重组，而 UDP 没有有序性的这种保证。报文段的差异TCP 和 UDP 同属于传输层协议，传输层协议传输的数据统称为报文段，TCP 和 UDP 的报文段的主要差异如下。UDP 报文段结构 端口号(Source Port) :这个字段占据 UDP 报文头的前 16 位，通常包含发送数据报的应用程序所使用的 UDP 端口。接收端的应用程序利用这个字段的值作为发送响应的目的地址。这个字段是可选项，有时不会设置源端口号。没有源端口号就默认为 0 ，通常用于不需要返回消息的通信中。 目标端口号(Destination Port): 表示接收端端口，字段长为 16 位。 长度(Length): 该字段占据 16 位，表示 UDP 数据报长度，包含 UDP 报文头和 UDP 数据长度。因为 UDP 报文头长度是 8 个字节，所以这个值最小为 8，最大长度为 2 ^ 16 = 65535 字节。 校验和(Checksum)：UDP 使用校验和来保证数据安全性，UDP 的校验和也提供了差错检测功能，差错检测用于校验报文段从源到目标主机的过程中，数据的完整性是否发生了改变。TCP 报文段结构TCP 报文段结构相比 UDP 报文结构多了很多内容。但是前两个 32 比特的字段是一样的。它们是 源端口号 和 目标端口号。另外，和 UDP 一样，TCP 也包含校验和(checksum field) ，除此之外，TCP 报文段首部还有下面这些 32 比特的序号字段(sequence number field) 和 32 比特的确认号字段(acknowledgment number field) 。这些字段被 TCP 发送方和接收方用来实现可靠的数据传输。 4 比特的首部字段长度字段(header length field)，这个字段指示了以 32 比特的字为单位的 TCP 首部长度。TCP 首部的长度是可变的，但是通常情况下，选项字段为空，所以 TCP 首部字段的长度是 20 字节。 16 比特的 接受窗口字段(receive window field) ，这个字段用于流量控制。它用于指示接收方能够/愿意接受的字节数量 可变的选项字段(options field)，这个字段用于发送方和接收方协商最大报文长度，也就是 MSS 时使用 6 比特的 标志字段(flag field)， ACK 标志用于指示确认字段中的值是有效的，这个报文段包括一个对已被成功接收报文段的确认；RST、SYN、FIN 标志用于连接的建立和关闭；CWR 和 ECE 用于拥塞控制；PSH 标志用于表示立刻将数据交给上层处理；URG 标志用来表示数据中存在需要被上层处理的 紧急 数据。紧急数据最后一个字节由 16 比特的紧急数据指针字段(urgeent data pointer field) 指出。一般情况下，PSH 和 URG 并没有使用。所以从报文段结构的对比可以看出，TCP 相比 UDP 多了许多 Flags、序号和确认号，这些都属于 TCP 的连接控制。除此之外还有接收窗口，这些属于拥塞控制和流量控制的内容。TCP 的首部开销要比 UDP 大，因为 TCP 首部固定有 20 字节，UDP 首部固定才 8 字节。TCP 和 UDP 都提供了数据校验功能。效率的差异TCP 报文段的发送采用的是”一问一答”形式的，每个请求都会被目标主机确认后再发送下一条报文，效率很慢，后来为了解决这个问题，TCP 引入了 窗口 这个概念，即使在往返时间较长、频次很多的情况下，它也能控制网络性能的下降。我们之前每次请求发送都是以报文段的形式进行的，引入窗口后，每次请求都可以发送多个报文段，也就是说一个窗口可以发送多个报文段。窗口大小就是指无需等待确认应答就可以继续发送报文段的最大值。在这个窗口机制中，大量使用了 缓冲区 ，通过对多个段同时进行确认应答的功能。如下图所示，发送报文段中高亮部分即是我们提到的窗口，在窗口内，即是没有收到确认应答也可以把请求发送出去。不过，在整个窗口的确认应答没有到达之前，如果部分报文段丢失，那么发送方将仍会重传。为此，发送方需要设置缓存来保留这些需要重传的报文段，直到收到他们的确认应答。在滑动窗口以外的部分是尚未发送的报文段和已经接受到的报文段，如果报文段已经收到确认则不可进行重发，此时报文段就可以从缓冲区中清除。在收到确认的情况下，会将窗口滑动到确认应答中确认号的位置，如上图所示，这样可以顺序的将多个段同时发送，用以提高通信性能，这种窗口也叫做 滑动窗口(Sliding window)。UDP 发送的报文段不需要确认，也就没有窗口的概念，所以 UDP 传输效率比较高。使用场景的差异TCP 和 UDP 在效率、报文段、流量控制、连接管理上均存在差异，由于这些差异导致了应用场景要有不同的选择，由于 TCP 每个包都需要进行确认，因此 TCP 不适合告诉传输数据的场景，像是这种场景使用 UDP 就好了；像是 Ping 和 DNS Lookup，这类型的操作只需要一次简单的请求/返回，不需要建立连接，用 UDP 就足够了。比如 HTTP 协议需要考虑请求响应的可靠性，这种场景应该使用 TCP 协议，但是像 HTTP 3.0 这类应用层协议，从功能性上思考，暂时没有找到太多的优化点，但是想要把网络优化到极致，就会用 UDP 作为底层技术，然后在 UDP 基础上解决可靠性。" }, { "title": "计算机网络网络层(二)", "url": "/posts/NetWorkLayerTwo/", "categories": "计算机网络", "tags": "学习", "date": "2024-01-07 07:51:00 +0000", "snippet": "计算机网络网络层这是计算机网络之网络层的第二篇文章。IP 协议路由器对分组进行转发后，就会把数据传到网络上，数据包最终是要传递到客户端或者服务器上的，那么数据包怎么知道要发往哪里呢？起到关键作用的就是 IP 协议。IP 主要分为三个部分，分别是 IP 寻址、路由和分包组包。下面我们主要围绕这三点进行阐述。IP 地址既然一个数据包要在网络上传输，那么肯定需要知道这个数据包到底发往哪里，也就是说...", "content": "计算机网络网络层这是计算机网络之网络层的第二篇文章。IP 协议路由器对分组进行转发后，就会把数据传到网络上，数据包最终是要传递到客户端或者服务器上的，那么数据包怎么知道要发往哪里呢？起到关键作用的就是 IP 协议。IP 主要分为三个部分，分别是 IP 寻址、路由和分包组包。下面我们主要围绕这三点进行阐述。IP 地址既然一个数据包要在网络上传输，那么肯定需要知道这个数据包到底发往哪里，也就是说需要一个目标地址信息，IP 地址就是连接网络中的所有主机进行通信的目标地址。因此在网络上的每个主机都需要有自己的 IP 地址。在 IP 数据包发送的链路中，有可能链路非常长，比如说由中国发往美国的一个数据包，由于网络抖动等一些意外因素可能会导致数据包丢失，这时我们在这条链路中会放入一些中转站，一方面能够判断数据包是否丢失，另一方面能够控制数据包的转发，这个中转站就是我们前面聊过的路由器，这个转发过程就是路由控制。路由控制(Routing)是指将分组数据发送到最终目标地址的功能，即使网络复杂多变，也能够通过路由控制到达目标地址。因此一个数据包能否到达目标主机，关键就在于路由控制。这里需要解释一个词就是 跳的概念，由于在一条从源地址到目标地址的链路中会布满很多路由器，路由器需要将一个数据包传送到另一个路由器，这样路由器和路由器之间的数据包传送就是跳，比如你和隔壁老王通信，中间就可能会经过老刘，于是你的数据包就会从你跳到老刘家，然后从老刘家跳到老王家。那么一跳的范围有多大呢？一跳是指从源 MAC 地址到目标 MAC 地址之间传输帧的区间，这里引出一个新的名词，MAC 地址是啥？MAC 地址指的就是计算机的物理地址(Physical Address)，它是用来确认网络设备位置的地址。在 OSI 网络模型中，网络层负责 IP 地址的定位，而数据链路层负责 MAC 地址的定位。MAC 地址用于在网络中唯一标示一个网卡，一台设备若有一或多个网卡，则每个网卡都需要并会有一个唯一的 MAC 地址，也就是说 MAC 地址和网卡是紧密联系在一起的。路由器的每一跳都需要询问当前中转的路由器，下一跳应该跳到哪里，从而跳转到目标地址。而不是数据报刚开始发送后，网络中所有的通路都会显示出来，这种多次跳转也叫做多跳路由。IP 地址定义现如今有两个版本的 IP 地址，IPv4 和 IPv6，我们首先探讨一下现如今还在广泛使用的 IPv4 地址，后面再考虑 IPv6 。IPv4 由 32 位正整数来表示，在计算机内部会转化为二进制来处理，但是二进制不符合人类阅读的习惯，所以我们根据易读性的原则把 32 位的 IP 地址以 8 位为一组，分成四组，每组之间以 . 进行分割，再将每组转换为十进制数。如下图所示那么上面这个 32 位的 IP 地址就会被转换为十进制的 156.197.1.1。除此之外，从图中我们还可以得到如下信息每个这样 8 位位一组的数字，自然是非负数，其取值范围是 [0,255]。IP 地址的总个数有 2^32 次幂个，这个数值算下来是 4294967296 ，大概能允许 43 亿台设备连接到网络。实际上真的如此吗？实际上 IP 不会以主机的个数来配置的，而是根据设备上的 网卡(NIC) 进行配置，每一块网卡都会设置一个或者多个 IP 地址，而且通常一台路由器会有至少两块网卡，所以可以设置两个以上的 IP 地址，所以主机的数量远远达不到 43 亿。IP 地址构造和分类IP 地址由 网络标识 和 主机标识 两部分组成，网络标识代表着网络地址，主机标识代表着主机地址。网络标识在数据链路的每个段配置不同的值。网络标识必须保证相互连接的每个段的地址都不重复。而相同段内相连的主机必须有相同的网络地址。IP 地址的 主机标识 则不允许在同一网段内重复出现。举个例子来说：比如说我在石家庄(好像不用比如昂)，我所在的小区的某一栋楼就相当于是网络标识，某一栋楼的第几户就相当于是我的主机标识，当然如果你有整栋楼的话，那就当我没说。你可以通过xx省xx市xx区xx路xx小区xx栋来定位我的网络标识，这一栋的第几户就相当于是我的主机标识。IP 地址分为四类，分别是 A类、B类、C类、D类、E类，它会根据 IP 地址中的第 1 位到第 4 位的比特对网络标识和主机标识进行分类。 A 类：(1.0.0.0 - 126.0.0.0)（默认子网掩码：255.0.0.0 或 0xFF000000）第一个字节为网络号，后三个字节为主机号。该类 IP 地址的最前面为 0 ，所以地址的网络号取值于 1~126 之间。一般用于大型网络。 B 类：(128.0.0.0 - 191.255.0.0)（默认子网掩码：255.255.0.0 或 0xFFFF0000）前两个字节为网络号，后两个字节为主机号。该类 IP 地址的最前面为 10 ，所以地址的网络号取值于 128~191 之间。一般用于中等规模网络。 C 类：(192.0.0.0 - 223.255.255.0)（子网掩码：255.255.255.0 或 0xFFFFFF00）前三个字节为网络号，最后一个字节为主机号。该类 IP 地址的最前面为 110 ，所以地址的网络号取值于 192~223 之间。一般用于小型网络。 D 类：是多播地址。该类 IP 地址的最前面为 1110 ，所以地址的网络号取值于 224~239 之间。一般用于多路广播用户。 E 类：是保留地址。该类 IP 地址的最前面为 1111 ，所以地址的网络号取值于 240~255 之间。为了方便理解，我画了一张 IP 地址分类图，如下所示根据不同的 IP 范围，有下面不同的地总空间分类子网掩码子网掩码(subnet mask)又叫做网络掩码，它是一种用来指明一个 IP 地址的哪些位标识的是主机所在的网络。子网掩码是一个 32位 地址，用于屏蔽 IP 地址的一部分以区别网络标识和主机标识。一个 IP 地址只要确定了其分类，也就确定了它的网络标识和主机标识，由此，各个分类所表示的网络标识范围如下用 1 表示 IP 网络地址的比特范围，0 表示 IP 主机地址的范围。将他们用十进制表示，那么这三类的表示如下保留地址在IPv4 的几类地址中，有几个保留的地址空间不能在互联网上使用。这些地址用于特殊目的，不能在局域网外部路由。IP 协议版本目前，全球 Internet 中共存有两个IP版本：IP 版本 4（IPv4）和 IP 版本6（IPv6）。 IP 地址由二进制值组成，可驱动 Internet 上所有数据的路由。 IPv4 地址的长度为 32 位，而 IPv6 地址的长度为 128 位。Internet IP 资源由 Internet 分配号码机构（IANA）分配给区域 Internet 注册表（RIR），例如 APNIC，该机构负责根 DNS ，IP 寻址和其他 Internet 协议资源。下面我们就一起认识一下 IP 协议中非常重要的两个版本 IPv4 和 IPv6。IPv4IPv4 的全称是 Internet Protocol version 4，是 Internet 协议的第四版。IPv4 是一种无连接的协议，这个协议会尽最大努力交付数据包，也就是说它不能保证任何数据包能到达目的地，也不能保证所有的数据包都会按照正确的顺序到达目标主机，这些都是由上层比如传输控制协议控制的。也就是说，单从 IP 看来，这是一个不可靠的协议。前面我们讲过网络层分组被称为数据报，所以我们接下来的叙述也会围绕着数据报展开。IPv4 的数据报格式如下IPv4 数据报中的关键字及其解释： 版本字段(Version)占用 4 bit，通信双方使用的版本必须一致，对于 IPv4 版本来说，字段值是 4。 首部长度(Internet Header Length) 占用 4 bit，首部长度说明首部有多少 32 位(4 字节)。由于 IPv4 首部可能包含不确定的选项，因此这个字段被用来确定数据的偏移量。大多数 IP 不包含这个选项，所以一般首部长度设置为 5， 数据报为 20 字节 。 服务类型(Differential Services Codepoint，DSCP) 占用 6 bit，以便使用不同的 IP 数据报，比如一些低时延、高吞吐量和可靠性的数据报。服务类型如下表所示。 拥塞通告(Explicit Congestion Notification，ECN) 占用 2 bit，它允许在不丢弃报文的同时通知对方网络拥塞的发生。ECN 是一种可选的功能，仅当两端都支持并希望使用，且底层网络支持时才被使用。最开始 DSCP 和 ECN 统称为 TOS，也就是区分服务，但是后来被细化为了 DSCP 和 ECN。 数据包长度(Total Length) 占用 16 bit，这 16 位是包括在数据在内的总长度，理论上数据的总长度为 2 的 16 次幂 - 1，最大长度是 65535 字节，但是实际上数据报很少有超过 1500 字节的。IP 规定所有主机都必须支持最小 576 字节的报文，但大多数现代主机支持更大的报文。当下层的数据链路协议的最大传输单元（MTU）字段的值小于 IP 报文长度时，报文就必须被分片。 标识符(Identification) 占用 16 bit，这个字段用来标识所有的分片，因为分片不一定会按序到达，所以到达目标主机的所有分片会进行重组，每产生一个数据报，计数器加1，并赋值给此字段。 标志(Flags) 占用 3 bit，标志用于控制和识别分片，这 3 位分别是 0 位：保留，必须为0； 1 位：禁止分片（Don’t Fragment，DF），当 DF = 0 时才允许分片； 2 位：更多分片（More Fragment，MF），MF = 1 代表后面还有分片，MF = 0 代表已经是最后一个分片。 如果 DF 标志被设置为 1 ，但是路由要求必须进行分片，那么这条数据报回丢弃 分片偏移(Fragment Offset) 占用 13 位，它指明了每个分片相对于原始报文开头的偏移量，以 8 字节作单位。 存活时间(Time To Live，TTL) 占用 8 位，存活时间避免报文在互联网中迷失，比如陷入路由环路。存活时间以秒为单位，但小于一秒的时间均向上取整到一秒。在现实中，这实际上成了一个跳数计数器：报文经过的每个路由器都将此字段减 1，当此字段等于 0 时，报文不再向下一跳传送并被丢弃，这个字段最大值是 255。 协议(Protocol) 占用 8 位，这个字段定义了报文数据区使用的协议。协议内容可以在 https://www.iana.org/assignments/protocol-numbers/protocol-numbers.xhtml 官网上获取。 首部校验和(Header Checksum) 占用 16 位，首部校验和会对字段进行纠错检查，在每一跳中，路由器都要重新计算出的首部检验和并与此字段进行比对，如果不一致，此报文将会被丢弃。 源地址(Source address) 占用 32 位，它是 IPv4 地址的构成条件，源地址指的是数据报的发送方 目的地址(Destination address)占用 32 位，它是 IPv4 地址的构成条件，目标地址指的是数据报的接收方 选项(Options) 是附加字段，选项字段占用 1 - 40 个字节不等，一般会跟在目的地址之后。如果首部长度 &gt; 5，就应该考虑选项字段。 数据 不是首部的一部分，因此并不被包含在首部检验和中。在 IP 发送的过程中，每个数据报的大小是不同的，每个链路层协议能承载的网络层分组也不一样，有的协议能够承载大数据报，有的却只能承载很小的数据报，不同的链路层能够承载的数据报大小如下。IPv4 分片一个链路层帧能承载的最大数据量叫做最大传输单元(Maximum Transmission Unit, MTU)，每个 IP 数据报封装在链路层帧中从一台路由器传到下一台路由器。因为每个链路层所支持的最大 MTU 不一样，当数据报的大小超过 MTU 后，会在链路层进行分片，每个数据报会在链路层单独封装，每个较小的片都被称为 片(fragement)。每个片在到达目的地后会进行重组，准确的来说是在运输层之前会进行重组，TCP 和 UDP 都会希望发送完整的、未分片的报文，出于性能的原因，分片重组不会在路由器中进行，而是会在目标主机中进行重组。当目标主机收到从发送端发送过来的数据报后，它需要确定这些数据报中的分片是否是由源数据报分片传递过来的，如果是的话，还需要确定何时收到了分片中的最后一片，并且这些片会如何拼接一起成为数据报。针对这些潜在的问题，IPv4 设计者将 标识、标志和片偏移放在 IP 数据报首部中。当生成一个数据报时，发送主机会为该数据报设置源和目的地址的同时贴上标识号。发送主机通常将它发送的每个数据报的标识 + 1。当某路由器需要对一个数据报分片时，形成的每个数据报具有初始数据报的源地址、目标地址和标识号。当目的地从同一发送主机收到一系列数据报时，它能够检查数据报的标识号以确定哪些数据是由源数据报发送过来的。由于 IP 是一种不可靠的服务，分片可能会在网路中丢失，鉴于这种情况，通常会把分片的最后一个比特设置为 0 ，其他分片设置为 1，同时使用偏移字段指定分片应该在数据报的哪个位置。IPv6随着端系统接入的越来越多，IPv4 已经无法满足分配了，所以，IPv6 应运而生，IPv6 就是为了解决 IPv4 的地址耗尽问题而被标准化的网际协议。IPv4 的地址长度为 4 个 8 字节，即 32 比特， 而 IPv6 的地址长度是原来的四倍，也就是 128 比特，一般写成 8 个 16 位字节。从 IPv4 切换到 IPv6 及其耗时，需要将网络中所有的主机和路由器的 IP 地址进行设置，在互联网不断普及的今天，替换所有的 IP 是一个工作量及其庞大的任务。我们后面会说。我们先来看一下 IPv6 的地址是怎样的 版本与 IPv4 一样，版本号由 4 bit 构成，IPv6 版本号的值为 6。 流量类型(Traffic Class) 占用 8 bit，它就相当于 IPv4 中的服务类型(Type Of Service)。 流标签(Flow Label) 占用 20 bit，这 20 比特用于标识一条数据报的流，能够对一条流中的某些数据报给出优先权，或者它能够用来对来自某些应用的数据报给出更高的优先权，只有流标签、源地址和目标地址一致时，才会被认为是一个流。 有效载荷长度(Payload Length) 占用 16 bit，这 16 比特值作为一个无符号整数，它给出了在 IPv6 数据报中跟在鼎昌 40 字节数据报首部后面的字节数量。 下一个首部(Next Header) 占用 8 bit，它用于标识数据报中的内容需要交付给哪个协议，是 TCP 协议还是 UDP 协议。 跳限制(Hop Limit) 占用 8 bit，这个字段与 IPv4 的 TTL 意思相同。数据每经过一次路由就会减 1，减到 0 则会丢弃数据。 源地址(Source Address) 占用 128 bit (8 个 16 位 )，表示发送端的 IP 地址。 目标地址(Destination Address) 占用 128 bit (8 个 16 位 )，表示接收端 IP 地址。可以看到，相较于 IPv4 ，IPv6 取消了下面几个字段 标识符、标志和比特偏移：IPv6 不允许在中间路由器上进行分片和重新组装。这种操作只能在端系统上进行，IPv6 将这个功能放在端系统中，加快了网络中的转发速度。 首部校验和：因为在运输层和数据链路执行了报文段完整性校验工作，IP 设计者大概觉得在网络层中有首部校验和比较多余，所以去掉了。IP 更多专注的是快速处理分组数据。 选项字段：选项字段不再是标准 IP 首部的一部分了，但是它并没有消失，而是可能出现在 IPv6 的扩展首部，也就是下一个首部中。IPv6 扩展首部IPv6 首部长度固定，无法将选项字段加入其中，取而代之的是 IPv6 使用了扩展首部。扩展首部通常介于 IPv6 首部与 TCP/UDP 首部之间，在 IPv4 中可选长度固定为 40 字节，在 IPv6 中没有这样的限制。IPv6 的扩展首部可以是任意长度。扩展首部中还可以包含扩展首部协议和下一个扩展字段。IPv6 首部中没有标识和标志字段，对 IP 进行分片时，需要使用到扩展首部。具体的扩展首部表如下所示下面我们来看一下 IPv6 都有哪些特点IPv6 特点IPv6 的特点在 IPv4 中得以实现，但是即便实现了 IPv4 的操作系统，也未必实现了 IPv4 的所有功能。而 IPv6 却将这些功能大众化了，也就表明这些功能在 IPv6 已经进行了实现，这些功能主要有 地址空间变得更大：这是 IPv6 最主要的一个特点，即支持更大的地址空间。 精简报文结构: IPv6 要比 IPv4 精简很多，IPv4 的报文长度不固定，而且有一个不断变化的选项字段；IPv6 报文段固定，并且将选项字段，分片的字段移到了 IPv6 扩展头中，这就极大的精简了 IPv6 的报文结构。 实现了自动配置：IPv6 支持其主机设备的状态和无状态自动配置模式。这样，没有 DHCP 服务器不会停止跨段通信。 层次化的网络结构： IPv6 不再像 IPv4 一样按照 A、B、C等分类来划分地址，而是通过 IANA -&gt; RIR -&gt; ISP 这样的顺序来分配的。IANA 是国际互联网号码分配机构，RIR 是区域互联网注册管理机构，ISP 是一些运营商（例如电信、移动、联通）。 IPSec：IPv6 的扩展报头中有一个认证报头、封装安全净载报头，这两个报头是 IPsec 定义的。通过这两个报头网络层自己就可以实现端到端的安全，而无需像 IPv4 协议一样需要其他协议的帮助。 支持任播：IPv6 引入了一种新的寻址方式，称为任播寻址。IPv6 地址我们知道，IPv6 地址长度为 128 位，他所能表示的范围是 2 ^ 128 次幂，这个数字非常庞大，几乎涵盖了你能想到的所有主机和路由器，那么 IPv6 该如何表示呢？一般我们将 128 比特的 IP 地址以每 16 比特为一组，并用 : 号进行分隔，如果出现连续的 0 时还可以将 0 省略，并用 :: 两个冒号隔开，记住，一个 IP 地址只允许出现一次两个连续的冒号(IPV6)。下面是一些 IPv6 地址的示例 二进制数表示 用十六进制数表示 出现两个冒号的情况如上图所示，A120 和 4CD 中间的 0 被 :: 所取代了。如何从 IPv4 迁移到 IPv6我们上面聊了聊 IPv4 和 IPv6 的报文格式、报文含义是什么、以及 IPv4 和 IPv6 的特征分别是什么，看完上面的内容，你已经知道了 IPv4 现在马上就变的不够用了，而且随着 IPv6 的不断发展和引用，虽然新型的 IPv6 可以做到向后兼容，即 IPv6 可以收发 IPv4 的数据报，但是已经部署的具有 IPv4 能力的系统却不能够处理 IPv6 数据报。所以 IPv4 噬需迁移到 IPv6，迁移并不意味着将 IPv4 替换为 IPv6。这仅意味着同时启用 IPv6 和 IPv4。那么现在就有一个问题了，IPv4 如何迁移到 IPv6 呢？这就是我们接下来讨论的重点。标志最简单的方式就是设置一个标志日，指定某个时间点和日期，此时全球的因特网机器都会在这时关机从 IPv4 迁移到 IPv6 。上一次重大的技术迁移是在 35 年前，但是很显然，不用我过多解释，这种情况肯定是 不行的。影响不可估量不说，如何保证全球人类都能知道如何设置自己的 IPv6 地址？一个设计数十亿台机器的标志日现在是想都不敢想的。隧道技术现在已经在实践中使用的从 IPv4 迁移到 IPv6 的方法是 隧道技术(tunneling)。什么是隧道技术呢？隧道技术是一种使用互联网络的基础设施在网络之间的传输数据的方式，使用隧道传递的数据可以是不同协议的数据帧或包。使用隧道技术所遵从的协议叫做隧道协议(tunneling protocol)。隧道协议会将这些协议的数据帧或包封装在新的包头中发送。新的包头提供了路由信息，从而使封装的负载数据能够通过互联网络进行传递。使用隧道技术一般都会建一个隧道，建隧道的依据如下：比如两个 IPv6 节点(下方 B、E)要使用 IPv6 数据报进行交互，但是它们是经由两个 IPv4 的路由器进行互联的。那么我们就需要将 IPv6 节点和 IPv4 路由器组成一个隧道，如下图所示借助于隧道，在隧道发送端的 IPv6 节点可将整个 IPv6 数据报放到一个 IPv4 数据报的数据(有效载荷) 字段中，于是，IPv4 数据报的地址被设置为指向隧道接收端的 IPv6 的节点，比如上面的 E 节点。然后再发送给隧道中的第一个节点 C，如下所示隧道中间的 IPv4 提供路由，路由器不知道这个 IPv4 内部包含一个指向 IPv6 的地址。隧道接收端的 IPv6 节点收到 IPv4 数据报，会确定这个 IPv4 数据报含有一个 IPv6 数据报，通过观察数据报长度和数据得知。然后取出 IPv6 数据报，再为 IPv6 提供路由，就好像两个节点直接相连传输数据报一样。" }, { "title": "计算机网络网络层(一)", "url": "/posts/NetWorkLayerOne/", "categories": "计算机网络", "tags": "学习", "date": "2023-12-31 04:32:00 +0000", "snippet": "计算机网络网络层前面我们学习了运输层如何为客户端和服务器输送数据的，提供进程端到端的通信。那么下面我们将学习网络层实际上是怎样实现主机到主机的通信服务的。几乎每个端系统都有网络层这一部分。所以，网络层必然是很重要的。下面我将花费大量篇幅来介绍一下计算机网络层的知识。网络层概述网络层是 OSI 参考模型的第三层，它位于传输层和链路层之间，网络层的主要目的是实现两个端系统之间透明的数据传输。网络...", "content": "计算机网络网络层前面我们学习了运输层如何为客户端和服务器输送数据的，提供进程端到端的通信。那么下面我们将学习网络层实际上是怎样实现主机到主机的通信服务的。几乎每个端系统都有网络层这一部分。所以，网络层必然是很重要的。下面我将花费大量篇幅来介绍一下计算机网络层的知识。网络层概述网络层是 OSI 参考模型的第三层，它位于传输层和链路层之间，网络层的主要目的是实现两个端系统之间透明的数据传输。网络层的作用从表面看上去非常简单，即将分组从一台主机移动到另外一台主机。为了实现这个功能，网络层需要两种功能 转发：因为在互联网中有很多路由器，这些路由器是构成互联网的基石，为什么这么说呢？因为路由器最重要的一个功能就是分组转发，就是靠着这些路由器，才会把一个个数据包通过链路传输给其他节点。当一个分组到达某路由器的一条输入链路时，该路由器会将分组移动到适当的输出链路。 路由选择：当分组由发送方流向接收方时，网络层必须选择这些分组的路径。计算这些路径选择的算法被称为路由选择算法(routing algorithm)。也就是说，转发是指将分组从一个输入链路转移到适当输出链路接口的路由器行为，而路由选择是指确定分组从源到目的地所定位的路径的选择。我们后面会经常提到转发和路由选择这两个名词。那么此处就有一个问题，路由器怎么知道有哪些路径可以选择呢？每台路由器都有一个关键的概念就是转发表(forwarding table)。路由器通过检查数据包标头中字段的值，来定位转发表中的项来实现转发。标头中的值即对应着转发表中的值，这个值指出了分组将被转发的路由器输出链路。如下图所示上图中有一个 1001 分组到达路由器后，首先会在转发表中进行索引，然后由路由选择算法决定分组要走的路径。每台路由器都有两种功能：转发和路由选择。下面我们就来聊一聊路由器的工作原理。路由器工作原理下面是一个路由器体系结构图，路由器主要是由 4 个组件构成。 输入端口：它有很多功能。输入端口查找/转发功能对路由器的交换功能来说至关重要，由路由器的交换结构来决定输出端口，具体来讲应该是查询转发表来确定的。 交换结构：就是将路由器的输入端口连接到它的输出端口。这种交换结构组成了是路由器内部的网络。 输出端口：通过交换结构转发分组，并通过物理层和数据链路层的功能传输分组，因此，输出端口作为输入端口执行反向数据链接和物理层功能。 路由选择处理器：在路由器内执行路由协议，维护路由表并执行网络管理功能。上面只是这几个组件的简单介绍，其实这几个组件的组成并不像描述的那样简单，下面我们就来深入聊一聊这几个组件。输入端口上面介绍了输入端口有很多功能，包括线路终端、数据处理、查找转发，其实这些功能在输入端口的内部有相应的模块，输入端口的内部实现如下图所示。每个输入端口中都有一个路由处理器维护的路由表的副本，根据路由处理器进行更新。这个路由表的副本能够使每个输入端口进行切换，而无需经过路由处理器统一处理。这是一种分散式的切换，这种方式避免了路由选择器统一处理造成转发瓶颈。在输入端口处理能力有限的路由器中，输入端口不会进行交换功能，而是由路由处理器统一处理，然后根据路由表查找并将数据包转发到相应的输出端口。一般这种路由器不是单独的路由器，而是工作站或者服务器充当的路由，这种路由器内部中，路由处理器其实就是 CPU，而输入端口其实只是网卡。输入端口会根据转发表定位输出端口，然后再会进行分组转发，那么现在就有一个问题，是不是每一个分组都有自己的一条链路呢？如果分组数量非常大，到达亿级的话，也会有亿个输出端口路径吗？潜意识告诉我显然不是的，来看下面一个例子。下面是三个输入端口对应了转发表中的三个输出链路的示例可以看到，对于这个例子来说，路由器转发表中不需要那么多条链路，只需要四条就够，即对应输出链路 0 1 2 3 。也就是说，能够使用 4 个转发表就可以实现亿级链路。如何实现呢？使用这种风格的转发表，路由器分组的地址前缀(prefix)会与该表中的表项进行匹配。如果存在一个匹配项，那么就会转发到对应的链路上，可能不好理解，我举个例子来说吧。比如这时有一个分组是 11000011 10010101 00010000 0001100 到达，因为这个分组与 11000011 10010101 00010000 相匹配，所以路由器会转发到 0 链路接口上。如果一个前缀不匹配上面三个输出链路中的一种，那么路由器将向链路接口 3 进行转发。路由匹配遵循最长前缀原则(longest prefix matching rule)，最长匹配原则说的是如果有两个匹配项一个长一个短的话，就匹配最长的。一旦通过查找功能确定了分组的输出端口后，那么该分组就会进入交换结构。在进入交换结构时，如果交换结构正在被使用，就会阻塞新到的分组，等到交换结构调度新的分组。交换结构交换结构是路由器的核心，通过交换功能把分组从输入端口转发至输出端口，这就是交换结构的主要功能。交换结构有多种形式，包括通过内存交换、通过总线交换、通过互联网络进行交换，下面我们分开来探讨一下。 经过内存交换：最开始的传统计算机就是使用内存交换的，在输入端口和输出端口之间是通过 CPU 进行的。输入端口和输出端口的功能就好像传统操作系统中的 I/O 设备一样。当一个分组到达输入端口时，这个端口会首先以中断的方式向路由选择器发出信号，将分组从输入端口拷贝到内存中。然后路由选择处理器从分组首部中提取目标地址，在转发表中找出适当的输出端口进行转发，同时将分组复制到输出端口的缓存中。这里需要注意一点，如果内存带宽以每秒读取或者写入 B 个数据包，那么总的交换机吞吐量(数据包从输入端口到输出端口的总速率) 必须小于 B/2。 经过总线交换：在这种处理方式中，总线经由输入端口直接将分组传送到输出端口，中间不需要路由选择器的干预。总线的工作流程如下：输入端口给分组分配一个标签，然后分组经由总线发送给所有的输出端口，每个输出端口都会判断标签中的端口和自己的是否匹配，如果匹配的话，那么这个输出端口就会把标签拆掉，这个标签只用于交换机内部跨越总线。如果同时有多个分组到达路由器的话，那么只有一个分组能够被处理，其他分组需要再进入交换结构前等待。 经过互联网络交换：克服单一、共享式总线带宽限制的一种方法是使用一个更复杂的互联网络。如下图所示输出端口处理如下图所示，输出端口取出已经存放在输出端口内存中的分组并将其发送到输出链路上。包括选择和去除排队的分组进行传输，执行所需的链路层和物理层的功能。在输入端口中有等待进入交换的排队队列，而在输出端口中有等待转发的排队队列，排队的位置和程度取决于流量负载、交换结构的相对频率和线路速率。随着队列的不断增加，会导致路由器的缓存空间被耗尽，进而使没有内存可以存储溢出的队列，致使分组出现丢包，这就是我们说的在网络中丢包或者被路由器丢弃。何时出现排队下面我们通过输入端口的排队队列和输出端口的排队队列来介绍一下可能出现的排队情况。输入队列如果交换结构的处理速度没有输入队列到达的速度快，在这种情况下，输入端口将会出现排队情况，到达交换结构前的分组会加入输入端口队列中，以等待通过交换结构传送到输出端口。为了描述清楚输入队列，我们假设以下情况： 使用网络互联的交换方式； 假定所有链路的速度相同； 在链路中一个分组由输入端口交换到输出端口所花的时间相同，从任意一个输入端口传送到给定的输出端口； 分组按照 FCFS 的方式，只要输出端口不同，就可以进行并行传送。但是如果位于任意两个输入端口中的分组是发往同一个目的地的，那么其中的一个分组将被阻塞，而且必须在输入队列中等待，因为交换结构一次只能传输一个到指定端口。如下图所示。在 A 队列中，输入队列中的两个分组会发送至同一个目的地 X，假设在交换结构正要发送 A 中的分组，在这个时候，C 队列中也有一个分组发送至 X，在这种情况下，C 中发送至 X 的分组将会等待，不仅如此，C 队列中发送至 Y 输出端口的分组也会等待，即使 Y 中没有出现竞争的情况。这种现象叫做线路前部阻塞(Head-Of-The-Line, HOL)。输出队列我们下面讨论输出队列中出现等待的情况。假设交换速率要比输入/输出的传输速率快很多，而且有 N 个输入分组的目的地是转发至相同的输出端口。在这种情况下，在向输出链路发送分组的过程中，将会有 N 个新分组到达传输端口。因为输出端口在一个单位时间内只能传输一个分组，那么这 N 个分组将会等待。然而在等待 N 个分组被处理的过程中，同时又有 N 个分组到达，所以 ，分组队列能够在输出端口形成。这种情况下最终会因为分组数量变的足够大，从而耗尽输出端口的可用内存。如果没有足够的内存来缓存分组的话，就必须考虑其他的方式，主要有两种：一种是丢失分组，采用弃尾(drop-tail)的方法；一种是删除一个或多个已经排队的分组，从而来为新的分组腾出空间。网络层的策略对 TCP 拥塞控制影响很大的就是路由器的分组丢弃策略。在最简单的情况下，路由器的队列通常都是按照 FCFS 的规则处理到来的分组。由于队列长度总是有限的，因此当队列已经满了的时候，以后再到达的所有分组（如果能够继续排队，这些分组都将排在队列的尾部）将都被丢弃。这就叫做尾部丢弃策略。通常情况下，在缓冲填满之前将其丢弃是更好的策略。如上图所示，A B C 每个输入端口都到达了一个分组，而且这个分组都是发往 X 的，同一时间只能处理一个分组，然后这时，又有两个分组分别由 A B 发往 X，所以此时有 4 个分组在 X 中进行等待。等上一个分组被转发完成后，输出端口就会选择在剩下的分组中根据分组调度(packet scheduleer)选择一个分组来进行传输，我们下面就会聊到分组传输。分组调度现在我们来讨论一下分组调度次序的问题，即排队的分组如何经输出链路传输的问题。我们生活中有无数排队的例子，但是我们生活中一般的排队算法都是先来先服务(FCFS)，也是先进先出(FIFO)。先进先出先进先出就映射为数据结构中的队列，只不过它现在是链路调度规则的排队模型。FIFO 调度规则按照分组到达输出链路队列的相同次序来选择分组，先到达队列的分组将先会被转发。在这种抽象模型中，如果队列已满，那么弃尾的分组将是队列末尾的后面一个。优先级排队优先级排队是先进先出排队的改良版本，到达输出链路的分组被分类放入输出队列中的优先权类，如下图所示通常情况下，每个优先级不同的分组有自己的优先级类，每个优先级类有自己的队列，分组传输会首先从优先级高的队列中进行，在同一类优先级的分组之间的选择通常是以 FIFO 的方式完成。循环加权公平排队在循环加权公平规则(round robin queuing discipline)下，分组像使用优先级那样被分类。然而，在类之间却不存在严格的服务优先权。循环调度器在这些类之间循环轮流提供服务。如下图所示在循环加权公平排队中，类 1 的分组被传输，接着是类 2 的分组，最后是类 3 的分组，这算是一个循环，然后接下来又重新开始，又从 1 -&gt; 2 -&gt; 3 这个顺序进行轮询。每个队列也是一个先入先出的队列。这是一种所谓的保持工作排队的规则，就是说如果轮询的过程中发现有空队列，输出端口不会等待分组，而是继续轮询下面的队列。" }, { "title": "评论功能前后端实现方案总结", "url": "/posts/CommentingSystem/", "categories": "技术科普", "tags": "学习", "date": "2023-12-17 11:15:00 +0000", "snippet": "评论功能本文主要就前后端嵌套评论功能的实现方案进行总结和讨论。主要包括前端显示的两种方式、后端评论数据的两种存储方式。类似:前端1. 两种方案不同角色看到评论类别的操作是由不同的，比如，本人发布的评论我们可以对其进行编辑和修改，且逻辑上不能举报自己，所以在同一页面实现不同显示，我们就需要状态量去控制不同页面元素的显隐。在此，有两种方案： 由前端设置状态量，并通过后端的数据特征维护这些状态量...", "content": "评论功能本文主要就前后端嵌套评论功能的实现方案进行总结和讨论。主要包括前端显示的两种方式、后端评论数据的两种存储方式。类似:前端1. 两种方案不同角色看到评论类别的操作是由不同的，比如，本人发布的评论我们可以对其进行编辑和修改，且逻辑上不能举报自己，所以在同一页面实现不同显示，我们就需要状态量去控制不同页面元素的显隐。在此，有两种方案： 由前端设置状态量，并通过后端的数据特征维护这些状态量，判断结束后，前端就能实现不同情况的合理显示。这种方案就是后端省事，前端需要自己做相当一部分事情去判断，要麻烦一些。 数据驱动，传入嵌套评论结构，给页面渲染。即后端在返回前端评论信息时，将这些状态量一并返回。这种的好处就是前后端都相对方便，前端专注于状态量判断和显示，后端专注于状态量的维护，在后端维护是方便的，因为数据就是从此返回的。2. 方案对比和总结这两种方案各有优劣，取决于具体的项目需求、团队分工、性能考虑以及维护成本等因素。以下是对比这两种方案的一些考虑因素：方案一：前端设置状态量，后端维护数据特征优势： 前后端职责分明： 前端负责逻辑判断和显示，后端专注于数据维护。 前端自由度高： 前端可以更灵活地处理和呈现不同状态，定制化程度较高。 性能优化： 前端可以根据需要灵活控制页面的显示逻辑，减轻后端负担。不足： 前端逻辑复杂： 前端需要处理复杂的状态判断和页面显示逻辑，可能增加前端开发难度。 可能涉及重复逻辑： 如果有多个地方需要类似的状态控制，可能需要在多个地方重复实现相同的逻辑。方案二：数据驱动，后端返回状态量优势： 简化前端逻辑： 前端只需要根据后端返回的状态量进行简单的判断，减轻前端的逻辑复杂度。 统一数据源： 所有状态相关的信息都来自后端，可以避免状态不一致的问题。 后端灵活性： 后端可以更方便地修改状态相关的逻辑，不需要修改前端代码。不足： 前后端关联紧密： 前端和后端的逻辑高度耦合，可能导致前后端开发难度增加，后端需考虑更多的前端逻辑。 性能考虑： 需要传输更多的数据到前端，可能增加网络传输成本。综合考虑： 项目需求： 如果项目对前端交互有较高的定制需求，方案一可能更适合。 团队技术栈和分工： 如果团队前端技术水平高，能够轻松处理复杂逻辑，方案一可能更适合；如果后端能够方便地维护状态逻辑，方案二可能更适合。 性能需求： 如果对网络传输和前端性能有较高要求，方案一可能更合适。在实际应用中，也可以综合两种方案，根据具体场景采取不同的策略，例如，简单的状态量由后端返回，复杂的逻辑由前端处理。后端评论功能的关键点我认为是在于如何对评论的嵌套和父子关系进行合理的数据存储。对于文章评论功能，可做出以下结构划分。 文章-一级评论（父级评论）：即用户对于文章做出的评论； 一级评论-二级评论（父-子评论）：即其他用户对文章评论的评论，以及父级评论者对这些评论的回复，简而言之，就是评论者之间的交互，其实这里已经与文章没有关系了，因为交互的对象都是评论者了。基于1、2，本文提出了两种存储方案予以处理。1. 简单方案除文章表之外，只设计一张评论表：article_comment。字段包括： articleId：文章id senderId：发送者id receiverId：接收者id comment：评论内容具体实现如下： receiveId为空时，表示该条评论是针对文章的，反之是针对评论者的。 限制：同一个人只能对一篇文章做最多一条评论，才能确保这个三元组（articleId-senderId-receiverId）的唯一性，即他人对一个人的评论做出的评论才能保证唯一，且这种方案下，被评论者也无法对评论者进行回复。局限性还是蛮多的。且没实现两级评论的解耦（文章-评论；评论-评论）。 不过，如果能容忍以上限制，那么我们可以很简单的实现一个简单的评论功能。2. 完备方案第二种方案则能完备的突破简单方案中的限制，实现完整的功能。即，除文章表之外，设计两张表： article_comment articleId：文章id userId：评论者id callId：流水号，作为该条记录的唯一标识 comment：评论内容 comment_relation userId：评论者id comment：评论内容 self_callId：该条记录的流水号，唯一标识 target_callId：评论的那条记录的流水号，来源有两个：1. article_comment表中的callId（父级评论的流水号）；2. comment_relation表中其他记录的sefl_callId流水号. 通过这两张表，我们就能实现，多级的评论和回复。 为什么用流水号而不用id：因为target_callId的来源有两个表，用记录id有可能会重复造成冲突。 callId流水号是该方案的核心，必须保证其唯一性。 这种方案的好处是我们无需关注评论者的信息如何如何，只需将流水号作为唯一标识即可，父级关系就用target和self区分即可。同时这种方案，实现了解耦，现在我们是文章评论，那么如果我们换成歌曲的评论，我们只需改article_comment，而comment_relation是可以复用的，实现了评论与被评论之物的解耦。3. 方案总结和对比当对比两种方案的好处时，我们可以着重考虑它们在可维护性、灵活性、性能以及扩展性方面的差异：简单方案的好处： 易实现和维护： 数据表结构相对简单，易于实现和维护，前端和后端的开发相对较为轻松。 适用于简单场景： 对于不需要多级评论和回复的简单场景，这种方案足够满足需求，减少了系统的复杂性。 性能优化： 由于数据表结构简单，可能对数据库的查询和操作有一定的性能优势。完备方案的好处： 多级评论和回复的支持： 这是该方案的核心优势，可以实现更丰富的评论交互，提高用户体验。 解耦性强： 通过两张表的设计，实现了评论与被评论之物的解耦。对于不同的被评论对象（例如文章、歌曲等），只需修改一张表而另一张表可以复用。 灵活扩展： 由于流水号的设计，方便扩展和修改评论系统的其他功能，如点赞、举报等。 更高的定制化： 具备更高的定制化程度，适用于复杂的业务场景，满足更多需求。 可复用性： 评论关系表(comment_relation)的设计使得该方案更容易在不同模块中复用，例如可以用于其他社交互动。综合对比： 简单方案适用于： 简单的评论系统，不需要多级回复的情况，且对数据库设计和维护要求较低。 完备方案适用于： 复杂的评论系统，需要支持多级评论和回复，并且对系统的扩展性、定制化程度有较高要求。总体来说，选择哪种方案应该根据具体的业务需求、系统规模和团队技术水平来决定。在简单场景下，可以选择简单方案，而在对评论系统有更多要求的情况下，完备方案可能更为合适。" }, { "title": "怎么做一个好的技术规划 ｜ PDCA 模型", "url": "/posts/PDCA/", "categories": "技术科普", "tags": "学习", "date": "2023-12-17 11:15:00 +0000", "snippet": "技术规划的本质首先，技术规划的本质 不是一时的头脑发热，不是一件件 TODO 列表，而是对一个明确目标起到导航的作用； 不是所谓的葵花宝典，而是通过合理的，设立规划目标、阶段性的里程碑，不断做有积累的事情，最终就能够得到理想的结果； 规划不是教科书，也不是永远一成不变的，需要有定期 review; 是外部视角快速理解你所做的事情的最佳渠道；（职场晋升之类）；这里，我想表达的是，没有任...", "content": "技术规划的本质首先，技术规划的本质 不是一时的头脑发热，不是一件件 TODO 列表，而是对一个明确目标起到导航的作用； 不是所谓的葵花宝典，而是通过合理的，设立规划目标、阶段性的里程碑，不断做有积累的事情，最终就能够得到理想的结果； 规划不是教科书，也不是永远一成不变的，需要有定期 review; 是外部视角快速理解你所做的事情的最佳渠道；（职场晋升之类）；这里，我想表达的是，没有任何一个需求是小需求，任何的事情都值得去好好的做一个规划，并且尝试将问题解决的更好更完美；PDCA 模型技术目标的实现可以基于 PDCA 模型去推进的，任何一个技术的需求或者需要拿到技术结果的点都可以采取 PDCA 模型进行规划和推进执行； Plan（计划）: 需求和目标的确定和计划的制定/调整； Do（执行）: 按照计划实现规划的内容； Check（检查）: 总结执行的结果，哪些做对了？哪些做错了？明确效果，找出问题； Act（行动）： 对检查的结果进行处理，成功的经验加以肯定，并形成标准和规范，后续强化，对于失败的教训进行总结和复盘，后续规避，对于遗留的事情在下一个 PDCA 循环中进行处理。规划的目标目标的设定非常的重要，是能够取得结果的关键因素，清晰的目标，也是保证高效工作的最重要的因素；###基于痛点/机会寻找目标寻找目标通常是规划中最难的部分，痛点或者机会并不是主观上的感受或者凭空的猜想，而是需要做大量的分析和假设验证，通过数据量化和分析，才能去找到适合团队/自己的可以够得到的正确的目标；设立目标一般可以遵循 SMART 原则，可以逐条对照目标原则看你的目标是否符合。- Specific （明确性）- Measurable （可量化性）- Attainable （可实现性）- Relevant （相关性）- Time-bound （时限性）举几个🌰比如说✅ 在这个双月将我们的系统稳定性提升到 99.99%，P99延时 降低到 50ms✅ 将 xx 功能在本双月完成上线，取得 D1 留存相对 5% 的提升；❌ 今年年底前赚他 1 个小目标❌ 这个双月把性能优化做好；❌ 持续推进工程效率提升；基于 W1H 分析法 Why / Who / When / Where / What / How✅ 因为经常在半夜接到服务的报警 / 我 / 在这个双月计划 / 在我们的系统中 / 将服务稳定性提高到 99.99% / 通过重构和报警阈值设置的方式；✅ 因为产品 DAU 增长放缓 / 我们团队 / 计划在今年 / 在自然流量和用户留存方向上 / 将 DNU 提升到 xxxM，D1 留存提升到 xx / 通过数据分析、数据挖掘和需求迭代的方式；✅ 因为上周体检结果脂肪肝的原因 / 我 / 在最近 4 个月计划 / 在健身房 / 将体重减 20 斤 / 通过有氧运动的方式；其他考虑因素 收益：目标一定是能够获得业务收益的，例如收入、成本、效率、体验，不要设立一个无法明确收益的目标。 技术：从技术层面来看，分为三个层次：现有扩展或者深度挖掘，新方向探索，发展趋势判断。越往后面的层次约考验技术同学的技术判断能力，这个需要大量的积累。 团队：目标是否是现有团队人力可以承受的？技术能力上团队是否能够支撑？比如做一个操作系统可能就不是一个中小公司可以考虑的事情，或许只有华为、BAT这类大型公司才有可能落地实现。目标拆解目标明确后，就要对目标实现的路径进行拆解了，和 OKR 拆解的方式是类似的，KR 全部实现后应该是能够达成目标的，可以衡量的；比如常见的有脑图思维，架构思维，具体就不展开了，感兴趣的同学可以自行去找资料。执行计划目标拆解清楚了之后，并不意味着技术规划就完成了，我们之前仅仅只是完成了理想状态下的拆解，还需要结合实际的情况看看怎么样执行，才能最终达成理想状态的目标，执行计划产出的是拆解列表；每个月需要做到什么程度，每个季度需要做到什么程度。等等等，接下来，需要考虑的是资源评估永远要搞清楚手上有什么牌？没有金刚钻，不揽瓷器活人力资源，机器资源，合作资源等。优先级评估 聚焦重点，稳步前进 所有的 KR 应该只有 P0, P1, P2 的优先级； P0: 重要紧急P1: 重要不紧急P2: 不重要不紧急 P0 总数建议最多不超过 3 个，每个子方向建议最多 1 个 P0； 优先级原则：如果现在的资源只能实现一个 KR，那这个 KR 是什么P0 是必须保证按时按预期目标交付的；风险评估 规划中的方案是否具备可行性？ 规划是否适合当前产品和团队/个人情况？ 是否有内部/外部 支持？ 是否有超出自己认知/能力外的部分？需要引入相关领域专家介入？ 是否会给自己或其他业务/团队带来影响？ 是否有 Plan B？规划对齐你的规划也许并没有考虑的非常周全，当完成初版后，有一个非常重要的工作就是对齐，保证所有人的优先级和资源判断都是一致的； 和上级/Owner 对 和合作方对齐 和外部合作方对齐 和下级对齐（如有）做好上面几步，基本上一个技术规划做的很不错了，接下来的，就可以开始执行了！" }, { "title": "为什么游戏项目不使用微服务架构呢？", "url": "/posts/Gaming&Microservices/", "categories": "技术科普", "tags": "学习", "date": "2023-12-10 13:02:00 +0000", "snippet": "微服务化？最近，在知乎上看到这样一个问题：”为什么游戏公司的 server 不愿意微服务化？”背景介绍笔者最近去面试了家游戏公司。最近面试了一家游戏公司（满大间的，有上市）我问他，公司有没有做微服务架构的打算及考量？他很惊讶的说，我没听说过微服务耶，你可以解释一下吗？我大概说了，方便测试，方便维护，方便升级，服务之间松耦合，可多语言开发，自动扩容…之类的点然后他说游戏 server 不太需要...", "content": "微服务化？最近，在知乎上看到这样一个问题：”为什么游戏公司的 server 不愿意微服务化？”背景介绍笔者最近去面试了家游戏公司。最近面试了一家游戏公司（满大间的，有上市）我问他，公司有没有做微服务架构的打算及考量？他很惊讶的说，我没听说过微服务耶，你可以解释一下吗？我大概说了，方便测试，方便维护，方便升级，服务之间松耦合，可多语言开发，自动扩容…之类的点然后他说游戏 server 不太需要微服务，因为要求 real time，做微服务会影响效能，分模组来开发就好了我也不确定，但微服务不是趋势吗？特别是大公司，游戏 server 的服务应该很容易拆分吧？回答整理了一些不错的回答：A:比如 moba 类游戏/王者荣耀/LOL，就看王者荣耀的客户端吧，想象一下。账号系统，符文系统，英雄系统，皮肤系统，好友系统，好友之间 messaging，这些都是常规操作，如果流量足够大，当然可以用微服务的架构去做。不过这不是这个游戏的核心，核心是 MOBA：Multiplayer online battle arena。特性是什么？10 个人之间各种游戏事件的高速多向通讯 streaming/broadcast/multicast/pubsub 各种通讯模式所以游戏的核心在于小规模群体之间的高速网络通信 。就是对方说的 realtime。多了一个 10ms 的延迟玩家就要骂娘了。 微服务为了把业务完美拆解，把原来的同一个进程里的模块拆分成不同的服务，显著增加额外的网络开销 。更别说什么 service mesh，各种 gateway，proxy，sidecar，简直就是担心延迟太低。 微服务基本只有 request/response 的模式。做不了 streaming？微服务通常要求应用是无状态的才能做到水平扩展。streaming 本身就是加入了状态 我可以想像，为了提高通讯的性能，一场英雄联盟游戏很可能会使用同一个服务器负责这 10 个玩家之间的通讯，这样就使得数据可以在本地交换，性能最大化 。这对客户端或者说服务端统一网关的要求是必须支持 sticky routing。假设客户端连接断了，接下来的必须重连之前的同一个服务器。微服务的 stateless，水瓶扩展要求本身就是反 sticky routing 的，因为 sticky routing 本身就是状态。 对服务端集群来说，同时有无数个王者荣耀的比赛在进行，每个都可以看成一个沙盒，每个沙盒都处于一个不同的状态：塔被推了几个了，你被杀了几次了，对面几个超神了，20 分钟到了没。这些都是长时间存在的状态，直到游戏结束，服务端才可以清理一场游戏的状态。所以虽然不用把这些状态写进持久性存储，但是必然会在内存中存在很长时间。都是状态，反正有状态，就别想用微服务。除非你说把这些状态都移到 redis 里去，那么在服务器在信息流传输到一半还要做一个 remote request，一来一回，延迟就上升了。总之怎样都不好。（比如想象对方在 A 你的水晶，每一次 A 的操作都是一个 event，被 streaming 到服务端的沙盒中，沙盒中有一个流处理器，每次接收到一个你水晶被 A 的 event 都会计算一下你水晶爆了没。这个计算需要极快，你是不可能把你水晶生命值的数据存在远端的）像这类游戏，都是对网络，内存，CPU 的优化需求很高，整个游戏进行过程中，几乎不存在什么 RPC call，真的需要 remote data，也应该是 prefetch，就是在游戏刚开始的时候加载好微服务不是什么银弹，也就是方便拆解一下原来的 CRUD 应用罢了而已，一没触及高级的交互方式，二没触及分布式系统真正的难点：状态，其实没有大家想的那么有用。之所以感觉上好像微服务改变了互联网，只不过 90%的互联网应用都只是简单小规模的 CRUD 而已。对方没有听说过微服务完全没有问题，因为这本身就不是什么高深的概念，反而对方听你一说一下就知道微服务不适合游戏，说明对方理解能力很强，对游戏系统设计也了解足够深。B:看来是是最近被 微服务洗脑了， 个人感觉正常微服务，一个服务必须有 3 个以上工程师单独维护，才真真把微服务盘起来。而且微服务现在最多就是 HTTP 这种协议跑，很占性能的，就算是走 TCP HTTP2 远远不如单体性能，尤其是微服务做业务分叉调用的时候怎么划分，数据事件一致性 是非常头痛的一件事。微服务用的广是 WEB 方面 而且工程师多，业务变来变多，而且它几乎是自己玩自己的。这种场景就非常适合。实时性不需要那么高那种我知道很多人，把微服务魔化了，别人要 100 层功力，而你只学到了 1 层。然后就硬搬上来跟别人说这很牛逼我看过那么多博客，技术文，唯独就微软官方那一篇 微服务技术写的最好，明确的告诉你这种架构适合什么样的场景跟团队，什么样的场景不要用，而不是一些文章无脑吹脱离业务的技术架构，就是为了框架而架构，就是没事给自己搬石头砸脚跟对于开发者来说，自己去研究研究新技术是值的非常推崇的。但是不考虑实际情况，那就是魔怔了。为啥有这种感触，因为我是受害人。所以奉劝各位 什么样的团队项目底子业务 就选择最合适自己的架构，不要盲目去跟风，更不要盲目的去对比大厂云云云，你的业务跟团队是别人的零头都不够。别人的 HR 团队可能都比你技术团队人数多。架构分的越单元化，那么需要的人数是翻倍起来维护的，不然你就会发现为什么这个架构我用起来这么啰嗦。不是别人的架构不够好，是你的团队还不需要C:…游戏服务器都是几乎都是带大量状态的，我就问你一个致命问题，游戏里面本来在同个进程内，内存直接可以访问到，走微服务就可能这个服务不在本机设备上面，那就需要走网络传输过去，那么你考虑过延迟问题吗？有考虑过如果连接挂掉？每个微服务跨本机都是一个 RPC 调用，这东西是不可靠的，如果你要可靠行啊，你准备约定一个接口调用多少毫秒超时重传？200/500?(这就存在两个情况：对面已经处理，对面没有处理，但实际上是同一个调用请求）如果玩家只是放一个技能，需要投射状态呢，因为断开重传等几百毫秒响应，那么玩家体验是啥。固然，我们可以使用协程，C++也可以，那么编码复杂度有考虑过吗？而且大量的异步编程在游戏服务器上面是很困难的，也就意味着需要更多的游戏服务器开发人员，而且还得要求开发人员的综合素质不能太差。" }, { "title": "什么是分布式系统？和集群有什么区别？", "url": "/posts/Distributed&Cluster/", "categories": "技术科普", "tags": "学习", "date": "2023-12-03 12:54:00 +0000", "snippet": "什么是集中式系统？集中式系统是指把整个系统的所有功能，比如数据库、缓存等这些全都部署在一起，然后通过整套系统对外提供服务。但是集中式系统存在大而复杂、难以维护、容易发生单点故障、扩展性比较差等问题。然而这些问题在分布式系统中就可以很好地解决。什么是分布式系统？分布式系统是相对相对于集中式系统而言的，分布式系统是指将一个集中式的系统拆分成多个系统，然后每一个系统单独对外提供部分功能，整个分布式...", "content": "什么是集中式系统？集中式系统是指把整个系统的所有功能，比如数据库、缓存等这些全都部署在一起，然后通过整套系统对外提供服务。但是集中式系统存在大而复杂、难以维护、容易发生单点故障、扩展性比较差等问题。然而这些问题在分布式系统中就可以很好地解决。什么是分布式系统？分布式系统是相对相对于集中式系统而言的，分布式系统是指将一个集中式的系统拆分成多个系统，然后每一个系统单独对外提供部分功能，整个分布式系统整体对外提供一整套服务。但是对于访问分布式系统的用户来说，其感知就如同在访问一台计算机一样。分布式系统即通过利用多台普通计算机（相对于昂贵的大型机）组成分布式集群，对外提供系统服务。随着计算机台数的增加，CPU、内存、存储资源等计算机资源也不断变多，该系统可以处理的并发访问量也就越大，但是随之而来也带来了一些问题，由于是集群部署，不是在一台机子上，所以该系统可能存在许多问题，如网络通信延迟、数据一致性等问题。这里我们拿一个电商网站来举例（主要是电商网站一般功能比较齐全，很多场景都能学习到），我们这里把这个电商系统拆分成多个子系统，比如订单系统、购物车系统、物流系统、支付系统等。我们把不同的模块部署到不同的机器上，各个模块之间通过RPC（Dubbo、gRPC）即远程服务调用等方式进行通信，然后以一个分布式系统的方式对外提供服务。分布式与集群之间的区别？分布式（Distributed）是指在多台不同的服务器中部署不同的服务模块，通过远程调用的方式相互之间进行协同工作，然后对外提供服务。集群（cluster）是指在多台不同的服务器中部署相同的应用或者服务模块，构成一个集群，通过请求负载均衡的方式对外提供服务。那如何理解集群与分布式之间的区别呢？你可以通过一个案例来理解，集群的话就是一个组织里的所有人都干同一件事情，比如，百度搜索，这个在每个人的电脑上其都是一样的，主要负责搜索的业务，而分布式就是指一个团队中每部分人负责一部分的内容，比如阿里里面，有些人负责淘宝业务的内容，有些人负责阿里云业务·······每部分人负责的业务有所差异，这种就是分布式。分布式系统的特征分布式系统需要各个主机之间通信和协调主要通过网络进行，所以，分布式系统中的计算机在空间上几乎没有任何限制，这些计算机可能被放在不同的机柜上，也可能被部署在不同的机房中，还可能在不同的城市中，对于大型的网站甚至可能分布在不同的国家和地区。但是，无论空间上如何分布，一个标准的分布式系统应该具有以下几个主要特征： 分布性分布式系统中的多台计算机之间在空间位置上可以随意分布，系统中的多台计算机之间没有主、从之分，即没有控制整个系统的主机，也没有受控的从机。 透明性系统资源被所有计算机共享。每台计算机的用户不仅可以使用本机的资源，还可以使用本分布式系统中其他计算机的资源(包括CPU、文件、打印机等)。 同一性系统中的若干台计算机可以互相协作来完成一个共同的任务，或者说一个程序可以分布在几台计算机上并行地运行。 通信性系统中任意两台计算机都可以通过通信来交换信息。相比集中式系统，分布式系统有什么优势与不足？和集中式系统相比，分布式系统的性价比更高、处理能力更强、可靠性更高、也有很好的扩展性。但是，分布式在解决了网站的高并发问题的同时也带来了一些其他问题。 分布式的必要条件就是网络，这可能对性能甚至服务能力造成一定的影响。 一个集群中的服务器数量越多，服务器宕机的概率也就越大。 由于服务在集群中分布式部署，用户的请求只会落到其中一台机器上，所以，一旦处理不好就很容易产生数据一致性问题。" }, { "title": "完蛋！我被算法包围了", "url": "/posts/surroundedByAlgorithms/", "categories": "技术科普", "tags": "学习", "date": "2023-11-26 07:29:00 +0000", "snippet": "算法在日常生活的应用算法听上去很复杂，离我们很遥远。但是以下 10 种算法广泛用于我们的日常生活，包括互联网搜索引擎、社交网络、WiFi、手机甚至卫星。以下是主宰我们世界的十种算法：01 排序我们学习算法一般都是从排序算法开始的，比如冒泡排序、快排序、归并排序、堆排序等。笔者曾经做过的一个大批量对账系统，就是在第一步使用归并外排序（External MergeSort）来整理数据并对齐来自不...", "content": "算法在日常生活的应用算法听上去很复杂，离我们很遥远。但是以下 10 种算法广泛用于我们的日常生活，包括互联网搜索引擎、社交网络、WiFi、手机甚至卫星。以下是主宰我们世界的十种算法：01 排序我们学习算法一般都是从排序算法开始的，比如冒泡排序、快排序、归并排序、堆排序等。笔者曾经做过的一个大批量对账系统，就是在第一步使用归并外排序（External MergeSort）来整理数据并对齐来自不同系统的数据。很难想象如果没有排序算法，这个第一步会花费多少时间。02 傅里叶变换和快速傅里叶变换傅立叶变换用于信号在时域和频域之间的变换。傅里叶变换在医学、数据科学、物理学、声学、光学、结构力学、量子力学、数论、组合数学、概率论、统计学、信号处理、密码学、大气科学、海洋学、通讯、金融等领域都有着广泛的应用（Wikipedia）。下图展示了傅立叶变换的原理 - 一个波形都可以用不同的正弦波来叠加合成。将复杂波形分解为不同的正弦波后，就可以做滤波处理。比如在图像处理中，使用离散傅立叶变换将图像变为频谱图后，可以做如下操作： 去除某个特定频率的噪声 减弱高频信号，降低图像清晰度和对比度，方便压缩 增强高频信号，提高图像细节清晰度和对比度03 Dijkstra 算法Dijkstra 算法是由荷兰计算机科学家Edsger Dijkstra在1956年发现的算法，用于在图上两个顶点间搜索最短路径。Dijkstra 算法广泛应用于交通、寻路、规划中（Wikipedia）。04 RSA 算法RSA 算法是非对称加密算法，名称来源于三名提出者的姓氏首字母，广泛应用于网络通信、金融、军工等。“非对称”是说 RSA 算法的加密和解密使用的是不同的密钥：加密使用公钥，解密使用私钥。公钥是公开的，任何人都可以获取并用其进行加密；私钥是不公开的，只有生成密钥对的人才能持有。我们熟悉的 HTTPS，其 TLS 握手就是用 RSA 算法实现的。由于 RSA 算法计算量较大，在握手成功后，后续的数据通信都使用基于 Session Key 的对称加密。05 安全散列算法（Secure Hash Algorithm）SHA 系列算法将文本转换为相同长度字符串，此过程不可逆。主要用于文件传输时的消息摘要、数字签名等。06 质因数分解（Integer Factorization）质因数分解是将一个正整数分解为几个质数约数的乘积的过程。这个算法应用于密码学中，比如上文提到的 RSA 算法就使用了质因数分解来计算密钥。07 链接分析（Link Analysis）链接分析，起源于对Web结构中超链接的多维分析。广泛应用于网络信息检索、数据挖掘、Web结构建模、社交网络分析等。比如在搜索引擎中，就是根据网页链接的流行度来给网页排序的。一个网页拥有的反向链接越多，就越可能是高质量网页。这样也就催生了SEO（Search Engine Optimization）这个职能，来逆向工程各种搜索引擎的排名规则。08 PID 算法PID（Proportional Integral Derivative）算法翻译过来就是比例、积分、微分控制，分别对应于图中的 3 个控制模块。这 3 种模块的输出会反馈叠加到系统的输入中，来控制系统的行为，使其稳定在一个目标值上。这个算法广泛应用于控制系统，比如温度控制，无人机姿态控制等。09 数据压缩算法我们平时浏览网页时接触到的音频、视频、文件是经过压缩的，这样可以极大地提高数据在网络上传输的速度，优化用户体验。比如用于有损图片压缩的 JPEG 算法，用于音频压缩的 MP3 算法等。10 随机数生成随机数最重要的特性是：后面生成的数与前面的数毫无关系。我们在编程中经常使用到的是伪随机数，是通过固定算法生成的。随机数生成在密码学中很重要，常用于生成密钥。金融行业对资产风险进行场景模拟时，常使用蒙特卡洛仿真，这也需要用到随机数，以保证测试数据的统计学随机性。" }, { "title": "计算机网络传输层协议(五)", "url": "/posts/NetWorkProtocol5/", "categories": "计算机网络", "tags": "学习", "date": "2023-11-19 11:54:00 +0000", "snippet": "传输层协议这是计算机网络传输层的第五篇文章。TCP 数据流和窗口管理我们在之前的讲述中知道了可以使用滑动窗口来实现流量控制，也就是说，客户端和服务器可以相互提供数据流信息的交换，数据流的相关信息主要包括报文段序列号、ACK 号和窗口大小。图中的两个箭头表示数据流方向，数据流方向也就是 TCP 报文段的传输方向。可以看到，每个 TCP 报文段中都包括了序列号、ACK 和窗口信息，可能还会有用户...", "content": "传输层协议这是计算机网络传输层的第五篇文章。TCP 数据流和窗口管理我们在之前的讲述中知道了可以使用滑动窗口来实现流量控制，也就是说，客户端和服务器可以相互提供数据流信息的交换，数据流的相关信息主要包括报文段序列号、ACK 号和窗口大小。图中的两个箭头表示数据流方向，数据流方向也就是 TCP 报文段的传输方向。可以看到，每个 TCP 报文段中都包括了序列号、ACK 和窗口信息，可能还会有用户数据。TCP 报文段中的窗口大小表示接收端还能够接收的缓存空间的大小，以字节为单位。这个窗口大小是一种动态的，因为无时无刻都会有报文段的接收和消失，这种动态调整的窗口大小我们称之为滑动窗口，下面我们就来具体认识一下滑动窗口。滑动窗口TCP 连接的每一端都可以发送数据，但是数据的发送不是没有限制的，实际上，TCP 连接的两端都各自维护了一个发送窗口结构 (send window structure) 和 接收窗口结构 (receive window structure)，这两个窗口结构就是数据发送的限制。发送方窗口下图是一个发送方窗口的示例。在这幅图中，涉及滑动窗口的四种概念： 已经发送并确认的报文段：发送给接收方后，接收方回回复 ACK 来对报文段进行响应，图中标注绿色的报文段就是已经经过接收方确认的报文段。 已经发送但是还没确认的报文段：图中绿色区域是经过接收方确认的报文段，而浅蓝色这段区域指的是已经发送但是还未经过接收方确认的报文段。 等待发送的报文段：图中深蓝色区域是等待发送的报文段，它属于发送窗口结构的一部分，也就是说，发送窗口结构其实是由已发送未确认 + 等待发送的报文段构成。 窗口滑动时才能发送的报文段：如果图中的 [4,9] 这个集合内的报文段发送完毕后，整个滑动窗口会向右移动，图中橙色区域就是窗口右移时才能发送的报文段。滑动窗口也是有边界的，这个边界是 Left edge 和 Right edge，Left edge 是窗口的左边界，Right edge 是窗口的右边界。当 Left edge 向右移动而 Right edge 不变时，这个窗口可能处于 close 关闭状态。随着已发送的数据逐渐被确认从而导致窗口变小时，就会发生这种情况。当 Right edge 向右移动时，窗口会处于 open 打开状态，允许发送更多的数据。当接收端进程读取缓冲区数据，从而使缓冲区接收更多数据时，就会处于这种状态。还可能会发生 Right edge 向左移动的情况，会导致发送并确认的报文段变小，这种情况被称为糊涂窗口综合症，这种情况是我们不愿意看到的。出现糊涂窗口综合症时，通信双方用于交换的数据段大小会变小，而网络固定的开销却没有变化，每个报文段中有用数据相对于头部信息的比例较小，导致传输效率非常低。这就相当于之前你明明有能力花一天时间写完一个复杂的页面，现在你花了一天的时间却改了一个标题的 bug，大材小用。每个 TCP 报文段都包含ACK 号和窗口通告信息，所以每当收到响应时，TCP 接收方都会根据这两个参数调整窗口结构。TCP 滑动窗口的 Left edge 永远不可能向左移动，因为发送并确认的报文段永远不可能被取消，就像这世界上没有后悔药一样。这条边缘是由另一段发送的 ACK 号控制的。当 ACK 标号使窗口向右移动但是窗口大小没有改变时，则称该窗口向前滑动。如果 ACK 的编号增加但是窗口通告信息随着其他 ACK 的到达却变小了，此时 Left edge 会接近 Right edge。当 Left edge 和 Right edge 重合时，此时发送方不会再传输任何数据，这种情况被称为零窗口。此时 TCP 发送方会发起窗口探测，等待合适的时机再发送数据。接收方窗口接收方也维护了一个窗口结构，这个窗口要比发送方的简单很多。这个窗口记录了已经接收并确认的数据，以及它能够接收的最大序列号。接收方的窗口结构不会存储重复的报文段和 ACK，同时接收方的窗口也不会记录不应该收到的报文段和 ACK。下面是 TCP 接收方的窗口结构。与发送端的窗口一样，接收方窗口结构也维护了一个 Left edge 和 Right edge。位于 Left edge 左边的被称为已经接收并确认的报文段，位于 Right edge 右边的被称为不能接收的报文段。对于接收端来说，到达序列号小于 Left efge 的被认为是已经重复的数据，需要丢弃。超过 Right edge 的被认为超出处理范围。只有当到达的报文段等于 Left edge 时，数据才不会被丢弃，窗口才能够向前滑动。接收方窗口结构也会存在零窗口的情况，如果某个应用进程消耗数据很慢，而 TCP 发送方却发送了大量的数据给接收方，会造成 TCP 缓冲区溢出，通告发送方不要再发送数据了，但是应用进程却以非常慢的速度消耗缓冲区的数据（比如 1 字节），就会告诉接收端只能发送一个字节的数据，这个过程慢慢持续，造成网络开销大，效率很低。我们上面提到了窗口存在 Left edge = Right edge 的情况，此时被称为零窗口，下面我们就来具体研究一下零窗口。零窗口TCP 是通过接收端的窗口通告信息来实现流量控制的。通告窗口告诉了 TCP ，接收端能够接收的数据量。当接收方的窗口变为 0 时，可以有效的阻止发送端继续发送数据。当接收端重新获得可用空间时，它会给发送端传输一个 窗口更新 告知自己能够接收数据了。窗口更新一般是纯 ACK ，即不带任何数据。但是纯 ACK 不能保证一定会到达发送端，于是需要有相关的措施能够处理这种丢包。如果纯 ACK 丢失的话，通信双方就会一直处于等待状态，发送方心想拉垮的接收端怎么还让我发送数据！接收端心想天杀的发送方怎么还不发数据！为了防止这种情况，发送方会采用一个持续计时器来间歇性的查询接收方，看看其窗口是否已经增长。持续计时器会触发窗口探测，强制要求接收方返回带有更新窗口的 ACK。窗口探测包含一个字节的数据，采用的是 TCP 丢失重传的方式。当 TCP 持续计时器超时后，就会触发窗口探测的发送。一个字节的数据能否被接收端接收，还要取决于其缓冲区的大小。拥塞控制有了 TCP 的窗口控制后，使计算机网络中两个主机之间不再是以单个数据段的形式发送了，而是能够连续发送大量的数据包。然而，大量数据包同时也伴随着其他问题，比如网络负载、网络拥堵等问题。TCP 为了防止这类问题的出现，使用了 拥塞控制 机制，拥塞控制机制会在面临网络拥塞时遏制发送方的数据发送。拥塞控制主要有两种方法 端到端的拥塞控制: 因为网络层没有为运输层拥塞控制提供显示支持。所以即使网络中存在拥塞情况，端系统也要通过对网络行为的观察来推断。TCP 就是使用了端到端的拥塞控制方式。IP 层不会向端系统提供有关网络拥塞的反馈信息。那么 TCP 如何推断网络拥塞呢？如果超时或者三次冗余确认就被认为是网络拥塞，TCP 会减小窗口的大小，或者增加往返时延来避免。 网络辅助的拥塞控制: 在网络辅助的拥塞控制中，路由器会向发送方提供关于网络中拥塞状态的反馈。这种反馈信息就是一个比特信息，它指示链路中的拥塞情况。TCP 拥塞控制如果你看到这里，那我就暂定认为你了解了 TCP 实现可靠性的基础了，那就是使用序号和确认号。除此之外，另外一个实现 TCP 可靠性基础的就是 TCP 的拥塞控制。如果说TCP 所采用的方法是让每一个发送方根据所感知到的网络的拥塞程度来限制发出报文段的速率，如果 TCP 发送方感知到没有什么拥塞，则 TCP 发送方会增加发送速率；如果发送方感知沿着路径有阻塞，那么发送方就会降低发送速率。但是这种方法有三个问题 TCP 发送方如何限制它向其他连接发送报文段的速率呢？ 一个 TCP 发送方是如何感知到网络拥塞的呢？ 当发送方感知到端到端的拥塞时，采用何种算法来改变其发送速率呢？我们先来探讨一下第一个问题，TCP 发送方如何限制它向其他连接发送报文段的速率呢？我们知道 TCP 是由接收缓存、发送缓存和变量(LastByteRead, rwnd，等)组成。发送方的 TCP 拥塞控制机制会跟踪一个变量，即 拥塞窗口(congestion window) 的变量，拥塞窗口表示为 cwnd，用于限制 TCP 在接收到 ACK 之前可以发送到网络的数据量。而接收窗口(rwnd) 是一个用于告诉接收方能够接受的数据量。一般来说，发送方未确认的数据量不得超过 cwnd 和 rwnd 的最小值，也就是LastByteSent - LastByteAcked &lt;= min(cwnd,rwnd)**由于每个数据包的往返时间是 RTT，我们假设接收端有足够的缓存空间用于接收数据，我们就不用考虑 rwnd 了，只专注于 cwnd，那么，该发送方的发送速率大概是 cwnd/RTT 字节/秒 。通过调节 cwnd，发送方因此能调整它向连接发送数据的速率。一个 TCP 发送方是如何感知到网络拥塞的呢？这个我们上面讨论过，是 TCP 根据超时或者 3 个冗余 ACK 来感知的。当发送方感知到端到端的拥塞时，采用何种算法来改变其发送速率呢 ?这个问题比较复杂，且容我娓娓道来，一般来说，TCP 会遵循下面这几种指导性原则 如果在报文段发送过程中丢失，那就意味着网络拥堵，此时需要适当降低 TCP 发送方的速率。 一个确认报文段指示发送方正在向接收方传递报文段，因此，当对先前未确认报文段的确认到达时，能够增加发送方的速率。为啥呢？因为未确认的报文段到达接收方也就表示着网络不拥堵，能够顺利到达，因此发送方拥塞窗口长度会变大，所以发送速率会变快 带宽探测，带宽探测说的是 TCP 可以通过调节传输速率来增加/减小 ACK 到达的次数，如果出现丢包事件，就会减小传输速率。因此，为了探测拥塞开始出现的频率， TCP 发送方应该增加它的传输速率。然后慢慢使传输速率降低，进而再次开始探测，看看拥塞开始速率是否发生了变化。在了解完 TCP 拥塞控制后，下面我们就该聊一下 TCP 的 拥塞控制算法(TCP congestion control algorithm) 了。TCP 拥塞控制算法主要包含三个部分：慢启动、拥塞避免、快速恢复，下面我们依次来看一下慢启动当一条 TCP 开始建立连接时，cwnd 的值就会初始化为一个 MSS 的较小值。这就使得初始发送速率大概是 MSS/RTT 字节/秒 ，比如要传输 1000 字节的数据，RTT 为 200 ms ，那么得到的初始发送速率大概是 40 kb/s 。实际情况下可用带宽要比这个 MSS/RTT 大得多，因此 TCP 想要找到最佳的发送速率，可以通过 慢启动(slow-start) 的方式，在慢启动的方式中，cwnd 的值会初始化为 1 个 MSS，并且每次传输报文确认后就会增加一个 MSS，cwnd 的值会变为 2 个 MSS，这两个报文段都传输成功后每个报文段 + 1，会变为 4 个 MSS，依此类推，每成功一次 cwnd 的值就会翻倍。如下图所示发送速率不可能会一直增长，增长总有结束的时候，那么何时结束呢？慢启动通常会使用下面这几种方式结束发送速率的增长。 如果在慢启动的发送过程出现丢包的情况，那么 TCP 会将发送方的 cwnd 设置为 1 并重新开始慢启动的过程，此时会引入一个 ssthresh(慢启动阈值) 的概念，它的初始值就是产生丢包的 cwnd 的值 / 2，即当检测到拥塞时，ssthresh 的值就是窗口值的一半。 第二种方式是直接和 ssthresh 的值相关联，因为当检测到拥塞时，ssthresh 的值就是窗口值的一半，那么当 cwnd &gt; ssthresh 时，每次翻番都可能会出现丢包，所以最好的方式就是 cwnd 的值 = ssthresh ，这样 TCP 就会转为拥塞控制模式，结束慢启动。 慢启动结束的最后一种方式就是如果检测到 3 个冗余 ACK，TCP 就会执行一种快速重传并进入恢复状态。拥塞避免当 TCP 进入拥塞控制状态后，cwnd 的值就等于拥塞时值的一半，也就是 ssthresh 的值。所以，无法每次报文段到达后都将 cwnd 的值再翻倍。而是采用了一种相对保守的方式，每次传输完成后只将 cwnd 的值增加一个 MSS，比如收到了 10 个报文段的确认，但是 cwnd 的值只增加一个 MSS。这是一种线性增长模式，它也会有增长逾值，它的增长逾值和慢启动一样，如果出现丢包，那么 cwnd 的值就是一个 MSS，ssthresh 的值就等于 cwnd 的一半；或者是收到 3 个冗余的 ACK 响应也能停止 MSS 增长。如果 TCP 将 cwnd 的值减半后，仍然会收到 3 个冗余 ACK，那么就会将 ssthresh 的值记录为 cwnd 值的一半，进入 快速恢复 状态。快速恢复在快速恢复中，对于使 TCP 进入快速恢复状态缺失的报文段，对于每个收到的冗余 ACK，cwnd 的值都会增加一个 MSS 。当对丢失报文段的一个 ACK 到达时，TCP 在降低 cwnd 后进入拥塞避免状态。如果在拥塞控制状态后出现超时，那么就会迁移到慢启动状态，cwnd 的值被设置为 1 个 MSS，ssthresh 的值设置为 cwnd 的一半。" }, { "title": "计算机网络传输层协议(四)", "url": "/posts/NetWorkProtocol4/", "categories": "计算机网络", "tags": "学习", "date": "2023-11-12 11:12:00 +0000", "snippet": "传输层协议这是计算机网络传输层的第四篇文章。TCP 状态转换我们上面聊到了三次握手和四次挥手，提到了一些关于 TCP 连接之间的状态转换，那么下面我就从头开始和你好好梳理一下这些状态之间的转换。首先第一步，刚开始时服务器和客户端都处于 CLOSED 状态，这时需要判断是主动打开还是被动打开，如果是主动打开，那么客户端向服务器发送 SYN 报文，此时客户端处于 SYN-SEND 状态，SYN-...", "content": "传输层协议这是计算机网络传输层的第四篇文章。TCP 状态转换我们上面聊到了三次握手和四次挥手，提到了一些关于 TCP 连接之间的状态转换，那么下面我就从头开始和你好好梳理一下这些状态之间的转换。首先第一步，刚开始时服务器和客户端都处于 CLOSED 状态，这时需要判断是主动打开还是被动打开，如果是主动打开，那么客户端向服务器发送 SYN 报文，此时客户端处于 SYN-SEND 状态，SYN-SEND 表示发送连接请求后等待匹配的连接请求，服务器被动打开会处于 LISTEN 状态，用于监听 SYN 报文。如果客户端调用了 close 方法或者经过一段时间没有操作，就会重新变为 CLOSED 状态，这一步转换图如下这里有个疑问，为什么处于 LISTEN 状态下的客户端还会发送 SYN 变为 SYN_SENT 状态呢？知乎看到了车小胖大佬的回答，这种情况可能出现在 FTP 中，LISTEN -&gt; SYN_SENT 是因为这个连接可能是由于服务器端的应用有数据发送给客户端所触发的，客户端被动接受连接，连接建立后，开始传输文件。也就是说，处于 LISTEN 状态的服务器也是有可能发送 SYN 报文的，只不过这种情况非常少见。处于 SYN_SEND 状态的服务器会接收 SYN 并发送 SYN 和 ACK 转换成为 SYN_RCVD 状态，同样的，处于 LISTEN 状态的客户端也会接收 SYN 并发送 SYN 和 ACK 转换为 SYN_RCVD 状态。如果处于 SYN_RCVD 状态的客户端收到 RST 就会变为 LISTEN 状态。这两张图一起看会比较好一些。这里需要解释下什么是 RST。这里有一种情况是当主机收到 TCP 报文段后，其 IP 和端口号不匹配的情况。假设客户端主机发送一个请求，而服务器主机经过 IP 和端口号的判断后发现不是给这个服务器的，那么服务器就会发出一个 RST 特殊报文段给客户端。因此，当服务端发送一个 RST 特殊报文段给客户端的时候，它就会告诉客户端没有匹配的套接字连接，请不要再继续发送了。RST：（Reset the connection）用于复位因某种原因引起出现的错误连接，也用来拒绝非法数据和请求。如果接收到 RST 位时候，通常发生了某些错误。上面没有识别正确的 IP 端口是一种导致 RST 出现的情况，除此之外，RST 还可能由于请求超时、取消一个已存在的连接等出现。位于 SYN_RCVD 的服务器会接收 ACK 报文，SYN_SEND 的客户端会接收 SYN 和 ACK 报文，并发送 ACK 报文，由此，客户端和服务器之间的连接就建立了。这里还要注意一点，同时打开的状态我在上面没有刻意表示出来，实际上，在同时打开的情况下，它的状态变化是这样的。为什么会是这样呢？因为你想，在同时打开的情况下，两端主机都发起 SYN 报文，而主动发起 SYN 的主机会处于 SYN-SEND 状态，发送完成后，会等待接收 SYN 和 ACK ， 在双方主机都发送了 SYN + ACK 后，双方都处于 SYN-RECEIVED(SYN-RCVD) 状态，然后等待 SYN + ACK 的报文到达后，双方就会处于 ESTABLISHED 状态，开始传输数据。好了，到现在为止，我给你叙述了一下 TCP 连接建立过程中的状态转换，现在你可以泡一壶茶喝点水，等着数据传输了。好了，现在水喝够了，这时候数据也传输完成了，数据传输完成后，这条 TCP 连接就可以断开了。现在我们把时钟往前拨一下，调整到服务端处于 SYN_RCVD 状态的时刻，因为刚收到了 SYN 包并发送了 SYN + ACK 包，此时服务端很开心，但是这时，服务端应用进程关闭了，然后应用进程发了一个 FIN 包，就会让服务器从 SYN_RCVD -&gt; FIN_WAIT_1 状态。然后把时钟调到现在，客户端和服务器现在已经传输完数据了 ，此时客户端发送了一条 FIN 报文希望断开连接，此时客户端也会变为 FIN_WAIT_1 状态，对于服务器来说，它接收到了 FIN 报文段并回复了 ACK 报文，就会从 ESTABLISHED -&gt; CLOSE_WAIT 状态。位于 CLOSE_WAIT 状态的服务端会发送 FIN 报文，然后把自己置于 LAST_ACK 状态。处于 FIN_WAIT_1 的客户端接收 ACK 消息就会变为 FIN_WAIT_2 状态。这里需要先解释一下 CLOSING 这个状态，FIN_WAIT_1 -&gt; CLOSING 的转换比较特殊CLOSING 这种状态比较特殊，实际情况中应该是很少见，属于一种比较罕见的例外状态。正常情况下，当你发送FIN 报文后，按理来说是应该先收到（或同时收到）对方的 ACK 报文，再收到对方的 FIN 报文。但是 CLOSING 状态表示你发送 FIN 报文后，并没有收到对方的 ACK 报文，反而却也收到了对方的 FIN 报文。什么情况下会出现此种情况呢？其实细想一下，也不难得出结论：那就是如果双方在同时关闭一个链接的话，那么就出现了同时发送 FIN 报文的情况，也即会出现 CLOSING 状态，表示双方都正在关闭连接。FIN_WAIT_2 状态的客户端接收服务端主机发送的 FIN + ACK 消息，并发送 ACK 响应后，会变为 TIME_WAIT 状态。处于 CLOSE_WAIT 的客户端发送 FIN 会处于 LAST_ACK 状态。这里不少图和博客虽然在图上画的是 FIN + ACK 报文后才会处于 LAST_ACK 状态，但是描述的时候，一般通常只对于 FIN 进行描述。也就是说 CLOSE_WAIT 发送 FIN 才会处于 LAST_ACK 状态。所以这里 FIN_WAIT_1 -&gt; TIME_WAIT 的状态也就是接收 FIN 和 ACK 并发送 ACK 之后，客户端处于的状态。然后位于 CLOSINIG 状态的客户端这时候还有 ACK 接收的话，会继续处于 TIME_WAIT 状态，可以看到，TIME_WAIT 状态相当于是客户端在关闭前的最后一个状态，它是一种主动关闭的状态；而 LAST_ACK 是服务端在关闭前的最后一个状态，它是一种被动打开的状态。TCP 超时和重传没有永远不出错误的通信，这句话表明着不管外部条件多么完备，永远都会有出错的可能。所以，在 TCP 的正常通信过程中，也会出现错误，这种错误可能是由于数据包丢失引起的，也可能是由于数据包重复引起的，甚至可能是由于数据包失序 引起的。TCP 的通信过程中，会由 TCP 的接收端返回一系列的确认信息来判断是否出现错误，一旦出现丢包等情况，TCP 就会启动重传操作，重传尚未确认的数据。TCP 的重传有两种方式，一种是基于时间，一种是基于确认信息，一般通过确认信息要比通过时间更加高效。所以从这点就可以看出，TCP 的确认和重传，都是基于数据包是否被确认为前提的。TCP 在发送数据时会设置一个定时器，如果在定时器指定的时间内未收到确认信息，那么就会触发相应的超时或者基于计时器的重传操作，计时器超时通常被称为重传超时(RTO)。但是有另外一种不会引起延迟的方式，这就是快速重传。TCP 在每次重传一次报文后，其重传时间都会加倍，这种”间隔时间加倍”被称为二进制指数补偿(binary exponential backoff) 。等到间隔时间加倍到 15.5 min 后，客户端会显示Connection closed by foreign host.TCP 拥有两个阈值来决定如何重传一个报文段，这两个阈值被定义在 RFC[RCF1122] 中，第一个阈值是 R1，它表示愿意尝试重传的次数，阈值 R2 表示 TCP 应该放弃连接的时间。R1 和 R2 应至少设为三次重传和 100 秒放弃 TCP 连接。这里需要注意下，对连接建立报文 SYN 来说，它的 R2 至少应该设置为 3 分钟，但是在不同的系统中，R1 和 R2 值的设置方式也不同。在 Linux 系统中，R1 和 R2 的值可以通过应用程序来设置，或者是修改 net.ipv4.tcp_retries1 和 net.ipv4.tcp_retries2 的值来设置。变量值就是重传次数。tcp_retries2 的默认值是 15，这个充实次数的耗时大约是 13 - 30 分钟，这只是一个大概值，最终耗时时间还要取决于 RTO ，也就是重传超时时间。tcp_retries1 的默认值是 3 。对于 SYN 段来说，net.ipv4.tcp_syn_retries 和 net.ipv4.tcp_synack_retries 这两个值限制了 SYN 的重传次数，默认是 5，大约是 180 秒。Windows 操作系统下也有 R1 和 R2 变量，它们的值被定义在下方的注册表中HKLM\\System\\CurrentControlSet\\Services\\Tcpip\\ParametersHKLM\\System\\CurrentControlSet\\Services\\Tcpip6\\Parameters其中有一个非常重要的变量就是 TcpMaxDataRetransmissions，这个 TcpMaxDataRetransmissions 对应 Linux 中的 tcp_retries2 变量，默认值是 5。这个值的意思表示的是 TCP 在现有连接上未确认数据段的次数。快速重传我们上面提到了快速重传，实际上快速重传机制是基于接收端的反馈信息来触发的，它并不受重传计时器的影响。所以与超时重传相比，快速重传能够有效的修复丢包情况。当 TCP 连接的过程中接收端出现乱序的报文（比如 2 - 4 - 3）到达时，TCP 需要立刻生成确认消息，这种确认消息也被称为重复 ACK。当失序报文到达时，重复 ACK 要做到立刻返回，不允许延迟发送，此举的目的是要告诉发送方某段报文失序到达了，希望发送方指出失序报文段的序列号。还有一种情况也会导致重复 ACK 发给发送方，那就是当前报文段的后续报文发送至接收端，由此可以判断当前发送方的报文段丢失或者延迟到达。因为这两种情况导致的后果都是接收方没有收到报文，但是我们却无法判断到底是报文段丢失还是报文段没有送达。因此 TCP 发送端会等待一定数目的重复 ACK 被接受来决定数据是否丢失并触发快速重传。一般这个判断的数量是 3，这段文字表述可能无法清晰理解，我们举个例子。如上图所示，报文段 1 成功接收并被确认为 ACK 2，接收端的期待序号为 2，当报文段 2 丢失后，报文段 3。失序到达，但是与接收端的期望不匹配，所以接收端会重复发送冗余 ACK 2。这样，在超时重传定时器到期之前，接收收到连续三个相同的 ACK 后，发送端就知道哪个报文段丢失了，于是发送方会重发这个丢失的报文段，这样就不用等待重传定时器的到期，大大提高了效率。SACK在标准的 TCP 确认机制中，如果发送方发送了 0 - 10000 序号之间的数据，但是接收方只接收到了 0 -1000, 3000 - 10000 之间的数据，而 1000 - 3000 之间的数据没有到达接收端，此时发送方会重传 1000 - 10000 之间的数据，实际上这是没有必要的，因为 3000 后面的数据已经被接收了。但是发送方无法感知这种情况的存在。如何避免或者说解决这种问题呢？为了优化这种情况，我们有必要让客户端知道更多的消息，在 TCP 报文段中，有一个 SACK 选项字段，这个字段是一种选择性确认(selective acknowledgment)机制，这个机制能告诉 TCP 客户端，用我们的俗语来解释就是：“我这里最多允许接收 1000 之后的报文段，但是我却收到了 3000 - 10000 的报文段，请给我 1000 - 3000 之间的报文段”。但是，这个选择性确认机制的是否开启还受一个字段的影响，这个字段就是 SACK 允许选项字段，通信双方在 SYN 段或者 SYN + ACK 段中添加 SACK 允许选项字段来通知对端主机是否支持 SACK，如果双方都支持的话，后续在 SYN 段中就可以使用 SACK 选项了。这里需要注意下：SACK 选项字段只能出现在 SYN 段中。伪超时和重传在某些情况下，即使没有出现报文段的丢失也可能会引发报文重传。这种重传行为被称为 伪重传(spurious retransmission) ，这种重传是没有必要的，造成这种情况的因素可能是由于伪超时(spurious timeout)，伪超时的意思就是过早的判定超时发生。造成伪超时的因素有很多，比如报文段失序到达，报文段重复，ACK 丢失等情况。检测和处理伪超时的方法有很多，这些方法统称为检测算法和响应算法。检测算法用于判断是否出现了超时现象或出现了计时器的重传现象。一旦出现了超时或者重传的情况，就会执行响应算法撤销或者减轻超时带来的影响，下面是几种算法，此篇文章暂不深入这些实现细节 重复 SACK 扩展- DSACK Eifel 检测算法 前移 RTO 恢复 - F-RTO Eifel 响应算法包失序和包重复上面我们讨论的都是 TCP 如何处理丢包的问题，我们下面来讨论一下包失序和包重复的问题。包失序数据包的失序到达是互联网中极其容易出现的一种情况，由于 IP 层并不能保证数据包的有序性，每个数据包的发送都可能会选择当前情况传输速度最快的链路，所以很有可能出现发送了 A - &gt; B -&gt; C 的三个数据包，到达接收端的数据包顺序是 C -&gt; A -&gt; B 或者 B -&gt; C -&gt; A 等等。这就是包失序的一种现象。在包传输中，主要分为两种链路：正向链路（SYN）和反向链路（ACK）如果失序发生在正向链路，TCP 是无法正确判断数据包是否丢失的，数据的丢失和失序都会导致接收端收到无序的数据包，造成数据之间的空缺。如果这种空缺不够大的话，这种情况影响不大；但是如果空缺比较大的话，可能会导致伪重传。如果失序发生在反向链路，就会使 TCP 的窗口前移，然后收到重复而应该被丢弃的 ACK，导致发送端出现不必要的流量突发，影响可用网络带宽。回到我们上面讨论的快速重传，由于快速重传是根据重复 ACK 推断出现丢包而启动的，它不用等到重传计时器超时。由于 TCP 接收端会对接收到的失序报文立刻返回 ACK，所以网络中任何一个失序到达的报文都可能会造成重复 ACK。假设一旦收到 ACK，就会启动快速重传机制，当 ACK 数量激增，就会导致大量不必要的重传发生，所以快速重传应该达到重复阈值(dupthresh) 再触发。但是在互联网中，严重的失序并不常见，因此 dupthresh 的值可以设置的尽量小，一般来说 3 就能处理绝大部分情况。包重复包重复也是互联网中出现很少的一种情况，它指的是在网络传输过程中，包可能会出现传输多次的情况，当重传生成时，TCP 可能会出现混淆。包的重复可以使接收端生成一系列的重复 ACK，这种情况可以使用 SACK 协商来解决。" }, { "title": "计算机网络传输层协议(三)", "url": "/posts/NetWorkProtocol3/", "categories": "计算机网络", "tags": "学习", "date": "2023-11-05 08:59:00 +0000", "snippet": "传输层协议这是计算机网络传输层的第三篇文章。连接管理在继续介绍下面有意思的特性之前，我们先来把关注点放在 TCP 的连接管理上，因为没有 TCP 连接，也就没有后续的一系列 TCP 特性什么事儿了。假设运行在一台主机上的进程想要和另一台主机上的进程建立 TCP 连接，会经过如下步骤：我们假设此时有一台客户端主机和一台服务端主机进行通信。 首先，客户端首先向服务器发送一个特殊的 TC...", "content": "传输层协议这是计算机网络传输层的第三篇文章。连接管理在继续介绍下面有意思的特性之前，我们先来把关注点放在 TCP 的连接管理上，因为没有 TCP 连接，也就没有后续的一系列 TCP 特性什么事儿了。假设运行在一台主机上的进程想要和另一台主机上的进程建立 TCP 连接，会经过如下步骤：我们假设此时有一台客户端主机和一台服务端主机进行通信。 首先，客户端首先向服务器发送一个特殊的 TCP 报文段。这个报文段首部不包含数据，但是在报文段的首部中有一个 SYN 标志位被置为 1，这个报文段也可以叫做 SYN 报文段。客户端主机随机选择一个初始序列号(client_isn) ，并将此数字放入初始 TCP SYN 段的序列号字段中发送给服务器。 一旦此报文到达服务器后，服务器会从报文中提取 TCP SYN 段，将 TCP 缓冲区和变量进行分配，然后给客户端回送一个报文段，这个报文段也不包含任何数据，只做通知的作用。不过它却包含了三个非常重要的信息。 这些缓冲区和变量的分配使 TCP 容易受到称为 SYN 泛洪的拒绝服务攻击。 首先，SYN 比特被置为 1 。 然后，TCP 报文段的首部确认号被设置为 client_isn + 1，也就是 ACK = SYN + 1。 最后，服务器选择自己的初始序号(server_isn) SYN ，并将其放置到 TCP 报文段首部的序号字段中。 如果用大白话解释下就是，我收到了你发起建立连接的 SYN 报文段，这个报文段具有首部字段 client_isn。我同意建立该连接，我自己的初始序号是 server_isn。这个允许连接的报文段被称为 SYNACK 报文段。 第三步，在收到 SYNACK 报文段后，客户端也要为该连接分配缓冲区和变量。客户端向服务器发送另外一个报文段，最后一个报文段对服务器发送的响应报文做了确认，确认的标准是客户端发送的数据段中确认号为 server_isn + 1，因为连接已经建立，所以 SYN 比特被置为 0 。以上就是 TCP 建立连接的三次数据段发送过程，也被称为 三次握手。一旦完成这三个步骤，客户和服务器主机就可以相互发送报文段了，在以后的每一个报文段中，SYN 比特都被置为 0 ，整个过程描述如下图所示在客户端主机和服务端主机建立连接后，参与一条 TCP 连接的两个进程中的任何一个都能终止 TCP 连接。连接结束后，主机中的缓存和变量将会被释放。假设客户端主机想要终止 TCP 连接，它会经历如下过程：客户应用进程发出一个关闭命令，客户 TCP 向服务器进程发送一个特殊的 TCP 报文段，这个特殊的报文段的首部标志 FIN 被设置为 1 。当服务器收到这个报文段后，就会向发送方发送一个确认报文段。然后，服务器发送它自己的终止报文段，FIN 位被设置为 1 。客户端对这个终止报文段进行确认。此时，在两台主机上用于该连接的所有资源都被释放了，如下图所示在一个 TCP 连接的生命周期内，运行在每台主机中的 TCP 协议都会在各种 TCP 状态(TCP State) 之间进行变化，TCP 的状态主要有 LISTEN、SYN-SEND、SYN-RECEIVED、ESTABLISHED、FIN-WAIT-1、FIN-WAIT-2、CLOSE-WAIT、CLOSING、LAST-ACK、TIME-WAIT 和 CLOSED 。这些状态的解释如下 LISTEN: 表示等待任何来自远程 TCP 和端口的连接请求。 SYN-SEND: 表示发送连接请求后等待匹配的连接请求。 SYN-RECEIVED: 表示已接收并发送连接请求后等待连接确认，也就是 TCP 三次握手中第二步后服务端的状态 ESTABLISHED: 表示已经连接已经建立，可以将应用数据发送给其他主机上面这四种状态是 TCP 三次握手所涉及的。 FIN-WAIT-1: 表示等待来自远程 TCP 的连接终止请求，或者等待先前发送的连接终止请求的确认。 FIN-WAIT-2: 表示等待来自远程 TCP 的连接终止请求。 CLOSE-WAIT: 表示等待本地用户的连接终止请求。 CLOSING: 表示等待来自远程 TCP 的连接终止请求确认。 LAST-ACK: 表示等待先前发送给远程 TCP 的连接终止请求的确认（包括对它的连接终止请求的确认）。 TIME-WAIT: 表示等待足够的时间以确保远程 TCP 收到其连接终止请求的确认。 CLOSED: 表示连接已经关闭，无连接状态。上面 7 种状态是 TCP 四次挥手，也就是断开链接所设计的。TCP 的连接状态会进行各种切换，这些 TCP 连接的切换是根据事件进行的，这些事件由用户调用：OPEN、SEND、RECEIVE、CLOSE、ABORT 和 STATUS。涉及到 TCP 报文段的标志有 SYN、ACK、RST 和 FIN ，当然，还有超时。我们下面加上 TCP 连接状态后，再来看一下三次握手和四次挥手的过程。三次握手建立连接下图画出了 TCP 连接建立的过程。假设图中左端是客户端主机，右端是服务端主机，一开始，两端都处于CLOSED（关闭）状态。 服务端进程准备好接收来自外部的 TCP 连接，一般情况下是调用 bind、listen、socket 三个函数完成。这种打开方式被认为是 被动打开(passive open)。然后服务端进程处于 LISTEN 状态，等待客户端连接请求。 客户端通过 connect 发起主动打开(active open)，向服务器发出连接请求，请求中首部同步位 SYN = 1，同时选择一个初始序号 sequence ，简写 seq = x。SYN 报文段不允许携带数据，只消耗一个序号。此时，客户端进入 SYN-SEND 状态。 服务器收到客户端连接后，，需要确认客户端的报文段。在确认报文段中，把 SYN 和 ACK 位都置为 1 。确认号是 ack = x + 1，同时也为自己选择一个初始序号 seq = y。请注意，这个报文段也不能携带数据，但同样要消耗掉一个序号。此时，TCP 服务器进入 SYN-RECEIVED(同步收到) 状态。 客户端在收到服务器发出的响应后，还需要给出确认连接。确认连接中的 ACK 置为 1 ，序号为 seq = x + 1，确认号为 ack = y + 1。TCP 规定，这个报文段可以携带数据也可以不携带数据，如果不携带数据，那么下一个数据报文段的序号仍是 seq = x + 1。这时，客户端进入 ESTABLISHED (已连接) 状态 服务器收到客户的确认后，也进入 ESTABLISHED 状态。TCP 建立一个连接需要三个报文段，释放一个连接却需要四个报文段。四次挥手数据传输结束后，通信的双方可以释放连接。数据传输结束后的客户端主机和服务端主机都处于 ESTABLISHED 状态，然后进入释放连接的过程。TCP 断开连接需要历经的过程如下： 客户端应用程序发出释放连接的报文段，并停止发送数据，主动关闭 TCP 连接。客户端主机发送释放连接的报文段，报文段中首部 FIN 位置为 1 ，不包含数据，序列号位 seq = u，此时客户端主机进入 FIN-WAIT-1(终止等待 1) 阶段。 服务器主机接受到客户端发出的报文段后，即发出确认应答报文，确认应答报文中 ACK = 1，生成自己的序号位 seq = v，ack = u + 1，然后服务器主机就进入 CLOSE-WAIT(关闭等待) 状态，这个时候客户端主机 -&gt; 服务器主机这条方向的连接就释放了，客户端主机没有数据需要发送，此时服务器主机是一种半连接的状态，但是服务器主机仍然可以发送数据。 客户端主机收到服务端主机的确认应答后，即进入 FIN-WAIT-2(终止等待2) 的状态。等待客户端发出连接释放的报文段。 当服务器主机没有数据发送后，应用进程就会通知 TCP 释放连接。这时服务端主机会发出断开连接的报文段，报文段中 ACK = 1，序列号 seq = w，因为在这之间可能已经发送了一些数据，所以 seq 不一定等于 v + 1。ack = u + 1，在发送完断开请求的报文后，服务端主机就进入了 LAST-ACK(最后确认)的阶段。 客户端收到服务端的断开连接请求后，客户端需要作出响应，客户端发出断开连接的报文段，在报文段中，ACK = 1, 序列号 seq = u + 1，因为客户端从连接开始断开后就没有再发送数据，ack = w + 1，然后进入到 TIME-WAIT(时间等待) 状态，请注意，这个时候 TCP 连接还没有释放。必须经过时间等待的设置，也就是 2MSL 后，客户端才会进入 CLOSED 状态，时间 MSL 叫做最长报文段寿命（Maximum Segment Lifetime）。 服务端主要收到了客户端的断开连接确认后，就会进入 CLOSED 状态。因为服务端结束 TCP 连接时间要比客户端早，而整个连接断开过程需要发送四个报文段，因此释放连接的过程也被称为四次挥手。TCP 连接的任意一方都可以发起关闭操作，只不过通常情况下发起关闭连接操作一般都是客户端。然而，一些服务器比如 Web 服务器在对请求作出相应后也会发起关闭连接的操作。TCP 协议规定通过发送一个 FIN 报文来发起关闭操作。所以综上所述，建立一个 TCP 连接需要三个报文段，而关闭一个 TCP 连接需要四个报文段。TCP 协议还支持一种半开启(half-open)状态，虽然这种情况并不多见。TCP 半开启TCP 连接处于半开启的这种状态是因为连接的一方关闭或者终止了这个 TCP 连接却没有通知另一方，也就是说两个人正在微信聊天，YKFIRE 你下线了你不告诉我，我还在跟你侃八卦呢。此时就认为这条连接处于半开启状态。这种情况发生在通信中的一方处于主机崩溃的情况下，人家发送方还是有理由说的啊，你 xxx 的，我电脑死机了我咋告诉你？只要处于半连接状态的一方不传输数据的话，那么是无法检测出来对方主机已经下线的。另外一种处于半开启状态的原因是通信的一方关闭了主机电源 而不是正常关机。这种情况下会导致服务器上有很多半开启的 TCP 连接。TCP 半关闭既然 TCP 支持半开启操作，那么我们可以设想 TCP 也支持半关闭操作。同样的，TCP 半关闭也并不常见。TCP 的半关闭操作是指仅仅关闭数据流的一个传输方向。两个半关闭操作合在一起就能够关闭整个连接。在一般情况下，通信双方会通过应用程序互相发送 FIN 报文段来结束连接，但是在 TCP 半关闭的情况下，应用程序会表明自己的想法：”我已经完成了数据的发送发送，并发送了一个 FIN 报文段给对方，但是我依然希望接收来自对方的数据直到它发送一个 FIN 报文段给我”。 下面是一个 TCP 半关闭的示意图。解释一下这个过程：首先客户端主机和服务器主机一直在进行数据传输，一段时间后，客户端发起了 FIN 报文，要求主动断开连接，服务器收到 FIN 后，回应 ACK ，由于此时发起半关闭的一方也就是客户端仍然希望服务器发送数据，所以服务器会继续发送数据，一段时间后服务器发送另外一条 FIN 报文，在客户端收到 FIN 报文回应 ACK 给服务器后，断开连接。TCP 的半关闭操作中，连接的一个方向被关闭，而另一个方向仍在传输数据直到它被关闭为止。只不过很少有应用程序使用这一特性。同时打开和同时关闭还有一种比较非常规的操作，这就是两个应用程序同时主动打开连接。虽然这种情况看起来不太可能，但是在特定的安排下却是有可能发生的。我们主要讲述这个过程。通信双方在接收到来自对方的 SYN 之前会首先发送一个 SYN，这个场景还要求通信双方都知道对方的 IP 地址 + 端口号。下面是同时打开的例子如上图所示，通信双方都在收到对方报文前主动发送了 SYN 报文，都在收到彼此的报文后回复了一个 ACK 报文。一个同时打开过程需要交换四个报文段，比普通的三次握手增加了一个，由于同时打开没有客户端和服务器一说，所以这里我用了通信双方来称呼。像同时打开一样，同时关闭也是通信双方同时提出主动关闭请求，发送 FIN 报文，下图显示了一个同时关闭的过程。同时关闭过程中需要交换和正常关闭相同数量的报文段，只不过同时关闭不像四次挥手那样顺序进行，而是交叉进行的。聊一聊初始序列号也许是我上面图示或者文字描述的不专业，初始序列号它是有专业术语表示的，初始序列号的英文名称是Initial sequence numbers (ISN)，所以我们上面表示的 seq = v，其实就表示的 ISN。在发送 SYN 之前，通信双方会选择一个初始序列号。初始序列号是随机生成的，每一个 TCP 连接都会有一个不同的初始序列号。RFC 文档指出初始序列号是一个 32 位的计数器，每 4 us（微秒） + 1。因为每个 TCP 连接都是一个不同的实例，这么安排的目的就是为了防止出现序列号重叠的情况。当一个 TCP 连接建立的过程中，只有正确的 TCP 四元组和正确的序列号才会被对方接收。这也反应了 TCP 报文段容易被伪造 的脆弱性，因为只要我伪造了一个相同的四元组和初始序列号就能够伪造 TCP 连接，从而打断 TCP 的正常连接，所以抵御这种攻击的一种方式就是使用初始序列号，另外一种方法就是加密序列号。什么是 TIME-WAIT我上面只是简单提到了一下 TIME-WAIT 状态和 2MSL 是啥，下面来聊一下这两个概念。MSL 是 TCP 报文段可以存活或者驻留在网络中的最长时间。RFC 793 定义了 MSL 的时间是两分钟，但是具体的实现还要根据程序员来指定，一些实现采用了 30 秒的这个最大存活时间。那么为什么要等待 2MSL 呢？主要是因为两个理由 为了保证最后一个响应能够到达服务器，因为在计算机网络中，最后一个 ACK 报文段可能会丢失，从而致使客户端一直处于 LAST-ACK 状态等待客户端响应。这时候服务器会重传一次 FINACK 断开连接报文，客户端接收后再重新确认，重启定时器。如果客户端不是 2MSL ，在客户端发送 ACK 后直接关闭的话，如果报文丢失，那么双方主机会无法进入 CLOSED 状态。 还可以防止已失效的报文段。客户端在发送最后一个 ACK 之后，再经过经过 2MSL，就可以使本链接持续时间内所产生的所有报文段都从网络中消失。从保证在关闭连接后不会有还在网络中滞留的报文段去骚扰服务器。这里注意一点：在服务器发送了 FIN-ACK 之后，会立即启动超时重传计时器。客户端在发送最后一个 ACK 之后会立即启动时间等待计时器。说好的 RST 呢说好的 RST、SYN、FIN 标志用于连接的建立和关闭，那么 SYN 和 FIN 都现身了，那 RST 呢？也是啊，我们上面探讨的都是一种理想的情况，就是客户端服务器双方都会接受传输报文段的情况，还有一种情况是当主机收到 TCP 报文段后，其 IP 和端口号不匹配的情况。假设客户端主机发送一个请求，而服务器主机经过 IP 和端口号的判断后发现不是给这个服务器的，那么服务器就会发出一个 RST 特殊报文段给客户端。因此，当服务端发送一个 RST 特殊报文段给客户端的时候，它就会告诉客户端没有匹配的套接字连接，请不要再继续发送了。上面探讨的是 TCP 的情况，那么 UDP 呢？使用 UDP 作为传输协议后，如果套接字不匹配的话，UDP 主机就会发送一个特殊的 ICMP 数据报。SYN 洪泛攻击下面我们来讨论一下什么是 SYN 洪泛攻击。我们在 TCP 的三次握手中已经看到，服务器为了响应一个收到的 SYN，分配并初始化变量连接和缓存，然后服务器发送一个 SYNACK 作为响应，然后等待来自于客户端的 ACK 报文。如果客户端不发送 ACK 来完成最后一步的话，那么这个连接就处在一个挂起的状态，也就是半连接状态。攻击者通常在这种情况下发送大量的 TCP SYN 报文段，服务端继续响应，但是每个连接都完不成三次握手的步骤。随着 SYN 的不断增加，服务器会不断的为这些半开连接分配资源，导致服务器的连接最终被消耗殆尽。这种攻击也是属于 Dos 攻击的一种。抵御这种攻击的方式是使用 SYN cookie ，下面是它的工作流程介绍 当服务器收到一个 SYN 报文段时，它并不知道这个报文段是来自哪里，是来自攻击者主机还是客户端主机(虽然攻击者也是客户端，不过这么说更便于区分) 。因此服务器不会为报文段生成一个半开连接。与此相反，服务器生成一个初始的 TCP 序列号，这个序列号是 SYN 报文段的源和目的 IP 地址与端口号这个四元组构造的一个复杂的散列函数，这个散列函数生成的 TCP 序列号就是 SYN Cookie，用于缓存 SYN 请求。然后，服务器会发送带着 SYN Cookie 的 SYNACK 分组。有一点需要注意的是，服务器不会记忆这个 Cookie 或 SYN 的其他状态信息。 如果客户端不是攻击者的话，它就会返回一个 ACK 报文段。当服务器收到这个 ACK 后，需要验证这个 ACK 与 SYN 发送的是否相同，验证的标准就是确认字段中的确认号和序列号，源和目的 IP 地址与端口号以及和散列函数的是否一致，散列函数的结果 + 1 是否和 SYNACK 中的确认值相同。(大致是这样，说的不对还请读者纠正) 。如果有兴趣读者可以自行深入了解。如果是合法的，服务器就会生成一个具有套接字的全开连接。 如果客户端没有返回 ACK，即认为是攻击者，那么这样也没关系，服务器没有收到 ACK，不会分配变量和缓存资源，不会对服务器产生危害。" }, { "title": "计算机网络传输层协议(二)", "url": "/posts/NetWorkProtocol2/", "categories": "计算机网络", "tags": "学习", "date": "2023-10-29 07:36:00 +0000", "snippet": "传输层协议这是计算机网络传输层的第二篇。UDPUDP 的全称是用户数据报协议(UDP，User Datagram Protocol)，UDP 为应用程序提供了一种无需建立连接就可以封装并发送 IP 数据包的方法。如果应用程序开发人员选择的是 UDP 而不是 TCP 的话，那么该应用程序相当于就是和 IP 直接打交道的。从应用程序传递过来的数据，会附加上多路复用/多路分解的源和目的端口号字段，...", "content": "传输层协议这是计算机网络传输层的第二篇。UDPUDP 的全称是用户数据报协议(UDP，User Datagram Protocol)，UDP 为应用程序提供了一种无需建立连接就可以封装并发送 IP 数据包的方法。如果应用程序开发人员选择的是 UDP 而不是 TCP 的话，那么该应用程序相当于就是和 IP 直接打交道的。从应用程序传递过来的数据，会附加上多路复用/多路分解的源和目的端口号字段，以及其他字段，然后将形成的报文传递给网络层，网络层将运输层报文段封装到 IP 数据报中，再交付给目标主机。UDP 特点UDP 协议一般作为流媒体应用、语音交流、视频会议所使用的传输层协议，我们大家都熟知的 DNS 协议底层也使用了 UDP 协议，这些应用或协议之所以选择 UDP 主要是因为以下这几点 速度快，采用 UDP 协议时，只要应用进程将数据传给 UDP，UDP 就会将此数据打包进 UDP 报文段并立刻传递给网络层，然后 TCP 有拥塞控制的功能，它会在发送前判断互联网的拥堵情况，如果互联网极度阻塞，那么就会抑制 TCP 的发送方。使用 UDP 的目的就是希望实时性。 无须建立连接，TCP 在数据传输之前需要经过三次握手的操作，而 UDP 则无须任何准备即可进行数据传输。因此 UDP 没有建立连接的时延。如果使用 TCP 和 UDP 来比喻开发人员：TCP 就是那种凡事都要设计好，没设计不会进行开发的工程师，需要把一切因素考虑在内后再开干！所以非常靠谱；而 UDP 就是那种上来直接干干干，接到项目需求马上就开干，也不管设计，也不管技术选型，就是干，这种开发人员非常不靠谱，但是适合快速迭代开发，因为可以马上上手！ 无连接状态，TCP 需要在端系统中维护连接状态，连接状态包括接收和发送缓存、拥塞控制参数以及序号和确认号的参数，在 UDP 中没有这些参数，也没有发送缓存和接受缓存。因此，某些专门用于某种特定应用的服务器当应用程序运行在 UDP 上，一般能支持更多的活跃用户。 分组首部开销小，每个 TCP 报文段都有 20 字节的首部开销，而 UDP 仅仅只有 8 字节的开销。 这里需要注意一点，并不是所有使用 UDP 协议的应用层都是不可靠的，应用程序可以自己实现可靠的数据传输，通过增加确认和重传机制。所以使用 UDP 协议最大的特点就是速度快。UDP 报文结构下面来一起看一下 UDP 的报文结构，每个 UDP 报文分为 UDP 报头和 UDP 数据区两部分。报头由 4 个 16 位长（2 字节）字段组成，分别说明该报文的源端口、目的端口、报文长度和校验值。 源端口号(Source Port) :这个字段占据 UDP 报文头的前 16 位，通常包含发送数据报的应用程序所使用的 UDP 端口。接收端的应用程序利用这个字段的值作为发送响应的目的地址。这个字段是可选项，有时不会设置源端口号。没有源端口号就默认为 0 ，通常用于不需要返回消息的通信中。 目标端口号(Destination Port): 表示接收端端口，字段长为 16 位。 长度(Length): 该字段占据 16 位，表示 UDP 数据报长度，包含 UDP 报文头和 UDP 数据长度。因为 UDP 报文头长度是 8 个字节，所以这个值最小为 8，最大长度为 65535 字节。 校验和(Checksum)：UDP 使用校验和来保证数据安全性，UDP 的校验和也提供了差错检测功能，差错检测用于校验报文段从源到目标主机的过程中，数据的完整性是否发生了改变。发送方的 UDP 对报文段中的 16 比特字的和进行反码运算，求和时遇到的位溢出都会被忽略，比如下面这个例子，三个 16 比特的数字进行相加 这些16比特的前两个和是然后再将上面的结果和第三个 16 比特的数进行相加最后一次相加的位会进行溢出，溢出位 1 要被舍弃，然后进行反码运算，反码运算就是将所有的 1 变为 0 ，0 变为 1。因此 1000 0100 1001 0101 的反码就是 0111 1011 0110 1010，这就是校验和，如果在接收方，数据没有出现差错，那么全部的 4 个 16 比特的数值进行运算，同时也包括校验和，如果最后结果的值不是 1111 1111 1111 1111 的话，那么就表示传输过程中的数据出现了差错。下面来想一个问题，为什么 UDP 会提供差错检测的功能？这其实是一种端到端的设计原则，这个原则说的是要让传输中各种错误发生的概率降低到一个可以接受的水平。UDP 不可靠的原因是它虽然提供差错检测的功能，但是对于差错没有恢复能力更不会有重传机制。TCPUDP 是一种没有复杂的控制，提供无连接通信服务的一种协议，换句话说，它将部分控制部分交给应用程序去处理，自己只提供作为传输层协议最基本的功能。而与 UDP 不同的是，同样作为传输层协议，TCP 协议要比 UDP 的功能多很多。TCP 的全称是 Transmission Control Protocol，它被称为是一种面向连接的协议，这是因为一个应用程序开始向另一个应用程序发送数据之前，这两个进程必须先进行握手，握手是一个逻辑连接，并不是两个主机之间进行真实的握手。这个连接是指各种设备、线路或者网络中进行通信的两个应用程序为了相互传递消息而专有的、虚拟的通信链路，也叫做虚拟电路。一旦主机 A 和主机 B 建立了连接，那么进行通信的应用程序只使用这个虚拟的通信线路发送和接收数据就可以保证数据的传输，TCP 协议负责控制连接的建立、断开、保持等工作。TCP 连接是全双工服务(full-duplex service) 的，全双工是什么意思？全双工指的是主机 A 与另外一个主机 B 存在一条 TCP 连接，那么应用程数据就可以从主机 B 流向主机 A 的同时，也从主机 A 流向主机 B。一条 TCP 连接只能是点对点(point-to-point)的，那么所谓的多播，即一个主机对多个接收方发送消息的情况是不存在的，TCP 连接只能连接两个一对主机。一旦 TCP 连接建立后，主机之间就可以相互发送数据了，客户进程通过套接字发送数据。数据一旦通过套接字后，它就由客户中运行的 TCP 协议所控制。TCP 会将数据临时存储到连接的发送缓存(send buffer)中，这个 send buffer 是三次握手之间设置的缓存之一，然后 TCP 在合适的时间将发送缓存中的数据发送到目标主机的接收缓存中，实际上，每一端都会有发送缓存和接收缓存，如下图所示机之间的发送是以报文段(segment)进行的，那么什么是 Segement 呢？TCP 会将要传输的数据流分为多个块，然后向每个块中添加 TCP 标头，这样就形成了一个 TCP 段也就是报文段。每一个报文段可以传输的长度是有限的，不能超过最大数据长度(Maximum Segment Size)MSS。在报文段向下传输的过程中，会经过链路层，链路层有一个 Maximum Transmission Unit，最大传输单元 MTU， 即数据链路层上所能通过最大数据包的大小，最大传输单元通常与通信接口有关。因为计算机网络是分层考虑的，这个很重要，不同层的称呼不一样，对于传输层来说，称为报文段而对网络层来说就叫做 IP 数据包，所以，MTU 可以认为是网络层能够传输的最大 IP 数据包，而 MSS（Maximum segment size）可以认为是传输层的概念，也就是 TCP 数据包每次能够传输的最大量。TCP 报文段结构在简单聊了聊 TCP 连接后，下面我们就来聊一下 TCP 的报文段结构，如下图所示TCP 报文段结构相比 UDP 报文结构多了很多内容。但是前两个 32 比特的字段是一样的。它们是源端口号和目标端口号，我们知道，这两个字段是用于多路复用和多路分解的。另外，和 UDP 一样，TCP 也包含校验和，除此之外，TCP 报文段首部还有下面这些 32 比特的序号字段(sequence number field) 和 32 比特的确认号字段(acknowledgment number field)。这些字段被 TCP 发送方和接收方用来实现可靠的数据传输。 4 比特的首部字段长度字段(header length field)，这个字段指示了以 32 比特的字为单位的 TCP 首部长度。TCP 首部的长度是可变的，但是通常情况下，选项字段为空，所以 TCP 首部字段的长度是 20 字节。 16 比特的接受窗口字段(receive window field)，这个字段用于流量控制。它用于指示接收方能够/愿意接受的字节数量 可变的选项字段(options field)，这个字段用于发送方和接收方协商最大报文长度，也就是 MSS 时使用 6 比特的标志字段(flag field)， ACK 标志用于指示确认字段中的值是有效的，这个报文段包括一个对已被成功接收报文段的确认；RST、SYN、FIN 标志用于连接的建立和关闭；CWR 和 ECE 用于拥塞控制；PSH 标志用于表示立刻将数据交给上层处理；URG 标志用来表示数据中存在需要被上层处理的 紧急 数据。紧急数据最后一个字节由 16 比特的紧急数据指针字段(urgeent data pointer field)指出。一般情况下，PSH 和 URG 并没有使用。TCP 的各种功能和特点都是通过 TCP 报文结构来体现的，在聊完 TCP 报文结构之后，我们下面就来聊一下 TCP 有哪些功能及其特点了。序号、确认号实现传输可靠性TCP 报文段首部中两个最重要的字段就是序号和确认号，这两个字段是 TCP 实现可靠性的基础，那么你肯定好奇如何实现可靠性呢？要了解这一点，首先我们得先知道这两个字段里面存了哪些内容吧？一个报文段的序号就是数据流的字节编号 。因为 TCP 会把数据流分割成为一段一段的字节流，因为字节流本身是有序的，所以每一段的字节编号就是标示是哪一段的字节流。比如，主机 A 要给主机 B 发送一条数据。数据经过应用层产生后会有一串数据流，数据流会经过 TCP 分割，分割的依据就是 MSS，假设数据是 10000 字节，MSS 是 2000 字节，那么 TCP 就会把数据拆分成 0 - 1999 , 2000 - 3999 的段，依次类推。所以，第一个数据 0 - 1999 的首字节编号就是 0 ，2000 - 3999 的首字节编号就是 2000 。。。。。。然后，每个序号都会被填入 TCP 报文段首部的序号字段中。至于确认号的话，会比序号要稍微麻烦一些。这里我们先拓展下几种通信模型。 单工通信：单工通信只支持数据在一个方向上传输；在同一时间只有一方能接受或发送信息，不能实现双向通信，比如广播、电视等。 双工通信：由两个或者多个发送方同时在两个方向上通信。双工通信模型有两种：全双工(FDX)和半双工(HDX) 半双工：在半双工系统中，连接双方可以进行通信，但不能同时通信，比如对讲机，只有把按钮按住的人才能够讲话，只有一个人讲完话后另外一个人才能讲话。 全双工：在全双工系统中，连接双方可以同时进行通信，一个最常见的例子就是电话通信。全双工通信是两个单工通信方式的结合，它要求发送设备和接收设备都有独立的接收和发送能力。单工、半双工、全双工通信如下图所示TCP 是一种全双工的通信协议，因此主机 A 在向主机 B 发送消息的过程中，也在接受来自主机 B 的数据。主机 A 填充进报文段的确认号是期望从主机 B 收到的下一字节的序号。稍微有点绕，我们来举个例子看一下。比如主机 A 收到了来自主机 B 发送的编号为 0 - 999 字节的报文段，这个报文段会写入序号中，随后主机 A 期望能够从主机 B 收到 1000 - 剩下的报文段，因此，主机 A 发送到主机 B 的报文段中，它的确认号就是 1000 。累积确认这里再举出一个例子，比如主机 A 在发送 0 - 999 报文段后，期望能够接受到 1000 之后的报文段，但是主机 B 却给主机 A 发送了一个 1500 之后的报文段，那么主机 A 是否还会继续进行等待呢？答案是会的，因为 TCP 只会确认流中至第一个丢失字节为止的字节，因为 1500 虽然属于 1000 之后的字节，但是主机 B 没有给主机 A 发送 1000 - 1499 之间的字节，所以主机 A 会继续等待。在了解完序号和确认号之后，我们下面来聊一下 TCP 的发送过程。下面是一个正常的发送过程主机 A 向主机 B 发送了两段报文，第一段报文是 0 - 999 ，主机 B 接收之后会发送确认应答报文，该报文中包含对主机 A 发送 0 - 999 报文段的确认号，应答报文到达主机 A 之后，经过一段时间主机 A 会发送 1000 - 1999 这段报文，主机 B 对其进行确认后再发送应答报文。通过上图可以看到，每次主机 A 发送完报文段之后，主机 B 都会发送一个应答报文，主机 A 才会发送接下来的报文段，那么这个应答报文是啥呢？实际上，TCP 就是通过确认应答(ACK) 来实现可靠的数据传输，当主机 A将数据发出之后会等待主机 B 的响应。如果有确认应答(ACK)，说明数据已经成功到达。反之，则数据很可能会丢失。如下图所示，如果在一定时间内主机 A 没有等到确认应答，则认为主机 B 发送的报文段已经丢失，并进行重发。主机 A 给主机 B 的响应可能由于网络抖动等原因无法到达，那么在经过特定的时间间隔后，主机 A 将重新发送报文段。主机 A 没有收到主机 B 的应答报文，可能是因为报文段在主机 B 在发送给主机 A 的过程中丢失。如上图所示，由主机 B 返回的确认应答，由于网络拥堵等原因在传送的过程中丢失，并没有到达主机 A。主机 A 会等待一段时间，如果在这段时间内主机 A 仍没有等到主机 B 的响应，那么主机 A 会重新发送报文段。要辩证的看待问题，如果主机 A 没有收到应答报文，不一定是主机 A 发送的报文段丢失，还有可能是主机 B 发送的应答报文丢失，还可能是主机 B 没有发送应答报文，所以没有收到报文的情况有很多种。那么现在就存在一个问题，如果主机 A 给主机 B 发送了一个报文段后，主机 B 接受到报文段发送响应，此刻由于网络原因，这个报文段并未到达，等到一段时间后主机 A 重新发送报文段，然后此时主机 B 发送的响应在主机 A 第二次发送后失序到达主机 A，那么主机 A 应该如何处理呢？TCP RFC 并未为此做任何规定，也就是说，我们可以自己决定如何处理失序到达的报文段。一般处理方式有两种 接收方立刻丢弃失序的报文段。 接收方接受时许到达的报文段，并等待后续的报文段。一般来说通常采取的做法是第二种。传输控制利用窗口控制提高速度前面我们介绍了 TCP 是以数据段的形式进行发送，如果经过一段时间内主机 A 等不到主机 B 的响应，主机 A 就会重新发送报文段，接受到主机 B 的响应，再会继续发送后面的报文段，我们现在看到，这一问一答的方式还存在许多意外条件，比如响应未收到、等待响应等，那么对崇尚性能的互联网来说，这种方式的性能应该不会很高。那么如何提升性能呢？为了解决这个问题，TCP 引入了窗口这个概念，这个窗口大家可以把它理解为发送期，就是说在这个窗口（发送期）中，通信双方可以任意发送数据，也就是说引入窗口后，从之前单次发送变成了一段时间内的多次报文发送。所以，即使在往返时间较长、频次很多的情况下，它也能控制网络性能的下降，如下图所示我们之前每次请求发送都是以报文段的形式进行的，引入窗口后，每次请求都可以发送多个报文段，也就是说一个窗口可以发送多个报文段。窗口大小就是指无需等待确认应答就可以继续发送报文段的最大值。在这个窗口机制中，大量使用了缓冲区的实现方式，通过对多个段同时进行确认应答的功能。如下图所示，发送报文段中高亮部分即是我们提到的窗口，在窗口内，即是没有收到确认应答也可以把请求发送出去。不过，在整个窗口的确认应答没有到达之前，如果部分报文段丢失，那么主机 A 将仍会重传。为此，主机 A 需要设置缓存来保留这些需要重传的报文段，直到收到他们的确认应答。在滑动窗口以外的部分是尚未发送的报文段和已经接受到的报文段，如果报文段已经收到确认则不可进行重发，此时报文段就可以从缓冲区中清除。在收到确认的情况下，会将窗口滑动到确认应答中确认号的位置，如上图所示，这样可以顺序的将多个段同时发送，用以提高通信性能，这种窗口也叫做 滑动窗口(Sliding window)。窗口控制和重发报文段的发送和接收，必然伴随着报文段的丢失和重发，窗口也是同样如此，如果在窗口中报文段发送过程中出现丢失怎么办？首先我们先考虑确认应答没有返回的情况。在这种情况下，主机 A 发送的报文段到达主机 B，是不需要再进行重发的。这和单个报文段的发送不一样，如果发送单个报文段，即使确认应答没有返回，也要进行重发。窗口在一定程度上比较大时，即使有少部分确认应答的丢失，也不会重新发送报文段。我们知道，如果在某个情况下由于发送的报文段丢失，导致接受主机未收到请求，或者主机返回的响应未到达客户端的话，会经过一段时间重传报文。那么在使用窗口的情况下，报文段丢失会怎么样呢？如下图所示，报文段 0 - 999 丢失后，但是主机 A 并不会等待，主机 A 会继续发送余下的报文段，主机 B 发送的确认应答却一直是 1000，同一个确认号的应答报文会被持续不断的返回，如果发送端主机在连续 3 次收到同一个确认应答后，就会将其所对应的数据重发，这种机制要比之前提到的超时重发更加高效，这种机制也被称为高速重发控制。这种重发的确认应答也被称为冗余 ACK(响应)。主机 B 在没有接收到自己期望序列号的报文段时，会对之前收到的数据进行确认应答。发送端则一旦收到某个确认应答后，又连续三次收到同样的确认应答，那么就会认为报文段已经丢失。需要进行重发。使用这种机制可以提供更为快速的重发服务。流量控制我们知道，在每个 TCP 连接的一侧主机都会有一个 socket 缓冲区，缓冲区会为每个连接设置接收缓存和发送缓存，当 TCP 建立连接后，从应用程序产生的数据就会到达接收方的接收缓冲区中，接收方的应用程序并不一定会马上读取缓冲区的数据，它需要等待操作系统分配时间片。如果此时发送方的应用程序产生数据过快，而接收方读取接受缓冲区的数据相对较慢的话，那么接收方中缓冲区的数据将会溢出，导致数据丢失。但是还好，TCP 有流量控制服务(flow-control service)机制用于消除缓冲区溢出的情况。流量控制是一个速度匹配服务，即发送方的发送速率与接受方应用程序的读取速率相匹配。TCP 通过使用一个接收窗口(receive window) 的变量来提供流量控制。接收窗口会给发送方一个指示到底还有多少可用的缓存空间。发送端会根据接收端的实际接受能力来控制发送的数据量。接收端向发送端通知自己可以接收数据量的大小，发送端会发送不超过这个限度的数据，这个大小限度就是窗口大小，还记得 TCP 的首部么，有一个接收窗口，我们上面聊的时候说这个字段用于流量控制。它用于指示接收方能够接受的字节数量。那么如何实时知道接收方能够接收的数据量大小呢？发送端主机会定期发送一个窗口探测包，这个包用于探测接收端主机是否还能够接受数据，当接收端的缓冲区一旦面临数据溢出的风险时，窗口大小的值也随之被设置为一个更小的值通知发送端，从而控制数据发送量。下面是一个流量控制示意图发送端主机根据接收端主机的窗口大小进行流量控制。由此也可以防止发送端主机一次发送过大数据导致接收端主机无法处理。如上图所示，当主机 B 收到报文段 2000 - 2999 之后缓冲区已满，不得不暂时停止接收数据。然后主机 A 发送窗口探测包，窗口探测包非常小仅仅一个字节。然后主机 B 更新缓冲区接收窗口大小并发送窗口更新通知给主机 A，然后主机 A 再继续发送报文段。在上面的发送过程中，窗口更新通知可能会丢失，一旦丢失发送端就不会发送数据，所以窗口探测包会随机发送，以避免这种情况发生。" }, { "title": "计算机网络传输层协议(一)", "url": "/posts/NetWorkProtocol/", "categories": "计算机网络", "tags": "学习", "date": "2023-10-22 07:00:00 +0000", "snippet": "传输层协议今天我们主要来聊一下计算机网络传输层的相关知识。传输层位于应用层和网络层之间，是 OSI 分层体系中的第四层，同时也是网络体系结构的重要部分。运输层主要负责网络上的端到端通信。运输层为运行在不同主机上的应用程序之间的通信起着至关重要的作用。下面我们就来一起探讨一下关于运输层的协议部分运输层概述 运输层与传输层的叫法都有计算机网络的运输层非常类似于高速公路，高速公路负责把人或者物品...", "content": "传输层协议今天我们主要来聊一下计算机网络传输层的相关知识。传输层位于应用层和网络层之间，是 OSI 分层体系中的第四层，同时也是网络体系结构的重要部分。运输层主要负责网络上的端到端通信。运输层为运行在不同主机上的应用程序之间的通信起着至关重要的作用。下面我们就来一起探讨一下关于运输层的协议部分运输层概述 运输层与传输层的叫法都有计算机网络的运输层非常类似于高速公路，高速公路负责把人或者物品从一个地方运到另一个地方，而计算机网络的运输层则负责把报文从一端运输到另一端，这个端指的就是端系统。在计算机网络中，任意一个可以交换信息的介质都可以称为端系统，比如手机、网络媒体、电脑、运营商等。在运输层运输报文的过程中，会遵守一定的协议规范，比如一次传输的数据限制、选择什么样的运输协议等。运输层实现了让两个互不相关的端系统进行逻辑通信的功能，看起来像是让两个人面对面对话一样！运输层协议是在端系统中实现的，而不是在路由器中实现的，因为路由器只是做识别转发功能。这也就是说，只有端系统自己知道要把数据包送到哪里！这就比如快递员送快递一样，当然是要由寄快递的人知道要给谁寄快递，快递员才不会管你这个快递是寄给谁的，人家只是负责运输！现在我们可以认为数据包已经发送到了某台计算机中，但是计算机中有很多程序，你这个数据包是发给哪个程序的呢？TCP 如何判断是哪个端口的呢？还记得数据包的结构吗，这里来回顾一下数据包经过每层后，该层协议都会在数据包附上包首部，一个完整的包首部图如上所示。在数据传输到运输层后，会为其附上 TCP 首部，首部包含着源端口号和目的端口号。在发送端，运输层将从发送应用程序进程接收到的报文转化成运输层分组，分组在计算机网络中也称为 报文段(segment)。运输层一般会将报文段进行分割，分割成为较小的块，为每一块加上运输层首部并将其向目的地发送。在发送过程中，可选的运输层协议(也就是交通工具) 主要有 TCP 和 UDP ，所以，关于这两种运输协议的选择及其特性也是我们着重探讨的重点。TCP 和 UDP 前置知识在 TCP/IP 协议中能够实现传输层功能的，最具代表性的就是 TCP 和 UDP。提起 TCP 和 UDP ，就得先从这两个协议的定义说起。TCP 叫做传输控制协议(TCP，Transmission Control Protocol)，通过名称可以大致知道 TCP 协议有控制传输的功能，主要体现在其可控，可控就表示着可靠，确实是这样的，TCP 为应用层提供了一种可靠的、面向连接的服务，它能够将分组可靠的传输到服务端。UDP 叫做 用户数据报协议(UDP，User Datagram Protocol)，通过名称可以知道 UDP 把重点放在了数据报上，它为应用层提供了一种无需建立连接就可以直接发送数据报的方法。怎么计算机网络中的术语对一个数据的描述这么多啊？在计算机网络中，在不同层之间会有不同的描述。我们上面提到会将运输层的分组称为报文段，除此之外，还会将 TCP 中的分组也称为报文段，然而将 UDP 的分组称为数据报，同时也将网络层的分组称为数据报。在数据链路层的数据被称为帧，在物理层都叫比特。但是为了统一，一般在计算机网络中我们统一称 TCP 和 UDP 的报文为 报文段，这个就相当于是约定，到底如何称呼不用过多纠结啦。套接字在 TCP 或者 UDP 发送具体的报文信息前，需要先经过一扇门，这个门就是套接字(socket)，套接字向上连接着应用层，向下连接着网络层。在操作系统中，操作系统分别为应用和硬件提供了接口(Application Programming Interface)。而在计算机网络中，套接字同样是一种接口，它也是有接口 API 的。使用 TCP 或 UDP 通信时，会广泛用到套接字的 API，使用这套 API 设置 IP 地址、端口号，实现数据的发送和接收。现在我们知道了， Socket 和 TCP/IP 没有必然联系，Socket 的出现只是方便了 TCP/IP 的使用，如何方便使用呢？你可以直接使用下面 Socket API 的这些方法。套接字类型套接字的主要类型有三种，下面我们分别介绍一下： 数据报套接字(Datagram sockets)：数据报套接字提供一种无连接的服务，而且并不能保证数据传输的可靠性。数据有可能在传输过程中丢失或出现数据重复，且无法保证顺序地接收到数据。数据报套接字使用UDP( User DatagramProtocol)协议进行数据的传输。由于数据报套接字不能保证数据传输的可靠性，对于有可能出现的数据丢失情况，需要在程序中做相应的处理。 流套接字(Stream sockets)：流套接字用于提供面向连接、可靠的数据传输服务。能够保证数据的可靠性、顺序性。流套接字之所以能够实现可靠的数据服务，原因在于其使用了传输控制协议，即 TCP 协议。 原始套接字(Raw sockets): 原始套接字允许直接发送和接收 IP 数据包，而无需任何特定于协议的传输层格式，原始套接字可以读写内核没有处理过的 IP 数据包。套接字处理过程在计算机网络中，要想实现通信，必须至少需要两个端系统，至少需要一对两个套接字才行。下面是套接字的通信过程。 socket 中的 API 用于创建通信链路中的端点，创建完成后，会返回描述该套接字的套接字描述符。就像使用文件描述符来访问文件一样，套接字描述符用来访问套接字。 当应用程序具有套接字描述符后，它可以将唯一的名称绑定在套接字上，服务器必须绑定一个名称才能在网络中访问。 在为服务端分配了 socket 并且将名称使用 bind 绑定到套接字上后，将会调用 listen api。listen 表示客户端愿意等待连接的意愿，listen 必须在 accept api 之前调用。 客户端应用程序在流套接字(基于 TCP)上调用 connect 发起与服务器的连接请求。 服务器应用程序使用acceptAPI 接受客户端连接请求，服务器必须先成功调用 bind 和 listen 后，再调用 accept api。 在流套接字之间建立连接后，客户端和服务器就可以发起 read/write api 调用了。 当服务器或客户端要停止操作时，就会调用 close API 释放套接字获取的所有系统资源。 虽然套接字 API 位于应用程序层和传输层之间的通信模型中，但是套接字 API 不属于通信模型。套接字 API 允许应用程序与传输层和网络层进行交互。在往下继续聊之前，我们先播放一个小插曲，简单聊一聊 IP聊聊 IPIP 是Internet Protocol（网际互连协议）的缩写，是 TCP/IP 体系中的网络层协议。设计 IP 的初衷主要想解决两类问题： 提高网络扩展性：实现大规模网络互联。 对应用层和链路层进行解藕，让二者独立发展。IP 是整个 TCP/IP 协议族的核心，也是构成互联网的基础。为了实现大规模网络的互通互联，IP 更加注重适应性、简洁性和可操作性，并在可靠性做了一定的牺牲。IP 不保证分组的交付时限和可靠性，所传送分组有可能出现丢失、重复、延迟或乱序等问题。我们知道，TCP 协议的下一层就是 IP 协议层，既然 IP 不可靠，那么如何保证数据能够准确无误地到达呢？这就涉及到 TCP 传输机制的问题了，我们后面聊到 TCP 的时候再说。端口号在聊端口号前，先来聊一聊文件描述以及 socket 和端口号的关系为了方便资源的使用，提高机器的性能、利用率和稳定性等等原因，我们的计算机都有一层软件叫做操作系统，它用于帮我们管理计算机可以使用的资源，当我们的程序要使用某个资源的时候，可以向操作系统申请，再由操作系统为我们的程序分配和管理资源。通常当我们要访问一个内核设备或文件时，程序可以调用系统函数，系统就会为我们打开设备或文件，然后返回一个文件描述符 fd（或称为 ID，是一个整数），我们要访问该设备或文件，只能通过该文件描述符。可以认为该编号对应着打开的文件或设备。而当我们的程序要使用网络时，要使用到对应的操作系统内核的操作和网卡设备，所以我们可以向操作系统申请，然后系统会为我们创建一个套接字 Socket，并返回这个 Socket 的ID，以后我们的程序要使用网络资源，只要向这个 Socket 的编号 ID 操作即可。而我们的每一个网络通信的进程至少对应着一个 Socket。向 Socket 的 ID 中写数据，相当于向网络发送数据，向 Socket 中读数据，相当于接收数据。而且这些套接字都有唯一标识符——文件描述符 fd。端口号是 16 位的非负整数，它的范围是 0 - 65535 ，这个范围会分为三种不同的端口号段，由 Internet 号码分配机构 IANA 进行分配 周知/标准端口号，它的范围是 0 - 1023 注册端口号，范围是 1024 - 49151 私有端口号，范围是 49152 - 65535一台计算机上运行着多个应用程序，当一个报文段到达主机后，应该传输给哪个应用程序呢？你怎么知道这个报文段就是传递给 HTTP 服务器而不是 SSH 服务器的呢？是凭借端口号吗？当报文到达服务器时，是端口号来区分不同应用程序的，所以应该借助端口号来区分。举个例子反驳一下，假如到达服务器的两条数据都是由 80 端口发出的你该如何区分呢？或者说到达服务器的两条数据端口一样，协议不同，该如何区分呢？所以仅凭端口号来确定某一条报文显然是不够的。互联网上一般使用四元组：源 IP 地址、目标 IP 地址、源端口号、目的端口号 来进行区分。如果其中的某一项不同，就被认为是不同的报文段。这些也是多路分解和多路复用的基础。确定端口号在实际通信之前，需要先确定一下端口号，确定端口号的方法分为两种： 标准既定的端口号标准既定的端口号是静态分配的，每个程序都会有自己的端口号，每个端口号都有不同的用途。0 - 1023 范围内的端口号都是动态分配的既定端口号，例如 HTTP 使用 80 端口来标识，FTP 使用 21 端口来标识，SSH 使用 22 来标识。这类端口号有一个特殊的名字，叫做周知端口号(Well-Known Port Number)。 操作系统分配的端口号第二种分配端口号的方式是一种动态分配法，在这种方法下，客户端应用程序可以完全不用自己设置端口号，凭借操作系统进行分配，操作系统可以为每个应用程序分配互不冲突的端口号。这种动态分配端口号的机制即使是同一个客户端发起的 TCP 连接，也能识别不同的连接。多路复用和多路分解我们上面聊到了在主机上的每个套接字都会分配一个端口号，当报文段到达主机时，运输层会检查报文段中的目的端口号，并将其定向到相应的套接字，然后报文段中的数据通过套接字进入其所连接的进程。下面我们来聊一下什么是多路复用和多路分解的概念。多路复用和多路分解分为两种，即无连接的多路复用/多路分解和面向连接的多路复用/多路分解无连接的多路复用和多路分解开发人员会编写代码确定端口号是周知端口号还是时序分配的端口号。假如主机 A 中的一个 10637 端口要向主机 B 中的 45438 端口发送数据，运输层采用的是 UDP 协议，数据在应用层产生后，会在运输层中加工处理，然后在网络层将数据封装得到 IP 数据报，IP 数据包通过链路层交付给主机 B，主机 B 会检查报文段中的端口号判断是哪个套接字的，这一系列的过程如下所示UDP 套接字就是一个二元组，二元组包含目的 IP 地址和目的端口号。所以，如果两个 UDP 报文段有不同的源 IP 地址和/或相同的源端口号，但是具有相同的目的 IP 地址和目的端口号，那么这两个报文会通过套接字定位到相同的目的进程。 这里思考一个问题，主机 A 给主机 B 发送一个消息，为什么还需要知道源端口号呢？在 A 到 B 的报文段中，源端口号会作为返回地址的一部分，即当 B 需要回发一个报文段给 A 时，B 需要从 A 到 B 中的源端口号取值，如下图所示面向连接的多路复用与多路分解如果说无连接的多路复用和多路分解指的是 UDP 的话，那么面向连接的多路复用与多路分解指的是 TCP 了，TCP 和 UDP 在报文结构上的差别是，UDP 是一个二元组而 TCP 是一个四元组，即源 IP 地址、目标 IP 地址、源端口号、目标端口号 ，这个我们上面也提到了。当一个 TCP 报文段从网络到达一台主机时，这个主机会根据这四个值拆解到对应的套接字上。上图显示了面向连接的多路复用和多路分解的过程，图中主机 C 向主机 B 发起了两个 HTTP 请求，主机 A 向主机 C 发起了一个 HTTP 请求，主机 A、B、C 都有自己唯一的 IP 地址，当主机 C 发出 HTTP 请求后，主机 B 能够分解这两个 HTTP 连接，因为主机 C 发出请求的两个源端口号不同，所以对于主机 B 来说，这是两条请求，主机 B 能够进行分解。对于主机 A 和主机 C 来说，这两个主机有不同的 IP 地址，所以对于主机 B 来说，也能够进行分解。" }, { "title": "如何设计一个RPC框架?", "url": "/posts/RPCApply/", "categories": "技术科普", "tags": "学习", "date": "2023-10-15 06:50:00 +0000", "snippet": " 如何设计一个RPC框架？ 本文将通过面试一问一答的形式，来讲解如何设计一个高可用、高性能的RPC框架！RPC 模拟面试 此处省略一番自我介绍和寒暄，面试官开始夺命十八连问～👨‍💼 面试官：小Y同学，你先说说你的 RPC 项目是怎么实现的？👦 小Y：（竟然一上来就问这么硬的，还好我早有准备）👦 小Y：我先给您说说我这个 RPC 项目的核心原理和组件吧，请看这张图：RPC 全称为 Rem...", "content": " 如何设计一个RPC框架？ 本文将通过面试一问一答的形式，来讲解如何设计一个高可用、高性能的RPC框架！RPC 模拟面试 此处省略一番自我介绍和寒暄，面试官开始夺命十八连问～👨‍💼 面试官：小Y同学，你先说说你的 RPC 项目是怎么实现的？👦 小Y：（竟然一上来就问这么硬的，还好我早有准备）👦 小Y：我先给您说说我这个 RPC 项目的核心原理和组件吧，请看这张图：RPC 全称为 Remote Procedure Call，意思为远程调用，并且这个过程就像调用本地方法一样简单！我们不需要关注底层的网络传输细节，只需要按照调用本地方法的流程去调用远程方法即可。通常，一个 RPC 框架有这么几个核心组件： Server Client Server Stub Client StubServer 和 Client 比较简单，就是常规意义的服务端和客户端，而在 RPC 中，又引入了一个新的概念 「Stub」。它起到的作用其实就是代理，处理一些琐碎的事情： 对于 Client Stub，它主要是将客户端的请求参数、请求服务地址、请求服务名称做一个封装，并发送给 Server Stub 对于 Server Stub，它主要用于接收 Client Stub 发送的数据并解析，去调用 Server 端的本地方法以上就是 RPC 的核心啦 ……（面试官突然打断）👨‍💼 面试官：了解，但这些内容属于比较基础的 RPC，实际应用场景中的 RPC 远不止这么简单，你的 RPC 框架还有其它设计吗？👦 小Y：（我正准备说呢！😭）是这样的，接下来我给您看看我这个 RPC 项目的层次结构吧！（好戏还在后面）首先，我讲一讲代理层。它其实对应到我之前提到的：按照调用本地方法的流程去调用远程方法。这一功能就是通过代理层来加以实现。通过使用代理模式，我们可以屏蔽远程方法的调用细节，如：网络连接建立、序列化、发送请求数据、获取返回结果、解析结果等一系列操作。对于调用者、框架使用者来说，他们只需要直接调用远程方法即可，复杂的逻辑都封装在 RPC 框架中处理。我顺便说一下，除了屏蔽调用细节，代理层的其他优点吧： 代理层可以扩展目标对象的功能 代理层可以与客户端进行解耦，提升系统的可扩展性（又被面试官打断）👨‍💼 面试官：那你的代理层是如何转发请求的呢？在微服务分布式场景下，是有许多服务的，一个服务也可能对应多个实例，你是如何处理的？👦 小Y：（这不就是注册中心需要解决的事情吗？简单！）这正是我准备说的「注册中心层」。如果只使用代理层的话，是很难处理您所说的这种情况的，因为我们需要考虑如何记录众多的服务的地址信息，并在某个服务上下线时，通知其他服务。若这个时候单纯使用代理层去管理这些琐碎的事情，就会造成代码复杂度、耦合度上升，不易于扩展与维护。因此，我在 RPC 框架中抽象出了「注册中心层」，专门用于处理服务注册、服务信息查找、服务上下线通知。更具体地来说，负责以下三类事项： 服务发现：客户端需要订阅注册中心。在需要远程调用时，从注册中心中获取信息，然后进行方法调用 服务注册：服务提供者将地址、接口、分组等信息存放在注册中心模块，当服务上线、下线均会通知注册中心 服务管理：提供服务的上下线管理、服务配置管理、服务健康检查等功能，以保证服务的可靠性和稳定性就像我图中所画的一般：👨‍💼 面试官：那你的注册中心具体是如何实现的？你是手写了一个注册中心组件吗？👦 小Y：（糟了，他不会以为我是手写的吧！我得迂回一下！）呃，并没有，因为正如之前所说，这里涉及到了数据存储、事件监听机制、心跳机制等多个复杂的工作，而市面上恰好有满足这些特性的开源组件，考虑到整体项目的进度以及手写的复杂程度，最后我还是选择了开源的解决方案。👨‍💼 面试官：那你是如何做选择的？换句话说，你之前有仔细了解过这些开源组件吗？👦 小Y：（就知道会问这个，好在我早有准备～）在谈如何选择注册中心之前，请让我先简单介绍一下 CAP 理论哈～因为之后我会根据 CAP 理论选择注册中心！CAP 理论是分布式系统中的重中之重！敲黑板！注册中心重点来喽！觉得有压力的同学可以先跳过，先接着往后读～CAP 是 Consistency（一致性）、Availability（可用性）、Partition Tolerance（分区容错性） 这三个单词首字母组合。一致性（Consistency） : 所有节点访问同一份最新的数据副本可用性（Availability）: 非故障的节点在合理的时间内返回合理的响应（不是错误或者超时的响应）。分区容错性（Partition Tolerance） : 分布式系统出现网络分区的时候，仍然能够对外提供服务。CAP 并不是简单的 3 选 2，因为分区容错性是必须实现的。以分区容错性作为前提，在一致性与可用性中做选择。 Zookeeper Zookeeper 通过 znode 节点来存储数据。因此我们可以利用这一特性进行服务注册，节点用于存储服务 IP、端口、协议等信息。 例如：服务提供者上线时，Zookeeper 创建该节点 - /provider/{serviceName}:{ip}:{port} Zookeeper 提供 Watcher 机制，可以监听相应的节点路径。因此我们可以利用这一机制监听对应的路径，一旦路径上的数据发生了变化，我们便向其他订阅该服务的服务发送数据变更消息。收到消息的服务便去更新本地缓存列表。 Zookeeper 提供心跳检测功能，定时向各个服务提供者发送心跳请求，确保各个服务存活。如果服务一直未响应，则说明服务挂了，将该节点删除。 Zookeeper 遵循一致性原则，即 「CP」 对于注册中心而言，最重要的是可用性，我们需要随时能够获取到服务提供者的信息，即使它可能是几分钟以前的旧信息。 但是 Zookeeper 由于其核心算法是 ZAB，主要适用于分布式协调系统（分布式配置、集群管理等场景）。当 master 节点故障后，剩余节点会重新进行 leader 选举，导致在选举期间整个 Zookeeper 集群不可用。 Nacos 服务提供者启动时，会向 Nacos Server 注册当前服务信息，并建立心跳机制，检测服务状态。 服务消费者启动时，从 Nacos Server 中读取订阅服务的实例列表，缓存到本地。并开启定时任务，每隔 10s 轮询一次服务列表并更新。 Nacos Server 采用 Map 保存实例信息。当配置持久化后，该信息会被保存到数据库中。 对于服务健康检查，Nacos 提供了 agent 上报与服务端主动监测两种模式 Nacos 支持 CP 和 AP 架构，根据 ephemeral 配置决定 ephemeral = true，则为 AP ephemeral = false，则为 CP Eureka 服务提供者启动时，会到 Eureka Server 去注册服务 服务消费者会从 Eureka Server 中定时以全量或增量的方式获取服务提供者信息，并缓存到本地 各个服务会每隔 30s 向 Eureka Server 发送一次心跳请求，确认当前服务正常运行。若 90s 内 Eureka Server 未收到心跳请求，则将对应服务节点剔除。 Eureka 遵循可用性原则，即「AP」。 Eureka 为「去中心化结构」，没有 master / slave 节点之分。只要还有一个 Eureka 节点存活，就仍然可以保证服务可用。但是可能会出现数据不一致的情况，即查到的信息不是最新的。 Eureka 节点收到请求后，会在集群节点间进行复制操作，复制到其他节点中。 Consul 服务提供者启动时，会向 Consul Server 发送一个 Post 请求，注册当前服务信息 服务消费者发起远程调用时，会向 Consul Server 发送一个 Get 请求，获取对应服务的全部节点信息 Consul Server 每隔 10s 会向服务提供者发送健康检查请求，确保服务存活，并更新服务节点列表信息。 Consul 遵循一致性原则，即「CP」这 4 种开源组件均满足注册中心需求。在这种场景下，技术选型就是一个 Trade-off 了，我们需要选择一个最适合的组件！ 对于 Consul，它底层语言是 Go，更支持容器化场景，而当前 RPC 框架采用的是 Java 语言，所以就先淘汰啦～ 对于 Eureka，它很适合作为注册中心，但是其维护更新频率很低，目前国内使用的人很少，所以在这里就先不使用啦～ 对于 Nacos，它是目前国内非常主流的一种注册中心，而且由 Alibaba 开源。 最后，我还是选择了 Zookeeper，虽然 Zookeeper 追求一致性导致其不太适合于注册中心场景，但是国内 Dubbo 框架选用了 Zookeeper 作为注册中心，能从 Dubbo 框架中参考到许多优秀的实现技巧。并且，我们可以通过操作 Zookeeper 节点，从更加底层的角度感受如何实现注册服务。（啊～终于说完了，好累）👨‍💼 面试官：嗯，说的很不错，看来在技术选型上做了很多功课！你最终选用了 Zookeeper，那你还知道 Zookeeper 的其它应用场景吗？👦 小Y：（竟然问这么细）除了注册中心，Zookeeper 还可以实现分布式锁、分布式 ID、配置中心等功能。对于分布式锁： Zookeeper 有一种节点为临时节点，它可以保证服务宕机后节点自动被删除，不需要额外考虑添加节点过期时间来解决死锁问题。 Zookeeper 可以通过使用顺序节点，满足公平锁特性。 Zookeper 节点加锁时，通过监听前驱节点状态，判断是否获取到锁。 如果监听到它的前驱节点被删除时，则相当于获取到锁；否则阻塞。对于分布式 ID： Zookeeper 可以通过其顺序节点，实现分布式 ID，确保分布式环境下 ID 不重复。对于配置中心： 通过 znode 节点实现配置存储 通过 Watcher 监听节点信息是否发生变化，若发生变化，则通知客户端更新配置信息。👨‍💼 面试官：掌握得可以呀～那你接着说吧。👦 小Y：之前我们有提到，一个服务可能对应着多个实例节点，从注册中心中获取到的可能不止有一个服务地址，可能是一个地址信息 List。这时候我们就需要借助「路由层」，帮助我们从多个实例节点中选取一个，这就是「负载均衡」。在我的 RPC 框架中，我提供了如下 5 种负载均衡策略： 随机选取策略 轮询策略 加权轮询策略 最少活跃连接策略 一致性 Hash 策略 对于「随机选取」策略，顾名思义，即从多个节点中随机选取一个节点进行访问。这种方式最大的优点就是简单，但是当请求数量较少时，随机性可能不强，可能会出现单实例节点负载过大的情况。当请求数量很大时，每个实例节点接受的请求数量会接近于均衡，效果较好。 对于「轮询」策略，即轮转调度。假设当前服务有 3 个实例节点，第一次请求发送给 A 节点，第二次请求发送给 B 节点，第三次请求发送给 C 节点，那么第四次请求就会再次发送给 A 节点，实现均衡请求的效果。 对于「加权轮询」策略，是为了解决「轮询」策略所面临的问题。试想一种场景，在当前服务的集群中，有的实例节点配置较高，内存大且多核处理器，那么它就可以承载更多请求。有的实例配置低，那么它的承载能力就会弱一些。这时候「轮询」策略不足以满足这一使用场景。因此，我们需要考虑为每个实例节点设置权重，使权重大配置高的节点处理更多的请求，这就是「加权轮询」策略。 对于「最少活跃连接」策略，是为了解决以上策略所面临的共同问题。我们再试想一种场景，某些请求的处理时间更长，比如拉取用户粉丝列表，对于头部博主来说，其粉丝数多，拉取时间长，而对于普通用户来说，其粉丝数少，很快就可以拉取完毕。这就导致拉取一个大 V 粉丝列表的时间远长于拉取 100 个普通用户粉丝列表的时间。这时候如果还是按照「轮询」策略，会导致 A 节点即使收到的请求比 B 节点少，但却超过所能承受的最大负载。而「最少活跃连接」策略的意思是选取当前活跃请求最少的服务节点。因此，在这种场景下，更适合使用「最少活跃连接」策略，会得到更合理的负载均衡效果。 对于「一致性 Hash」策略，是通过请求中携带的参数来定位对应的实例节点。比如，请求参数中携带了用户 ID。用户 ID 为 1 ～ 10 的请求永远对应到 A 节点，用户 ID 为 11 ～ 20 的请求永远对应到 B 节点，依次类推…这就是路由层的核心作用 —— 「负载均衡」啦～👨‍💼 面试官：我刚才听你提到一致性 Hash 策略，但好像没有提到 Hash，你能在具体说说吗？ 对于「随机选取」策略，顾名思义，即从多个节点中随机选取一个节点进行访问。这种方式最大的优点就是简单，但是当请求数量较少时，随机性可能不强，可能会出现单实例节点负载过大的情况。当请求数量很大时，每个实例节点接受的请求数量会接近于均衡，效果较好。 对于「轮询」策略，即轮转调度。假设当前服务有 3 个实例节点，第一次请求发送给 A 节点，第二次请求发送给 B 节点，第三次请求发送给 C 节点，那么第四次请求就会再次发送给 A 节点，实现均衡请求的效果。 对于「加权轮询」策略，是为了解决「轮询」策略所面临的问题。试想一种场景，在当前服务的集群中，有的实例节点配置较高，内存大且多核处理器，那么它就可以承载更多请求。有的实例配置低，那么它的承载能力就会弱一些。这时候「轮询」策略不足以满足这一使用场景。因此，我们需要考虑为每个实例节点设置权重，使权重大配置高的节点处理更多的请求，这就是「加权轮询」策略。 对于「最少活跃连接」策略，是为了解决以上策略所面临的共同问题。我们再试想一种场景，某些请求的处理时间更长，比如拉取用户粉丝列表，对于头部博主来说，其粉丝数多，拉取时间长，而对于普通用户来说，其粉丝数少，很快就可以拉取完毕。这就导致拉取一个大 V 粉丝列表的时间远长于拉取 100 个普通用户粉丝列表的时间。这时候如果还是按照「轮询」策略，会导致 A 节点即使收到的请求比 B 节点少，但却超过所能承受的最大负载。而「最少活跃连接」策略的意思是选取当前活跃请求最少的服务节点。因此，在这种场景下，更适合使用「最少活跃连接」策略，会得到更合理的负载均衡效果。 对于「一致性 Hash」策略，是通过请求中携带的参数来定位对应的实例节点。比如，请求参数中携带了用户 ID。用户 ID 为 1 ～ 10 的请求永远对应到 A 节点，用户 ID 为 11 ～ 20 的请求永远对应到 B 节点，依次类推…这就是路由层的核心作用 —— 「负载均衡」啦～👨‍💼 面试官：我刚才听你提到一致性 Hash 策略，但好像没有提到 Hash，你能在具体说说吗？👦 小Y：（竟然听的这么细！）好嘞，可能是我漏啦～我们可以从一致性 Hash 环的原理讲起！我们可以看到如图「圆环」中存在有 3 个节点，分别为 NodeA / Node B / Node C，4 个请求，分别为 Req 1 / Req 2 / Req 3 / Req 4。为什么叫这个「圆环」为「一致性 Hash 环」呢？这是因为我们要对每一个节点根据 Hash 算法计算得到一个 Hash 值，并将其映射到圆环中的某一个位置。对于请求也是如此，根据参数来计算具体的 Hash 值，也映射到圆环中对应的位置。接下来的事情就很简单啦，每一个请求沿着当前圆环 顺时针 寻找，找到的第一个节点就是对应的处理请求节点。如图对应关系为，Node A 处理 Req 3 和 Req 4，Node B 处理 Req 1，Node C 处理 Req 2。但是一致性 Hash 环容易造成一个问题，看下面这个图就一目了然啦！当服务节点过少时，节点 Hash 值映射到圆环的位置可能聚集于某一处，容易因为节点分布不均匀而造成请求均衡问题，即「数据倾斜」。在图中，Node A 承担了大部分请求，Node C 只承担了一个请求，Node B 一个都没有。为此，我们需要引入「虚拟节点」解决这一问题。图中黄色节点对应的就是「虚拟节点」，起到均衡请求、避免数据倾斜的作用。👨‍💼 面试官：理解的很到位！你接着说路由层吧～👦 小Y：（感觉稳了！）路由层除了实现核心的「负载均衡」功能之外，还承担了分配流量的作用。在 RPC 框架中，我们可以将流量标签、实例标签、路由规则等信息存储在请求中，这样一来，我们就可以随意控制流量，将请求分配到不同的流量环境。基于此，我们可以实现「泳道测试」，即对于生产环境请求，打上对应的 prod 标签，对于测试环境请求，打上对应的 test 标签。这样就可以让大部分请求转发到生产环境服务，而新版本测试请求转发到测试环境服务。👨‍💼 面试官：时间不多啦，你接着说下一层吧～ 关于路由层的知识点，面试一般只会考到「负载均衡」算法。所以为了考虑到大部分读者快速准备面试的需求，对于路由层的高级路由部分，如：条件路由、泳道测试、灰度测试等具体内容，我会放到我的微服务专栏进行详细的讲解～👦 小Y：好嘞！咱们接着说序列化层！由于 RPC 调用的底层是网络请求，当我们的请求携带参数时，请求发送方需要将参数进行「序列化」，从而实现在网络中传输。而请求接收方也需要进行「反序列化」操作，获取到可理解的参数。 序列化：将数据结构或对象转化为二进制字节流 反序列化：将在序列化过程中生成的二进制字节流转化为数据结构或对象为了整合多种序列化框架，我抽象出了序列化层，使用工厂模式，定义了抽象工厂接口，其中有两个方法：serialize 和 deserialize并定义相应的序列化实现类，包含如下序列化框架： JDK序列化 通过ObjectOutputStream的writeObject和readObject方法实现，可通过重写指定其他序列化方式 JDK默认序列化方式性能差，且只适用于Java Protocol Buffer 支持跨语言、跨平台，可扩展性强 需要使用IDL来定义Schema描述文件，定义完描述文件后，可以直接使用protoc来直接生成序列化与反序列化代码 性能低于Kyro，但是高于大部分序列化协议，序列化后的size也较小 Kyro 主要适用于Java，不支持字段扩展 使用简洁，直接使用Input、Output对象 高性能，序列化与反序列化时间开销都很低，序列化后的size也很小 Hessian 支持跨语言、跨平台，可扩展性强 易用：只需要实现Serializable接口即可 序列化时间与大小都比较小👨‍💼 面试官：可以的～那在数据传输过程中，可能会出现粘包和半包问题，你是如何解决的？👦 小Y：（好问题，正好我复习了！）我在做之前，是有了解调研过业界的解决方案，有以下几种： 固定长度传输：固定好每次数据包的长度，比如规定每次传输长度为 32 个字节。当接收方接满 32 个字节时，代表接受到了完整的信息。 该方法灵活性太低 特殊字符分割：即每次接收方读取数据时，读到事先约定好的特殊字符时，代表接受到了完整的信息。 如果消息内容中刚好有这一特殊字符，需要提前做转义，还是比较麻烦。 自定义消息结构最后，考虑到灵活性，决定自定义消息结构，因此我抽象出了「协议层」，专门用于定义消息收发格式。自定义协议结构体如下： MagicNumber 魔数：用于做安全检测，能够快速确认当前请求是否合法。请求发送方和接收方可以提前约定好魔数。 ContentLength 请求长度：协议长度。 Content 核心传输数据：封装请求的接收方名称、请求的方法、请求参数等内容 这里的 Content 为二进制字节流，对应的就是「序列化层」序列化后得到的二进制字节流。👨‍💼 面试官：似乎还有「链路层」和「容错层」，你再展开说说吧～👦 小Y：好的。我先说说为什么需要「链路层」，其主要用于解决以下两个问题： 对 RPC 请求做鉴权：请求在到达请求接收方之前，先校验其是否有请求凭证（Token） 记录请求过程中的调用日志信息「链路层」的核心设计思想是责任链设计模式： 责任链模式可以使我们任意添加请求的「前处理」和「后处理」对象，并调整处理顺序，提高了维护性和可拓展性，可以根据需求新增处理类，满足开闭原则。 责任链简化了对象之间的连接。每个对象只需保持一个指向其后继者的引用，不需保持其他所有处理者的引用，这避免了使用众多的 if 或者 if···else 语句。 责任分担，职责分离。每个类只需要处理自己该处理的工作，不该处理的传递给下一个对象完成，明确各类的责任范围，符合类的单一职责原则。对于「容错层」，我主要是为了实现服务稳定性治理，确保服务的高可用性。主要通过以下几种手段实现容错层： 超时重试机制 请求一般可以分为「幂等」与「非幂等」请求，幂等性指的是多次请求某一个资源，最后的结果相同，对系统产生的影响相同。 在请求重试时，我们需要额外考虑「非幂等请求」重试所带来的风险（比如转账、下单等涉及资金业务场景），当请求超时时，很难判断数据包是否已经到达服务接收方。因此，在请求参数中添加 retry 参数，重试次数由用户自行决定。当 retry = 0 时，则代表请求失败不进行重试。 服务限流 在高并发场景下，通过限制瞬发 QPS 最大值，从而防止系统被流量击溃，最大限度保证服务高可用。 服务熔断 在调用过程中，可能出现 Bug、故障等问题。为了防止由于此类问题导致故障在调用链路中扩散，引起「链路雪崩」，我们选择触发「服务熔断」，直接抛出异常，放弃继续调用下游服务，最大程度保护其他服务。👨‍💼 面试官：不错，说得很好。但是你的 RPC 框架最后有做压力测试吗？所有框架最后都得使用呀，一定要能投入到生产环境使用。👦 小Y：（这也要问吗？还好我做了这一步！）您说的没错，我也有考虑到这一点。在写完代码后，我做了一次压力测试：我通过设置连续请求次数为 100 / 1000 / 10000，对框架进行压力测试，发现随着请求次数梯度上升，整体接口的响应速度和结果并没有发生变化，说明框架稳定。👨‍💼 面试官：好的。最后我再问一个问题，你觉得你的 RPC 框架还有什么设计亮眼的地方？👦 小Y：（啊，终于快要问完了）我在框架中多次运用到了「异步设计」，对各个操作进行解耦。对于服务端： 当请求抵达服务器时，将其直接丢入业务阻塞队列中，然后开辟一个新的线程，从阻塞队列中循环获取Handler请求任务。 将获取到的任务对象交付于业务线程池进行消费处理。对于客户端： 代理层在发送完请求之后，不需要同步阻塞等待响应结果。结果的返回为异步。 并且用户可以通过配置文件的方式，自行选择异步或同步。👨‍💼 面试官：咦，你提到了你有用线程池技术，那么你是如何选择线程数的呀？👦 小Y：（什么？不是最后一个问题吗？怎么还有？）通用的选择方式是根据线程池处理任务的类型进行选择： 如果是CPU密集型任务，如：加密、解密、压缩、计算，应该根据当前服务器CPU核心数进行选择，最好是CPU核心数的1~2倍 如果是IO密集型任务，如：数据库、网络传输、文件读写，应该尽可能提升线程数 公式为：线程数 = CPU 核心数 *（1+平均等待时间/平均工作时间） 平均等待时间越长，说明是IO密集型，需要增大线程数 平均工作时间越长，说明是CPU密集型，需要减少线程数对于我这个框架来说，大部分都是 IO 密集型任务，因此我调大了线程数。👨‍💼 面试官：好的，可以看出 小Y同学 对于 RPC 设计掌握得确实不错。行吧，回去等通知吧～👦 小Y：（？？？？？？？？？？？？）总结终于写完了，洋洋洒洒将近 1w 字。总结一下全文：我们从面试的角度出发，将 RPC 框架拆分了多个层次，逐层剖析 RPC 框架的具体实现原理，也可以帮助大家了解到设计一个RPC框架时应该注意哪些层面的问题。" }, { "title": "计算机网络应用层协议(三)", "url": "/posts/ApplyProtocol3/", "categories": "计算机网络", "tags": "学习", "date": "2023-10-08 13:56:00 +0000", "snippet": "应用层协议Web 缓存Web 缓存 (Web cache) 也叫做代理服务器缓存，它是代表 HTTP 服务器来满足用户需求的网络实体。Web 缓存器有自己的磁盘存储空间，并会在存储空间内保存最近请求过的对象，如下图所示Web 缓存可以在用户的浏览器中进行配置，一旦配置后，用户首先访问的就不是初始服务器了，需要先访问代理服务器判断请求的对象是否存在，如果代理服务器没有，再由代理服务器来请求初始...", "content": "应用层协议Web 缓存Web 缓存 (Web cache) 也叫做代理服务器缓存，它是代表 HTTP 服务器来满足用户需求的网络实体。Web 缓存器有自己的磁盘存储空间，并会在存储空间内保存最近请求过的对象，如下图所示Web 缓存可以在用户的浏览器中进行配置，一旦配置后，用户首先访问的就不是初始服务器了，需要先访问代理服务器判断请求的对象是否存在，如果代理服务器没有，再由代理服务器来请求初始服务器把对象返回给客户，同时在自己的磁盘空间保存对象。这里需要注意，客户和初始服务器的架构是客户-服务器模式，而代理服务器不仅能当服务器使用，也可以当作客户端使用。代理服务器一般由ISP (Internet Service Provider)，提供。ISP 也就是我们常说的运营商。那么为什么需要代理服务器的存在呢？相信你看完上面的描述应该能大致猜到它的作用。 首先代理服务器可以大大减少对客户请求的响应时间，能够更快给用户响应。 其次代理服务器可以减少一个机构接入链路到网络的通信量，降低网络带宽，降低运营商成本。 然后代理服务器可以分担初始服务器的压力，改善应用程序的性能。DASH现在我们知道了 HTTP 是可以传输普通文件、音频、视频的，这些传输的信息统称为 MIME 类型。HTTP 在传输视频的过程中，也会把视频当做一个对象，而一个对象其实就是一个文件，一个文件都在 HTTP 中都可以用 URL 来表示。当用户在看视频时，客户与服务器建立一个 TCP 连接并发送对该 URL 的 GET 请求，然后服务器响应给客户端时，客户端会缓存一定量的字节数据，当数据超过预先设定的门限时，客户应用程序就开始播放视频。这种方式有一种局限性就是对每个客户端来说，尽管每个客户端可用的带宽量不同，但所有客户端都收到相同的视频编码。这就造成带宽浪费。这就相当我是一个 2M 的网络和 50M 的光纤都能收到相同的视频编码，以几乎相同的等待时间开始播放视频，那么我为什么还要花 50 兆光纤的钱呢？为了改善这一现象，出现了 HTTP 的 DASH 动态适应流。它的理念是针对不同流量的网络来说，所能够传输的比特数据也不相同。DASH 允许客户使用不同的因特网传输速率可以播放不同编码速率的视频。对于 3G 用户和光纤用户自然会选择以不同的速率传输比特数据，从而最大限度的使用带宽。CDN随着互联网的接入用户变得越来越多，视频逐渐成为了比特传输的瓶颈和用户的强烈需求。作为一个因特网视频公司，最一开始提供流式服务最直接的方式是建立单一的大规模数据中心。在数据中心内缓存所有视频，并直接从数据中心向世界范围内传播视频。但是这种方式存在三种问题 如果客户远离数据中心，那么服务器到客户分组会跨越许多通信链路并且可能通过许多 ISP，这样你的视频播放能快到哪去？ 每次视频数据都会重新传递给客户端，这样会严重浪费网络带宽，而且视频公司会支付重复的带宽费用 单点故障问题，只要视频数据中心宕机或者其他事故，直接导致全球范围内的视频无法播放。为了应对能够向全世界的用户 24 小时不间断的分发视频，几乎所有的主流视频公司都会使用内容分发网。CDN 管理分布在多个地理位置上的服务器，在每个服务器上缓存各种视频、音频、文件等。CDN 内容选择策略CDN 管理分布在多个地理位置上的服务器，在它的服务器上存储视频副本，并且所有试图将每个用户请求定向到一个提供最好用户体验的 CDN 位置。那么服务器如何选址呢？事实上有两种服务器安置原则： 深入：它的主要目标是靠近用户，通过减少端用户和 CDN 集群之间链路和路由器的数量，从而改善了用户感受的时延和吞吐量。 邀请做客：这个原则是通过在少量（例如 10 个）关键位置建造大集群来邀请 ISP 来做客，与深入设计原则相比，邀请做客设计通常产生较低的维护和管理开销。CDN 工作流程CDN 可以是专用 CDN，即它由内容提供商自己所拥有；另一种 CDN 是 第三方 CDN，它代表多个内容提供商分发内容。下面我们来聊一下 CDN 工作流程，如下图所示： 用户想要访问指定网站的内容。 用户首先发起对本地 DNS，LDNS 的查询，LDNS 会将请求中继到网站 DNS 服务器，网站的 DNS 服务器会返回给 LDNS 一个网站 CDN 权威服务器的地址。 LDNS 服务器会发送第二个请求给网站 CDN 权威服务器，希望获取网站内容分发服务器的地址，网站 CDN 会把 CDN 内容分发服务器的地址发送给本地 DNS 服务器。 本地 DNS 服务器会把网站 CDN 内容分发服务器的地址发送给用户。 用户知道网站 CDN 内容分发服务器的地址后，无需额外操作，直接和网站 CDN 内容分发服务器建立 TCP 连接，并且发出 HTTP GET 请求，如果使用了 DASH 流，会根据不同 URL 的版本选择不同速率的块发送给用户。 CDN 集群选择策略任何 CDN 的部署，其核心是集群选择策略 (cluster selection strategy)，即动态的将客户定向到 CDN 中某个服务器集群或数据中心的机制。一种简单的策略是指派客户到地理上最为临近 (geographically closest)的集群。这种选择策略忽略了时延和可用带宽随因特网路径时间而变化，总是为特定的客户指派相同的集群；还有一种选择策略是实时测量 (real-time measurement)。该机制是基于集群和客户之间的时延和丢包性能执行周期性检查。DNS 域名解析协议试想一个问题，我们人类可以有多少种识别自己的方式？可以通过身份证来识别，可以通过社保卡号来识别，也可以通过驾驶证来识别，尽管我们有多种识别方式，但在特定的环境下，某种识别方法可能比另一种方法更为适合。因特网上的主机和人类一样，可以使用多种识别方式进行标识。互联网上主机的一种标识方法是使用它的主机名，如 www.facebook.com、 www.google.com 等。但是这是我们人类的记忆方式，路由器不会这么理解，路由器喜欢定长的、有层次结构的 IP 地址，还记得什么是 IP 地址吗？IP 地址现在简单表述一下，就是一个由 4 字节组成，并有着严格的层次结构。例如 121.7.106.83 这样一个 IP 地址，其中的每个字节都可以用 . 进行分割，表示了 0 - 255 的十进制数字。然而，路由器喜欢的是 IP 地址进行解析，我们人类却便于记忆的是网址，那么路由器如何把 IP 地址解析为我们熟悉的网址地址呢？这时候就需要 DNS 出现了。DNS 是一个由分层的 DNS 服务器 (DNS server) 实现的分布式数据库；它还是一个使得主机能够查询分布式数据库的应用层协议。DNS 服务器通常是运行 BIND (Berkeley Internet Name Domain) 软件的 UNIX 机器。DNS 协议运行在 UDP 上，使用 53 端口。P2P 文件分发我们上面探讨的协议 HTTP、SMTP、DNS 都采用了客户-服务器模式，这种模式会极大依赖总是打开的基础设施服务器。而 P2P 是客户端与客户端模式，对总是打开的基础设施服务器有最小的依赖。P2P 的全称是 Peer-to-peer, P2P，是一种分布式体系结构的计算机网络。在 P2P 体系中，所有的计算机和设备都被称为对等体，他们互相交换工作。对等网络中的每个对等方都等于其他对等方。网络中没有特权对等体，也没有主管理员设备。从某种意义上说，对等网络是计算机世界中最平等的网络。每个对等方都相等，并且每个对等方具有与其他对等方相同的权利和义务。对等体同时是客户端和服务器。实际上，对等网络中可用的每个资源都是在对等之间共享的，而无需任何中央服务器。P2P 网络中的共享资源可以是诸如处理器使用率，磁盘存储容量或网络带宽等。P2P 用来做什么P2P 的主要目标是共享资源并帮助计算机和设备协同工作，提供特定服务或执行特定任务。如前面说到的，P2P 用于共享各种计算资源，例如网络带宽或磁盘存储空间。 但是，对等网络最常见的例子是 Internet 上的文件共享。 对等网络非常适合文件共享，因为它们允许连接到它们计算机等同时接收文件和发送文件。BitTorrent 是 P2P 使用的主要协议。P2P 网络的作用P2P 网络具有一些使它们有用的特征： 很难完全掉线，即使其中的一个对等方掉线，其他对等方仍在运行并进行通信。 为了使 P2P（对等）网络停止工作，你必须关闭所有对等网络。对等网络具有很强的可扩展性。 添加新的对等节点很容易，因为你无需在中央服务器上进行任何中央配置。 当涉及到文件共享时，对等网络越大，速度越快。 在 P2P 网络中的许多对等点上存储相同的文件意味着当某人需要下载文件时，该文件会同时从多个位置下载。TELNETTELNET 又称为远程登录，是一种应用层协议，它为用户提供了在本地机器上就能够操控远程主机工作的能力。例如下面这幅图所示。主机 A 可以直接通过 TELNET 协议访问主机 B。TELNET 利用 TCP 的一条连接，通过一条连接向主机发送文字命令并在主机上执行。使用 TELNET 协议进行远程登录时需要满足一下几个条件： 必须知道远程主机的 IP 地址或者域名。 必须知道登录标识和口令。TELNET 远程登录一般使用 23 端口。TELNET 的工作过程如下： 本地主机与远程主机建立连接，这个连接其实是 TCP 连接，用户需要知道指定主机的 IP 地址或者域名 与远程主机建立连接后，在本地主机终端上输入的字符都会以 NVT (Net Virtual Terminal) 的形式发送至远程主机，这个过程实际上是发送一个数据包到远程主机。 远程主机接受数据包后，产生的输出会以 NVT 的格式发送给本地主机一个数据包，包括输入命令回显和命令执行结果 最后，本地主机终端对远程主机撤销链接，这个过程实际上就是 TCP 断开连接的过程。SSHTELNET 有一个非常明显的缺点，那就是在主机和远程主机的发送数据包的过程中是明文传输，未经任何安全加密，这样的后果是容易被互联网上不法分子嗅探到数据包来搞一些坏事，为了数据的安全性，我们一般使用 SSH 进行远程登录。SSH 是加密的远程登录系统。使用 SSH 可以加密通信内容，即时数据包被嗅探和抓取也无法破解所包含的信息，除此之外，SSH 还有一些其他功能： SSH 可以使用更强的认证机制。 SSH 可以转发文件。 SSH 可以使用端口转发功能。端口转发是 SSH 为网络安全通信使用的一种方法。SSH 可以利用端口转发技术来传输其他 TCP/IP 协议的报文，当使用这种方式时，SSH 就为其他服务在客户端和服务器端建立了一条安全的传输管道端口转发是指将特定端口号所收到的消息转发到指定 IP 地址和端口号的一种机制。FTPFTP (File Transfer Protocol，文件传输协议)是应用层协议之一，它是计算机网络上用处最广的文件传输协议。FTP 协议包括两个组成部分，分为 FTP 服务器和 FTP 客户端。其中 FTP 服务器用来存储文件，用户可以使用 FTP 客户端通过 FTP 协议访问位于 FTP 服务器上的资源。由于 FTP 传输效率非常高，一般用来在网络上传输大的文件。默认情况下 FTP 协议使用 TCP 端口中的 20 和 21 这两个端口，其中 20 用于传输数据，21 用于传输控制信息。FTP TCP 21 号端口上进行文件传输时，每次都会建立一个用于数据传输的 TCP 连接，数据传输完毕后，传输数据的这条连接也会被断开，在控制用的连接上继续进行命令或应答的处理。简单文件传输协议 TFTPTCP/IP 协议簇中还有一个简单文件传输协议(TFTP,Trivial File Transfer Protocol)，它是一个很小且易于实现的文件传输协议。TFTP 也使用的是客户 - 服务器方式，只不过 TFTP 是基于 UDP 来传输报文的。由于 UDP 没有差错检测这一系列对报文完整性的要求，所以 TFTP 需要自己实现差错检测以及错误修改的动作，TFTP 的端口号默认为 69。由于 TFTP 使用 UDP 来传输，所以当需要很多客户端下载文件时往往考虑这种方式；而且 TFTP 代码所占内存空间小，这对某些小型机或一些特殊设备比较重要，特殊设备比如某种小容量的只读存储器。当设备通电时，会执行只读存储器中的代码，然后在网络上广播一个 TFTP 请求，网络上的 TFTP 服务器会发送响应，响应中包括可执行的二进制程序，设备收到后会把程序放入内存中并开始执行。TFTP 的主要特点是： 每次会传输 512 字节的数据，最后不足 512 字节的可作为结束标志位 报文按序号排序，从 1 开始 支持 ASCII 和二进制传输 可对文件进行读写TFTP 工作时，每当发送完数据块时对等待对方确认，确认时应该指明所确认的块编号。如果规定时间内没有收到确认就会重发数据 PDU。TFTP 的主要特点是： 每次会传输 512 字节的数据，最后不足 512 字节的可作为结束标志位 报文按序号排序，从 1 开始 支持 ASCII 和二进制传输 可对文件进行读写TFTP 工作时，每当发送完数据块时对等待对方确认，确认时应该指明所确认的块编号。如果规定时间内没有收到确认就会重发数据 PDU。DHCPDHCP 协议的全称是 Dynamic Host Configuration Protocol，动态主机配置协议。简单来讲一句就是：这个协议能够让我们自动配置 IP 连接网络。我们知道，在互联网中 IP 地址是标识网络上主机的一种方式，这个 IP 地址的配置方法有两种：一种是通过手动来设置 IP 地址，设置完成后不管你走到哪里这个 IP 地址不会改变，这会导致一个问题：你可能离开某个区域内就会导致无法连接网络；还有一种方式是自动配置 IP 地址，这样不管你走到哪里，你的网卡都会帮你连接上网。DHCP 的前身是 BOOTP 协议，BOOTP 协议是一种引导协议，需要人工手动干预进行协议配置，非常不方便。于是 BOOTP 被淘汰了出现了 DHCP，根据上面的描述我们可以得出一个结论：DHCP 是一种即插即用式的协议。即插即用的意思就是插上就能用，不需要再手动设置 IP ，这一系列过程都由操作系统和网卡直接分配并设置好。如果你仔细观察过 IP 地址就会发现，当你在不同的地点时，你的 IP 地址会不断变化。这一切的背后我会在后面详细聊 DHCP 的时候做全面的阐述。SMTP提供电子邮件服务的协议叫做SMTP (Simple Mail Transfer Protocol)，SMTP 在传输层也是用了 TCP 协议。早期电子邮件是在发送端主机和接收端主机之间直接建立 TCP 连接。发送方编写好邮件之后会将邮件保存在磁盘中，然后与接受主机建立 TCP 连接，将邮件发送到接受主机的磁盘中。当发送方把邮件发送后，再从本地磁盘中删除邮件。如果接受主机因为特殊情况无法接收，发送端将等待一段时间后重新发送。这种方法虽然能够保证电子邮件的完整性和有效性，但却不适合当今的互联网，因为早期的电子邮件只能在线发送，这种方式显然不够成熟。针对于此，提出了邮件服务器的概念。邮件服务器构成了整个邮件系统的核心。每个接收方在其中的邮件服务器上会有一个邮箱 (mailbox)存在。用户的邮箱管理和维护发送给他的报文。一个典型的邮件发送过程是：从发送方的用户代理开始，传输到发送方的邮件服务器，再传输到接收方的邮件服务器，然后在这里被分发到接收方的邮箱中。用接收方的用户想要从邮箱中读取邮件时，他的邮件服务器会对用户进行认证。如果发送方发送的邮件无法正确交付给接收方的服务器，那么发送方的用户代理会把邮件存储在一个报文队列中，并在以后尝试再次发送。通常每 30 分钟发送一次，如果一段时间后还发送不成功，服务器就会删除报文队列中的邮件并以电子邮件的方式通知发送方。现在你知道了两台邮件服务器邮件发送的大体过程，那么，SMTP 是如何将邮件从 Alice 邮件服务器发送到 Bob 的邮件服务器的呢？主要分为下面三个阶段 建立连接：在这一阶段，SMTP 客户请求与服务器的25端口建立一个 TCP 连接。一旦连接建立，SMTP 服务器和客户就开始相互通告自己的域名，同时确认对方的域名。 邮件传送：一旦连接建立后，就开始邮件传输。SMTP 依靠 TCP 能够将邮件准确无误地传输到接收方的邮件服务器中。SMTP 客户将邮件的源地址、目的地址和邮件的具体内容传递给 SMTP 服务器，SMTP 服务器进行相应的响应并接收邮件。 连接释放：SMTP 客户发出退出命令，服务器在处理命令后进行响应，随后关闭 TCP 连接。MIME 类型最一开始，互联网中的电子邮件只能处理文本格式，后来也逐渐扩展为 MIME 类型，我们上面也简单提到了一句 MIME 类型，MIME (Multipurpose Internet Mail Extensions)是用途互联网邮件扩展类型。它是一个互联网标准，扩展了电子邮件标准，使其能够支持很多格式，这些格式如下： 超文本标记语言文本 .html text/html xml文档 .xml text/xml 普通文本 .txt text/plain PNG图像 .png image/png GIF图形 .gif image/gif JPEG图形 .jpeg,.jpg image/jpeg AVI 文件 .avi video/x-msvideo 等。" }, { "title": "计算机网络应用层协议(二)", "url": "/posts/ApplyProtocol2/", "categories": "计算机网络", "tags": "学习", "date": "2023-10-01 15:30:00 +0000", "snippet": "应用层协议WWW 和 HTTP万维网 (WWW, World Wide Web)是将互联网中的信息以超文本的形式展现的系统，用来显示 WWW 结果的客户端被称为 Web 浏览器。通过浏览器，我们无需关注想要访问的内容在哪个服务器上，我们只需要知道我们想访问的内容就可以了。万维网定义了三个比较重要的概念： URI：定义了访问信息的手段和位置 HTML：定义了信息的表现形式 HTTP：定义...", "content": "应用层协议WWW 和 HTTP万维网 (WWW, World Wide Web)是将互联网中的信息以超文本的形式展现的系统，用来显示 WWW 结果的客户端被称为 Web 浏览器。通过浏览器，我们无需关注想要访问的内容在哪个服务器上，我们只需要知道我们想访问的内容就可以了。万维网定义了三个比较重要的概念： URI：定义了访问信息的手段和位置 HTML：定义了信息的表现形式 HTTP：定义了 WWW 的访问规范URI / URLURI (Uniform Resource Identifier)中文名称是统一资源标识符，使用它就能够唯一地标识互联网上的资源。URL (Uniform Resource Locator)中文名称是统一资源定位符，也就是我们俗称的网址，它实际上是 URI 的一个子集。URI 不仅包括 URL，还包括 URN（统一资源名称），它们之间的关系如下而且 URI 已经不局限于标识互联网资源，它可以作为所有资源的识别码。HTMLHTML 称为超文本标记语言，是一种标识性的语言。它包括一系列标签．通过这些标签可以将网络上的文档格式统一，使分散的 Internet 资源连接为一个逻辑整体。HTML 文本是由 HTML 命令组成的描述性文本，HTML 命令可以说明文字，图形、动画、声音、表格、链接等。HTTPWeb 的应用层协议就是HTTP (HyperText Transfer Protocol, HTTP)， 超文本传输协议，它是 Web 的核心协议。下面是 HTTP 协议中的几个核心概念。Web 页面Web 页面也叫做 Web Page，它是由无数个对象组成，一个对象就是一个文件，这个文件可以是 HTML 文件、一个图片、一段 Java 应用程序等，它们都可以通过 URI 来唯一定位。一个 Web 页面包含了很多对象，Web 页面可以说是对象的集合体。浏览器就如同各大邮箱使用电子邮件传送协议 SMTP 一样，浏览器是使用 HTTP 协议的主要载体，说到浏览器，你能想起来哪几个？随着网景大战结束后，浏览器迅速发展，至今已经出现过的浏览器主要有 IE、Firefox、Chrome、Safari、Opera、Netscape、傲游等。Web 服务器Web 服务器的正式名称叫做 Web Server，Web 服务器可以向浏览器等 Web 客户端提供文档，也可以放置网站文件，让全世界浏览；可以放置数据文件，让全世界下载。目前最主流的三个 Web 服务器是 Apache、 Nginx 、IIS。CDNCDN 是一种内容分发网络，它应用了 HTTP 协议里的缓存和代理技术，代替源站响应客户端的请求。CDN 是构建在现有网络基础之上的网络，它依靠部署在各地的边缘服务器，通过中心平台的负载均衡、内容分发、调度等功能模块，使用户就近获取所需内容，降低网络拥塞，提高用户访问响应速度和命中率。CDN 的关键技术主要有内容存储和分发技术。打比方说你要去亚马逊上买书，之前你只能通过购物网站购买后从美国发货过海关等重重关卡送到你的家里，现在在中国建立一个亚马逊分基地，你就不用通过美国进行邮寄，从中国就能把书尽快给你送到。WAFWAF 是一种 Web应用程序防护系统 (Web Application Firewall，简称 WAF)，它是一种通过执行一系列针对 HTTP / HTTPS 安全策略来专门为 Web 应用提供保护的一款产品，它是应用层面的防火墙，专门检测 HTTP 流量，是防护 Web 应用的安全技术。WAF 通常位于 Web 服务器之前，可以阻止如 SQL 注入、跨站脚本等攻击，目前应用较多的一个开源项目是 ModSecurity，它能够完全集成进 Apache 或 Nginx。WebServiceWebService 是一种 Web 应用程序，WebService 是一种跨编程语言和跨操作系统平台的远程调用技术。WebService 是一种由 W3C 定义的应用服务开发规范，使用 client-server 主从架构，通常使用 WSDL 定义服务接口，使用 HTTP 协议传输 XML 或 SOAP 消息，它是一个基于 Web（HTTP）的服务架构技术，既可以运行在内网，也可以在适当保护后运行在外网。下面我们就来认识一下应用层常见的几个协议，一些我们常用且重要的协议比如 HTTP、DNS 会在后面详细介绍，目前可以作为了解。HTTPHTTP 是一个在网络中专门在两点之间传输文字、图片、音频、视频等超文本数据的约定和规范。HTTP 是一种应用层协议，它使用 TCP 作为运输层协议。HTTP 请求响应过程让我们通过一个例子来探讨一下 HTTP 的请求响应过程，我们假设访问的 URL 地址为http://www.someSchool.edu/someDepartment/home.index，当我们输入网址并点击回车时，浏览器内部会进行如下操作 DNS 服务器会首先进行域名的映射，找到访问 www.someSchool.edu 所在的 IP 地址，然后 HTTP 客户端进程在 80 端口发起一个到服务器 www.someSchool.edu 的 TCP 连接（80 端口是 HTTP 的默认端口）。在客户和服务器进程中都会有一个套接字与其相连。 HTTP 客户端通过它的套接字向服务器发送一个 HTTP 请求报文。该报文中包含了路径 someDepartment/home.index 的资源（我们后面会详细讨论 HTTP 请求报文）。 HTTP 服务器通过它的套接字接受该报文，进行请求的解析工作，并从其存储器 ( RAM 或磁盘 ) 中检索出对象 www.someSchool.edu/someDepartment/home.index，然后把检索出来的对象进行封装，封装到 HTTP 响应报文中，并通过套接字向客户进行发送。 HTTP 服务器随即通知客户端断开 TCP 连接，实际上是需要等到客户接受完响应报文后才会断开 TCP 连接。 HTTP 客户端接受完响应报文后，TCP 连接关闭。HTTP 客户端从响应体里提取 HTML 响应文件，并检查该 HTML 文件，然后循环检查报文中其他内部对象。 检查完成后，HTTP 客户端会通过浏览器渲染把对应的资源通过显示器呈现给用户。 至此，键入网址再按下回车的过程就结束了。上述过程描述的是一种简单的请求-响应全过程，真实的请求-响应情况要比上面描述的过程复杂很多。HTTP 请求特征从上面整个过程中我们可以总结出 HTTP 进行分组传输是具有以下特征： 支持客户 - 服务器模式的问答形式。 简单快速：客户向服务器请求服务时，只需传送请求方法和路径。请求方法常用的有 GET、HEAD、POST。每种方法规定了客户与服务器联系的类型不同。由于 HTTP 协议简单，使得 HTTP 服务器的程序规模小，因而通信速度很快。 灵活：HTTP 允许传输任意类型的数据对象。正在传输的类型由 Content-Type 加以标记。 无连接：无连接的含义是限制每次连接只处理一个请求。服务器处理完客户的请求，并收到客户的应答后，即断开连接。采用这种方式可以节省传输时间。 无状态：HTTP 协议是无状态协议。无状态是指协议对于事务处理没有记忆能力。缺少状态意味着如果后续处理需要前面的信息，则它必须重传，这样可能导致每次连接传送的数据量增大。另一方面，在服务器不需要先前信息时它的应答就较快。 持久连接和非持久连接我们上面描述的 HTTP 请求响应过程就是一种非持久性连接，因为每次 TCP 在传递完报文后，都会关闭 TCP 连接，每个 TCP 连接只传输一个请求报文和响应报文。非持久性连接有一些缺点： 首先必须为每个请求的对象建立和维护一个全新的连接。 对于每个这样的连接来说，在客户端和服务器中都要分配 TCP 的缓冲区，这无疑给 Web 服务器带来了严重的负担。因为一台 Web 服务器可能要同时服务于数百甚至上千个客户请求。 在采用 HTTP/1.1 持久连接的情况下，服务器在发送响应后会保持该 TCP 连接，后续的请求和响应报文能够通过相同的连接进行传输。如果在一段时间内（可配置）该连接并未再次使用，HTTP 服务器就会断开连接。HTTP 报文格式我们上面描述了一下 HTTP 的请求响应过程，相信你对 HTTP 有了更深的认识，下面我们就来一起认识一下 HTTP 的报文格式是怎样的。HTTP 协议主要由三大部分组成： 起始行（start line）：描述请求或响应的基本信息。 头部字段（header）：使用 key-value 形式更详细地说明报文。 消息正文（entity）：实际传输的数据，它不一定是纯文本，可以是图片、视频等二进制数据。 其中起始行和头部字段并成为请求头或者响应头，统称为 Header；消息正文也叫做实体，称为 body。HTTP 协议规定每次发送的报文必须要有 Header，但是可以没有 body，也就是说头信息是必须的，实体信息可以没有。而且在 header 和 body 之间必须要有一个空行（CRLF）。如果用一幅图来表示一下 HTTP 请求的话，我觉得应该是下面这样如果细化一点的话，那就是下面这样：这幅图需要注意一下，如果使用 GET 方法，是没有实体体的，如果你使用的是 POST 方法，才会有实体体。当用户提交表单时，HTTP 客户端通常使用 POST 方法；与此相反，HTML 表单的获取通常使用 GET 方法。HEAD 方法类似于 GET 方法，只不过 HEAD 方法不会返回对象。下面我们来看一下 HTTP 响应报文可以看到，请求报文和响应报文只有请求头是不同的，其他信息均一致。请求报文请求行：GET /some/page.html HTTP/1.1响应报文：HTTP/1.1 200 OKCookie 和 SessionHTTP 协议是一种无状态协议，即每次服务端接收到客户端的请求时，都是一个全新的请求，服务器并不知道客户端的历史请求记录；但是一些网站却需要记住客户信息，比如电商网站客户登录之后选购商品，客户把商品加入购物车之后会跳转到结算页面，这个时候网站需要知道客户信息，如果使用无状态的 HTTP 协议，那么客户根本无法结算。Session 和 Cookie 的主要目的就是为了弥补 HTTP 的无状态特性。Session 是什么客户端请求服务端，服务端会为这次请求开辟一块内存空间，这个对象便是 Session 对象，存储结构是一个 Map 映射，具体一点是 ConcurrentHashMap。Session 弥补了 HTTP 无状态特性，服务器可以利用 Session 存储客户端在同一个会话期间的一些操作记录。Session 如何判断是否是同一会话服务器第一次接收到请求时，开辟了一块 Session 空间（创建了 Session 对象），同时生成一个 sessionId ，并通过响应头的 Set-Cookie：JSESSIONID=XXXXXXX 命令，向客户端发送要求设置 Cookie 的响应； 客户端收到响应后，在本机客户端设置了一个 JSESSIONID=XXXXXXX 的 Cookie 信息，该 Cookie 的过期时间为浏览器会话结束；接下来客户端每次向同一个网站发送请求时，请求头都会带上该 Cookie信息（包含 sessionId ），然后服务器通过读取请求头中的 Cookie 信息，获取名称为 JSESSIONID 的值，得到此次请求的 sessionId。Session 机制有个缺点，比如 A 服务器存储了 Session，就是做了负载均衡后，假如一段时间内 A 的访问量激增，会转发到 B 进行访问，但是 B 服务器并没有存储 A 的 Session，会导致 Session 的失效。Cookies 是什么HTTP 协议中的 Cookie 包括 Web Cookie 和浏览器 Cookie，它是服务器发送到 Web 浏览器的一小块数据。服务器发送到浏览器的 Cookie，浏览器会进行存储，并与下一个请求一起发送到服务器。通常，它用于判断两个请求是否来自于同一个浏览器，例如用户保持登录状态。HTTP Cookie 机制是 HTTP 协议无状态的一种补充和改良。Cookie 主要用于下面三个目的 会话管理：登陆、购物车、游戏得分或者服务器应该记住的其他内容。 个性化：用户偏好、主题或者其他设置。 追踪：记录和分析用户行为。 Cookie 曾经用于一般的客户端存储。虽然这是合法的，因为它们是在客户端上存储数据的唯一方法，但如今建议使用现代存储 API。Cookie 随每个请求一起发送，因此它们可能会降低性能（尤其是对于移动数据连接而言）。创建 Cookie当接收到客户端发出的 HTTP 请求时，服务器可以发送带有响应的 Set-Cookie 标头，Cookie 通常由浏览器存储，然后将 Cookie 与 HTTP 标头一同向服务器发出请求。Set-Cookie 和 Cookie 标头Set-Cookie HTTP 响应标头将 cookie 从服务器发送到用户代理。下面是一个发送 Cookie 的例子：此标头告诉客户端存储 Cookie。现在，随着对服务器的每个新请求，浏览器将使用 Cookie 头将所有以前存储的 Cookie 发送回服务器。有两种类型的 Cookies，一种是 Session Cookies，一种是 Persistent Cookies，如果 Cookie 不包含到期日期，则将其视为会话 Cookie。会话 Cookie 存储在内存中，永远不会写入磁盘，当浏览器关闭时，此后 Cookie 将永久丢失。如果 Cookie 包含有效期 ，则将其视为持久性 Cookie。在到期指定的日期，Cookie 将从磁盘中删除。还有一种是 Cookie 的 Secure 和 HttpOnly 标记，下面依次来介绍一下：会话 Cookies上面的示例创建的是会话 Cookie ，会话 Cookie 有个特征，客户端关闭时 Cookie 会删除，因为它没有指定Expires 或 Max-Age 指令。但是，Web 浏览器可能会使用会话还原，这会使大多数会话 Cookie 保持永久状态，就像从未关闭过浏览器一样。永久性 Cookies永久性 Cookie 不会在客户端关闭时过期，而是在特定日期 ( Expires ) 或特定时间长度 ( Max-Age ) 外过期。例如Set-Cookie: id=a3fWa; Expires=Wed, 21 Oct 2015 07:28:00 GMT;对 Cookie 的争论尽管 Cookie 能够简化用户的网络活动，但是 Cookie 的使用存在争议，因为不少人认为它对用户是一种侵权行为。因为结合 Cookie 和用户提供的账户信息，Web 站点可以知道更多关于用户的信息。" }, { "title": "数据库分库分表", "url": "/posts/DataBaseSplit/", "categories": "技术科普", "tags": "学习", "date": "2023-09-25 14:20:00 +0000", "snippet": "数据库分库分表分库分表关系型数据库容易造成性能瓶颈，单机存储容量，连接数，处理能力都有限。（当单表数据量达到1000w或100G以后），当查询维度较多时，即使添加从库，优化索引等性能仍下降严重，此时可以考虑进行数据库拆分，拆分目的在于减少数据库的负担，缩短查询时间。数据库分布式的核心内容关键是数据切分（Sharding）及切分后对数据的定位，整合。将数据分散到各个数据库中，使得每一数据库的性...", "content": "数据库分库分表分库分表关系型数据库容易造成性能瓶颈，单机存储容量，连接数，处理能力都有限。（当单表数据量达到1000w或100G以后），当查询维度较多时，即使添加从库，优化索引等性能仍下降严重，此时可以考虑进行数据库拆分，拆分目的在于减少数据库的负担，缩短查询时间。数据库分布式的核心内容关键是数据切分（Sharding）及切分后对数据的定位，整合。将数据分散到各个数据库中，使得每一数据库的性能达到较好的状态，达到提升数据库操作性能的目的。切分类型：垂直切分和水平切分垂直切分常见有垂直分库和垂直分表垂直切分垂直分库根据 业务耦合性，将关联度低的不同表存储在不同数据库中。垂直分表基于数据库中的 列 进行，某个表字段多，可以新建一张扩展表，将不常用或字段长度较大的字段拆分出去到扩展表中，通过 大表拆小表 ，便于开发与维护，也能避免 跨页问题 （在分页查询时，如果有大量数据的情况下，可能会出现跳页或漏页的情况，MySQL底层通过数据页存储，一条记录占用空间过大会导致跨页，造成额外的性能开销）这种拆分方式主要基于字段的逻辑关系和使用频率等考虑因素，将不同的字段分布到不同的表中，以提高查询性能和减少数据冗余。注意：垂直分表也会带来一些挑战，包括处理跨表查询的复杂性、维护数据一致性和关联性等。在使用垂直分表时，需要综合考虑具体的业务需求、查询模式和数据特点，设计合适的分表策略，并在应用层级上处理相关的数据关联和查询操作。水平切分它将一个数据库中的数据按照某种规则分散到多个独立的数据库或多个表中。每个表都包含部分数据行，从而实现数据的分布式存储和处理。常用切分规则：1.范围切分：根据数据范围将数据划分到不同的节点，例如可以根据用户ID的范围进行划分，节点1存储用户ID从0到10000的数据，节点2存储用户从10000到20000的数据。2.散列切分：使用数据的散列值来确定数据应该存储在哪个节点上，使用散列函数对某个数据属性进行散列计算，并将结果映射到节点ID，保证数据在节点上均匀分布。分库分表带来的问题1.数据一致性：在分库分表环境下，数据的一致性变得更加复杂。跨多个数据库或表的事务操作变得更加困难，需要采用分布式事务或其他一致性保证机制来确保数据的一致性。2.跨节点查询：当需要跨多个数据库或表进行查询时，性能会受到影响。跨节点查询可能需要在多个节点上进行并行查询，增加了网络开销和响应时间。3.数据迁移和扩缩容：在分库分表环境中，数据的迁移和节点的扩缩容变得复杂。当需要增加或减少节点时，需要进行数据迁移和重新分片，这涉及到数据的重新分布和迁移，可能导致服务中断、数据不一致等问题。4.跨节点事务：在分库分表环境中，跨多个数据库或表的事务操作变得更加复杂。保持跨节点事务的原子性和一致性需要使用分布式事务或其他协调机制，增加了系统的复杂性和开发的难度。5.查询和分析复杂性：在分库分表环境中，对于需要跨多个数据库或表进行查询和分析的复杂查询操作变得更加困难。需要使用联合查询、分布式计算等技术来处理分片的数据，并进行结果的合并和处理。6.应用层逻辑复杂性：分库分表会引入应用层的逻辑复杂性。应用程序需要处理数据的分布和路由逻辑，管理多个数据库连接和事务，并处理跨节点查询和一致性问题。7.数据倾斜：在分库分表环境中，数据可能会出现不均匀分布的情况，导致某些节点或表的负载过重，而其他节点或表的负载较轻。这可能影响系统的性能和可扩展性，需要进行负载均衡和数据重分布的优化。" }, { "title": "计算机网络应用层协议(一)", "url": "/posts/ApplyProtocol/", "categories": "计算机网络", "tags": "学习", "date": "2023-09-18 13:30:00 +0000", "snippet": "应用层协议在介绍完计算机基础知识和 TCP/IP 协议簇的基础概念之后，相信读者对计算机网络有了初步的认识，那么下面我们就要对不同的协议层进行分类介绍了，我们选用从应用层产生数据开始，逐步下探至数据链路层，因为这种介绍对读者来说更容易接纳，吸收程度更好。 具体内容看上一篇文章：[计算机网络TCP/IP基础 YKFire](https://ykfire.gith...", "content": "应用层协议在介绍完计算机基础知识和 TCP/IP 协议簇的基础概念之后，相信读者对计算机网络有了初步的认识，那么下面我们就要对不同的协议层进行分类介绍了，我们选用从应用层产生数据开始，逐步下探至数据链路层，因为这种介绍对读者来说更容易接纳，吸收程度更好。 具体内容看上一篇文章：[计算机网络TCP/IP基础 YKFire](https://ykfire.github.io/posts/NetworkTCPandIP/) 一般情况下，用户不会在意网络应用程序是按照怎样的机制运行的，用户也不会关心它们是如何产生数据的，如何经过协议栈层层包装把数据发出去的，但是我们是程序员，必须要专业一点，势必要掌握好理论知识然后在合适的时间讲出来好让别人认为我们是大佬 ：）应用层位于 TCP/IP 模型的最上层，同时应用层也对应 OSI 标准模型的第 5、6、7层，也就是会话层、表现层、应用层。应用层概念现如今，越来越多的应用程序通过计算机网络实现相互通信，这些应用包括 Web 浏览器、远程登录、电子邮件、文件传输、文件下载等，而这些应用的日常使用势必要遵循某种协议和规范，应用层协议正是进行这些行为活动的规则和标准。简单来说，应用层就是规定应用程序在进行通信时所遵循的协议。应用层协议的定义应用层协议定义了在不同端系统上应用程序是如何相互传输报文的。一般来说，应用层协议会规定如下内容： 交换的报文类型：交换的是请求报文还是响应报文。 报文字段的解释：对报文中各个字段的详细描述。 报文字段的语义：报文各个字段的含义是什么。 报文交换时间、方式：程序何时、以什么方式发送报文以及响应。直白一点来说，应用层只是产生和使用数据的逻辑层，在这一层次我们并不会关心它们是如何发送数据的以及数据发到哪里。应用层有两种层次结构。应用层体系结构应用层体系结构定义了应用层端系统之间数据交换的方式，一般来说，主流的体系结构有两种： 客户-服务器体系结构 - CS 结构 (client-server architecture) 对等体系结构 - P2P 结构 (P2P architecture)在客户-服务器体系结构中，分为请求方和服务方。有一个总是打开的主机称为服务端 (Server)，它向客户端 (client) 提供服务。客户端会发送请求给服务端，服务端会根据客户端的请求做出响应。举一个简单的例子：我今天下馆子去，找到了一个饭店，这时候会有一个服务员热情接见我，我看完菜单给服务员说我想吃猪肉，这个”我想吃猪肉”就是请求，然后服务员报以热情的微笑并扇了我一巴掌，说：”我们这是清真馆”，这个服务员对我笑并扇了我一巴掌然后对我说了句话，就是响应。我们最常见的服务端就是 Web 服务器，Web 服务器提供于来自浏览器的请求。我们日常访问百度、谷歌，其实就是在访问它们的 Web 服务器。一般常见的 Web 服务器主要有Apache、IIS、Jboss、Tomcat、WebSphere、WebLogic 等。当 Web 服务器通过浏览器接收到用户请求后，它会经过一系列的处理把信息或者页面等通过浏览器呈现给用户，这种模式就是客户 - 服务器模式。这里有两点需要注意 在客户 - 服务器模式下，通常客户端彼此之间是并不互相通信的。 服务器通常具有固定的、周知的 IP 地址可以提供访问。客户 - 服务器模式通常会出现随着客户数量的急剧增加导致单台服务器无法满足大量请求的情况。为此，通常需要配备大量的数据中心 (data center)，用来跟踪所有的用户请求。与此相反，P2P 对等体系结构对这种数据中心的依赖性很低，因为在 P2P 体系结构中，应用程序在两个主机之间直接通信，这些主机被称为对等方，与有中心服务器的中央网络系统不同，对等网络的每个用户端既是一个请求方也是一个服务提供方。常见的 P2P 体系结构的应用有文件共享、视频会议、网络电话等。举个例子：毕业了很多大学生都有许多个人物品来不及处理，这时候大家会搞一个跳蚤市场，来彼此出售、交换一些个人物品，这里面每个人既是请求方，同时也是服务提供方。说到跳蚤市场，啰嗦两句，一般这种跳蚤市场有很多好东西，好用不贵，而且卖的东西种类繁多，物品齐全，性价比贼高，懂得都懂。P2P 一个最大的特点就是扩展性 (self-scalability)，因为 P2P 网络的一个重要的目标就是让所有的客户端都能提供资源、获取资源，共享带宽，存储空间等。因此，当有更多节点加入且对系统请求增多，整个系统的容量也增大。这是具有一组固定服务器的客户 - 服务器结构不具备的，这也就是 P2P 的优势。在了解完上述这两种结构之后，我们把关注点放在这两种体系结构下，各个端系统之间是如何进行通信的。进程通信我们上面说到了两种体系结构，一种是客户 - 服务器 CS 模式，一种是 P2P 对等模式。我们都知道一个计算机允许同时运行多个应用程序，在我们看起来这些应用程序好像是同时运行的，那么它们之间是如何通信的呢？从操作系统的角度上来说，进行通信的实际上是进程 (process) 而不是程序。我们一般所讲的是进程通信，但是没人说程序通信的。进程是一类程序，但是程序并不特指的是进程。一个进程可以被认为是运行在端系统中的程序。当多个进程运行在相同的端系统上时，它们使用进程间的通信机制相互通信。进程间的通信规则由操作系统来确定。进程与计算机网络之间的接口计算机是庞大且繁杂的，计算机网络也是，应用程序不可能只有一个进程组成，它同样是多个进程共同作用协商运行，那么分布在多个端系统之间的进程是如何进行通信的呢？实际上，每个进程中会有一个套接字 (socket)的，它其实是一个软件接口，套接字也是应用程序的内部接口，应用程序可以通过它发送或接收数据，同时也可以对其进行像对文件一样的打开、读写和关闭等操作。举个例子来简单类比一下套接字和网络进程：进程可类比一座房子，而它的套接字相当于是房子的门，当一个进程想要与其他进程进行通信时，它会把报文推出门外，然后通过运输设备把报文运输到另外一座房子，通过门进入房子内部，提供给其他进程使用。下图是一个通过套接字进行通信的示意图。从上图可以看到，Socket 属于主机或者服务进程的内部接口，由应用程序开发人员进行控制，两台端系统之间进行通信会通过 TCP 缓冲区经由网络传输到另一个端系统的 TCP 缓冲区，Socket 从 TCP 缓冲区读取报文供应用程序内部使用。套接字是建立网络应用程序的可编程接口，因此套接字也被称为应用程序和网络之间的应用程序编程接口( Application Programming Interface，API )。应用程序开发人员可以控制套接字内部细节，但是无法控制运输层的传输，只能对运输层的传输协议进行选择，还可以对运输层的传输参数进行选择，比如最大缓存和最大报文长度等。关于更多 Socket 的内容，我们会在后面进行讨论。进程寻址我们上面提到网络应用程序之间会相互发送报文，那么它是如何知道自己应该向哪个应用进程发送呢？是不是存在某种机制能够让它知道发到哪里？这就好比你要发送电子邮件，你写好了内容但是你不知道给谁发，所以这个时候必须要有一种知道对方地址的机制，并且能够辨明对方唯一的地址，这个地址就是IP 地址。我们会在后面的内容中详细讨论 IP 地址的内容，目前只需要知道 IP 地址是一个 32 比特的数据并且能够唯一标示互联网中任意一台主机的地址就可以了。只知道 IP 地址是否就可以了呢？当然不行，举个例子：IP 地址就相当于是你在地图上的唯一坐标，比如说是河北省石家庄市xx小区xx栋xx层xx号，可以具体到门牌号，但是你家里可能会有多个人，你到底要给家里的谁发送信息？这个需要明确下，所以上面的门牌号，就相当于是 IP 地址，你要给家里的谁发送消息，这个谁就是端口号。例如，Web 应用程序需要用 80 端口来标示，邮件服务器程序需要使用 25 来标示，这个谁就是 80 或者 25 端口。应用程序如何选择运输服务我们知道应用程序是按照应用层协议的标准传输数据的，应用程序只是起到一个生产数据和应用数据的程序，那么我们该如何发送报文呢？这就好比你知道目的地是哪里了，你应该选择哪种交通工具？是走路，公交，地铁还是打车？应用程序发送报文的交通工具的选择也有很多，我们可以从下面这几个方面来进行考量。 数据传输是否可靠分组在计算机网络中会存在丢包问题，丢包问题的严重性和网络协议的性质有关，如果像是电子邮件、文件传输、远程主机、Web 文档传输的过程中出现问题，数据丢失可能会造成非常严重的后果。如果像是网络游戏，多人视频会议造成的影响可能比较小。鉴于此，数据传输的可靠性也是首先需要考虑的问题。因此，如果协议提供了这样的确保数据交付的服务，就认为提供了可靠数据传输，能够忍受数据丢失的应用被称为容忍丢失的应用。 吞吐量吞吐量就是在数据传输过程中，发送进程能够向接收进程交付比特的速率。具有吞吐量要求的应用程序被称为带宽敏感的应用。带宽敏感的应用具有特定的吞吐量要求，而弹性应用能够根据当时可用的带宽或多或少地利用可供使用的吞吐量。 定时定时能够确保网络中两个应用程序的收发能否在指定的时间内完成，这也是应用程序选择运输服务需要考虑的一个因素。比如在游戏中，你一包数据迟迟发送不过去，对面都推塔了你还卡在半路上呢。 安全性最后，选择运输协议一定要能够为应用程序提供一种或多种安全性服务，这个也是非常重要的。因特网能够提供的运输服务说完运输服务的选型，接下来该聊一聊因特网能够提供哪些服务了。实际上，因特网为应用程序提供了两种运输层的协议，即 UDP 和 TCP，下面是一些网络应用的选择要求，可以根据需要来选择适合的运输层协议。下面我们就来聊一聊这两种运输协议的应用场景：TCPTCP 服务模型的特性主要有下面几种： 面向连接的服务在应用层数据报发送后， TCP 会让客户端和服务器互相交换运输层控制信息，交换信息前需要先建立一条连接通道，这个建立连接的过程就是握手。这个握手过程就是提醒客户端和服务器，你们准备好接收数据了吗？握手完成后，一个TCP 连接 (TCP Connection) 就此建立。TCP 连接是一条全双工的，全双工的意思是连接双方的进程都可以在此连接上同时进行收发报文。当应用程序结束报文发送后，必须断开连接。 可靠的数据传输通信进程能够依靠 TCP 无差错、按适当顺序交付所有发送的数据。应用程序能够依靠 TCP 将相同的字节流交付给接收方的套接字，没有字节的丢失和冗余。 拥塞控制当接收方和发送方之间的网络出现拥塞时，TCP 的拥塞控制会抑制发送进程（客户端或服务器），等到网络状态恢复正常后再继续发送。TCP 的拥塞控制并不一定为通信进程带来直接好处，但能为因特网带来整体好处。我们会在后面具体探讨拥塞控制。UDPUDP 是一种轻量级的传输协议，UDP 是无连接的，因此使用 UDP 协议不需要经过握手过程，直接发就行。UDP 也不会保证报文是否传输到服务端，它就像是一个撒手掌柜。不仅如此，到达接收进程的报文也可能是乱序到达的。但是 UDP 的传输速率非常快，非常适合大多数互联网应用。下面是上表列出来的一些应用所选择的协议：" }, { "title": "计算机网络TCP/IP基础", "url": "/posts/NetworkTCPandIP/", "categories": "计算机网络", "tags": "学习", "date": "2023-09-11 13:27:00 +0000", "snippet": "计算机网络TCP/IP基础要说我们开发人员接触计算机网络最多的协议，那势必离不开 TCP/IP 协议了，TCP/IP 协议同时也是互联网中最为著名的协议，下面我们就来聊一下 TCP/IP 协议。TCP/IP 协议的历史背景最初还没有 TCP/IP 协议的时候，也就是在 20 世纪 60 年代，许多国家和地区认识到通信技术的重要性。美国国防部希望能够研究一种即使通信线路被破坏也能够通过其他路线...", "content": "计算机网络TCP/IP基础要说我们开发人员接触计算机网络最多的协议，那势必离不开 TCP/IP 协议了，TCP/IP 协议同时也是互联网中最为著名的协议，下面我们就来聊一下 TCP/IP 协议。TCP/IP 协议的历史背景最初还没有 TCP/IP 协议的时候，也就是在 20 世纪 60 年代，许多国家和地区认识到通信技术的重要性。美国国防部希望能够研究一种即使通信线路被破坏也能够通过其他路线进行通信的技术。为了实现这种技术，出现了分组网络。如上图所示，在两个节点通信的过程中，即使几个节点遭到破坏，却依然能够通过改变线路等方式达使两个节点之间进行通信。这种分组网络促进了阿帕网-ARPANET(Advanced Research Projects Agency Network) 的诞生。ARPANET 是第一个具有分布式控制的广域包分组交换网络，也是最早实现 TCP/IP 协议的前身。ARPANET 其实是由美国国防部高级研究计划局计划建立。所以，计算机网络的出现在最一开始是因为军事研究目的。20 世纪 90 年代，IOS 开展了 OSI 这一国际标准化的进程，然而却没有取得实质性的进展，但是却使 TCP/IP 协议得到了广泛使用。这种致使 TCP/IP 协议快速发展的原因是由于 TCP/IP 的标准化。也就是说 TCP/IP 协议中会涉及到 OSI 所没有的标准，而这种标准将是我们接下来主要探讨的内容。TCP/IP 协议说的不仅仅只是 TCP 和 IP 这两种协议，实际上，TCP/IP 指的是协议簇，协议簇简单来说就是一系列协议的综合，如果下次再问你 TCP/IP 协议有哪些的话，你就可以拿下面这幅图来告诉他。上图只是列出来了一部分各个层次之间的重点协议，实际上 TCP/IP 协议簇中的协议数量比列出来的要多。TCP/IP 标准TCP/IP 相较于其他协议的标准，更注重两点：开放性和实用性。开放性说的是 TCP/IP 是由 IETF 讨论制定的，而 IETF 本身就是一个允许任何人加入进行讨论的组织。实用性说的是就拿框架来说，如果只浮于理论，而没有落地的实践，那么永远成为不了主流，即标准化能否被实际使用。TCP/IP 的标准协议就是我们所熟知的 RFC 文档，当然你可以在网络上看到。RFC 不仅规范了协议标准，还包含了协议的实现和使用信息。关于更多 RFC 协议，可以从官方文档查阅 https://www.rfc-editor.org/rfc-index.html 。TCP/IP 协议簇下面我们就开始聊一聊 TCP/IP 协议簇，这也是我们大多数程序员应重点关注的部分。TCP/IP 协议是我们程序员接触最多的协议，OSI 模型共有七层，从下到上分别是物理层、数据链路层、网络层、运输层、会话层、表示层和应用层。但是这显然是有些复杂的，所以在 TCP/IP 协议中，它们被简化为了四个层次。下面我们从通信链路层开始介绍一下这些层以及与层之间的协议。通信链路层再细分一点，通信链路层还可以分为物理层和数据链路层。物理层物理层是 TCP/IP 的最低层，物理层为传输数据创造了底层环境，物理层所关注的大多是物理设备的接口标准，它所关注的数据单位是比特流，物理层会将比特流转换为电信号或者光信号实现物理传输，也就是说物理层确保原始数据能够在各种物理媒体上传输。数据链路层另外一层是数据链路层，数据链路层位于物理层和网络层中间，数据链路层定义了在单个链路上如何传输数据，数据链路层的数据单位是帧，它提供了在单个链路上的诸如帧同步、流量控制和差错检测等技术来确保帧传输的完整性和可靠性，并且将来自物理层的数据可靠的传输到相邻节点的网络层。网络层网络层位于数据链路层和传输层之间，网络层主要使用 IP 协议，IP 协议基于 IP 地址转发分包数据。IP 协议的主要作用就是将分组发送到目标主机。TCP/IP 协议中网络层与传输层的功能通常由操作系统提供。IP 还隐含着数据链路层的功能，通过 IP 协议，相互通信的主机之间不论经过怎样的底层数据链路，都能够实现相互通信。虽然 IP 是一种分组交换协议，但是 IP 却不具备重发机制。即使数据没有到达另一端也不会进行重发，所以 IP 属于不可靠的协议。网络层还有一种协议是 ICMP，因为 IP 在数据包的发送过程中可能会出现异常，当 IP 数据包因为异常而无法到达目标地址时，需要给发送端发送一个异常通知，ICMP 的主要功能就在于此了。鉴于此情况，ICMP 也可以被用来诊断网络情况。传输层我们上面刚介绍完 TCP/IP 协议最重要的 IP 协议后，下面我们来介绍一下传输层协议。传输层就好像高速公路一样，连接两个城市的道路。下面是互联网的逻辑通道，你可以把它想象成为高速公路。传输层最主要的功能就是让应用程序之间完成通信和数据交换。在计算机内部运行着很多应用程序，每个应用程序都对应一个端口号，我们一般使用端口号来区分这些应用程序。传输层的协议主要分为面向有连接的协议 TCP 和面向无连接的协议 UDP。TCPTCP 是一种可靠的协议，它能够保证数据包的可靠性交付，TCP 能够正确处理传输过程中的丢包、传输顺序错乱等异常情况。此外，TCP 还提供拥塞控制用于缓解网络拥堵。UDPUDP 是一种不可靠的协议，它无法保证数据的可靠交付，相比 TCP ，UDP 不会检查数据包是否到达、网络是否阻塞等情况，但是 UDP 的传输效率比较高。UDP 常用于分组数据较少或者广播、多播等视频通信和多媒体领域。应用层在 TCP/IP 协议簇中，将 OSI 标准模型中的会话层、表示层都归为了应用层。应用层的架构大多属于客户端/服务端模型，提供服务的程序叫做服务端、接受服务的程序叫做客户端。在这种架构中，服务端通常会提前部署到服务器上，等待客户端的连接，从而提供服务。数据包的发送历程下面我们来介绍一下一个数据包是如何经过应用层、运输层、网络层和通信链路层进行传输的。数据包结构我们首先先来认识一下数据包的结构，如下图所示。在上面的每个分层中，每一层都会对要发送的数据增加一个首部，这个首部中包含了该层必要的信息。下面我们就来聊一聊数据包的发送过程以及每一层都添加了哪些必要信息。数据包发送历程假设主机 A 和主机 B 进行通信，主机 A 想要向主机 B 发送一个数据包，都会经历哪些操作？应用层的处理****主机 A 也就是用户点击了某个应用或者打开了一个聊天窗口输入了 cxuan，然后点击了发送，那么这个 cxuan 就作为一个数据包遨游在了网络中，第一步，应用层需要对这个数据包进行处理，包括字符编码、格式化等等，这一层其实是 OSI 中表现层做的工作，只不过在 TCP/IP 协议中都归为了应用层。数据包在发送的那一刻建立 TCP 连接，这个连接相当于通道，在这之后其他数据包也会使用通道传输数据。传输层的处理****为了描述信息能准确的到达另一方，我们使用 TCP 协议来进行描述。TCP 会根据应用的指示，负责建立连接、发送数据和断开连接。TCP 会在应用数据层的前端附加一个 TCP 首部字段，TCP 首部包含了源端口号和目的端口号，这两个端口号用于表明数据包是从哪里发出的，需要发送到哪个应用程序上；TCP 首部还包含序号，用以表示该包中数据是发送端整个数据中第几个字节的序列号；TCP 首部还包含校验和，用于判断数据是否损坏，随后将 TCP 头部附加在数据包的首部发送给 IP。网络层的处理****网络层主要负责处理数据包的是 IP 协议，IP 协议将 TCP 传过来的 TCP 首部和数据结合当作自己的数据，并在 TCP 首部的前端加上自己的 IP 首部。因此，IP 数据包后面会紧跟着 TCP 数据包，后面才是数据本身。IP 首部包含目的和源地址，紧随在 IP 首部的还有用来判断后面是 TCP 还是 UDP 的信息。IP 包生成后，会由路由控制表判断应该发送至哪个主机，IP 修饰后的数据包继续向下发送给路由器或者网络接口的驱动程序，从而实现真正的数据传输。如果不知道目标主机的 IP 地址，可以利用 ARP(Address Resolution Protocol)地址解析协议进行查找。通信链路层的处理经由 IP 传过来的数据包，以太网会给数据附上以太网首部并进行发送处理。以太网首部包含接收端的 MAC 地址、发送端的 MAC 地址以及标志以太网类型的以太网数据协议等。下面是完整的处理过程和解析过程。如上图所示，左侧是数据的发送处理过程，应用层的数据经过层层处理后会变为可以发送的数据包，经过物理介质发送至指定主机中。数据包的接收流程是发送流程的逆序过程，数据包的解析同样也会经过下面这几步。通信链路的解析****目标主机收到数据包后，首先会从以太网的首部找到 MAC 地址判断是否是发给自己的数据包，如果不是发给自己的数据包则会丢弃该数据包。如果收到的数据包是发送给自己的，就会查找以太网类型判断是哪种协议，如果是 IP 协议就会扔给 IP 协议进行处理，如果是 ARP 协议就会扔给 ARP 协议进行处理。如果协议类型是一种无法识别的协议，就会将该数据包直接丢弃。网络层的解析****经过以太网处理后的数据包扔给网络层进行处理，我们假设协议类型是 IP 协议，那么，在 IP 收到数据包后就会解析 IP 首部，判断 IP 首部中的 IP 地址是否和自己的 IP 地址匹配，如果匹配则接收数据并判断上一层协议是 TCP 还是 UDP；如果不匹配则直接丢弃。注意：在路由转发的过程中，有的时候 IP 地址并不是自己的，这个时候需要借助路由表协助处理。传输层的处理****在传输层中，我们默认使用 TCP 协议，在 TCP 处理过程中，首先会计算一下校验和，判断数据是否被损坏。然后检查是否按照序号接收数据，最后检查端口号，确定具体是哪个应用程序。数据被完整的识别后，会传递给由端口号识别的应用程序进行处理。应用程序的处理接收端指定的应用程序会处理发送方传递过来的数据，通过解码等操作识别出数据的内容，然后把对应的数据存储在磁盘上，返回一个保存成功的消息给发送方，如果保存失败，则返回错误消息。上面是一个完整的数据包收发过程，在上面的数据收发过程中，涉及到不同层之间的地址、端口号、协议类型等，那么我们现在就来剖析一下。数据包经过每层后，该层协议都会在数据包附上包首部，一个完整的包首部图如下所示。在数据包的发送过程中，各层以此对数据包添加了首部信息，每个首部都包含发送端和接收端地址以及上一层的协议类型。以太网会使用 MAC 地址、IP 会使用 IP 地址、TCP/UDP 则会用端口号作为识别两端主机的地址。此外，每个分层中的包首部还包含一个识别位，它是用来标识上一层协议的种类信息。" }, { "title": "计算机网络发展史(三)", "url": "/posts/NetworkThree/", "categories": "计算机网络", "tags": "学习", "date": "2023-09-04 13:50:00 +0000", "snippet": "网络核心概念计算机网络中，有一些核心概念是大家需要知道和理解的。传输方式网络根据传输方式可以进行分类，一般分为面向连接型和面向无连接型。 面向连接型中，在发送数据之前，需要在主机之间建立一条通信线路。 面向无连接型则不要求建立和断开连接，发送方可用于任何时候发送数据。接收端也不知道自己何时从哪里接收到数据。分组交换先来介绍一下端系统的概念。处在互联网边缘部分的机器，也就是互联网终端主机，...", "content": "网络核心概念计算机网络中，有一些核心概念是大家需要知道和理解的。传输方式网络根据传输方式可以进行分类，一般分为面向连接型和面向无连接型。 面向连接型中，在发送数据之前，需要在主机之间建立一条通信线路。 面向无连接型则不要求建立和断开连接，发送方可用于任何时候发送数据。接收端也不知道自己何时从哪里接收到数据。分组交换先来介绍一下端系统的概念。处在互联网边缘部分的机器，也就是互联网终端主机，它们官方一点的话术就是端系统。在互联网应用中，每个终端系统都可以彼此交换信息，这种信息也被称为报文(Message)，报文是一个超集的概念，它可以包括你想要的任何东西，比如文字、数据、电子邮件、音频、视频等。为了从源目的地向端系统发送报文，需要把长报文切分为一个个小的数据块，这种数据块称为分组(Packets)，也就是说，报文是由一个个小块的分组组成。在分组的传输过程中，每个分组都要经过通信链路和分组交换机，分组要在端系统之间传输需要经过一定的时间，如果两个端系统之间需要传输的分组为 L 比特，链路的传输速率问 R 比特/秒，那么传输时间就是 L / R秒。一个端系统需要经过交换机给其他端系统发送分组，当分组到达分组交换机时，交换机会如何操作？交换机会直接进行转发吗？不是的，交换机可没有这么无私，你想让我帮你转发分组？好，首先你需要先把整个分组数据都给我，我再考虑给你发送的问题，这就是存储转发传输。存储转发传输下面是一个存储转发传输的示意图。由上图可以看出，分组 1、2、3 向交换器进行分组传输，并且交换机已经收到了分组 1 发送的比特，此时交换机会直接进行转发吗？答案是不会的，交换机会把你的分组先缓存在本地。这就和考试作弊一样，一个学霸要经过学渣 A 给学渣 B 传答案，学渣 A 在收到答案后，它可能直接把卷子传过去吗？学渣 A 说，等我先把答案抄完（保存功能）后再把卷子给你，当然一个及其有素质的学渣就另说了。排队时延和分组丢失什么？你认为交换机只能和一条通信链路进行相连？那你就大错特错了，这可是交换机啊，怎么可能只有一条通信链路呢？所以我相信你一定能想到这个问题，当多个端系统同时给交换器发送分组，一定存在顺序到达和排队问题。事实上，对于每条相连的链路，该分组交换机都会有一个输出缓存(output buffer)和 输出队列(output queue) 与之对应，它用于存储路由器准备发往每条链路的分组。如果到达的分组发现路由器正在接收其他分组，那么新到达的分组就会在输出队列中进行排队，这种等待分组转发所耗费的时间也被称为排队时延，上面提到分组交换器在转发分组时会进行等待，这种等待被称为存储转发时延，所以我们现在了解到的有两种时延，但是其实是有四种时延。这些时延不是一成不变的，其变化程序取决于网络的拥塞程度。因为队列是有容量限制的，当多条链路同时发送分组导致输出缓存无法接受超额的分组后，这些分组会丢失，这种情况被称为丢包(packet loss)，到达的分组或者已排队的分组将会被丢弃。下图说明了一个简单的分组交换网络。下面来一个情景模拟：假定主机 A 和 主机 B 要向主机 E 发送分组，主机 A 和 B 首先通过 100 Mbps 以太网链路将其数据包发送到第一台路由器，然后路由器将这些数据包定向到 15 Mbps 的链路。如果在较短的时间间隔内，数据包到达路由器的速率（转换为每秒比特数）超过 15 Mbps，则在数据包在链路输出缓冲区中排队之前，路由器上会发生拥塞，然后再传输到链路上。例如，如果主机 A 和主机 B 背靠背同时发了 5 包数据，那么这些数据包中的大多数将花费一些时间在队列中等待。实际上，这种情况与许多普通情况完全相似，例如，当我们排队等候银行出纳员或在收费站前等候时。转发表和路由器选择协议我们刚刚讲过，路由器和多个通信线路进行相连，如果每条通信链路同时发送分组的话，可能会造成排队和丢包的情况，然后分组在队列中等待发送，现在我就有一个问题问你，队列中的分组发向哪里？这是由什么机制决定的？换个角度想问题，路由的作用是什么？其实是把不同端系统中的数据包进行存储和转发 。在互联网中，每个端系统都会有一个 IP 地址，当主机发送分组时，会在分组的首部加上主机的 IP 地址。每台路由器都会有一个转发表(forwarding table)，当一个分组到达路由器后，路由器会检查分组中目的地址的一部分，并用目的地址搜索转发表，以找出适当的传输链路，然后映射到输出链路实现转发操作。那么问题来了，路由器内部是怎样设置转发表的呢？详细的我们后面会讲到，这里只是说个大概，路由器内部也是具有路由选择协议的，用于自动设置转发表。电路交换在计算机网络中，另一种通过网络链路和路由进行数据传输的另外一种方式就是电路交换(circuit switching)。电路交换在资源预留上与分组交换不同，这是什么意思呢？就是分组交换不会预留每次端系统之间交互分组的缓存和链路传输速率，所以每次都会进行排队传输；而电路交换会预留这些信息。一个简单的例子帮助你理解：这就好比有两家餐馆，餐馆 A 需要预定而餐馆 B 不需要预定，对于可以预定的餐馆 A，我们必须先提前与其进行联系，但是当我们到达目的地时，我们能够立刻入座并选菜。而对于不需要预定的那家餐馆来说，你可能不需要提前联系，但是你必须承受到达目的地后需要排队的风险。下面显示了一个电路交换网络在这个网络中，4 条链路用于 4 台电路交换机。这些链路中的每一条都有 4 条电路，因此每条链路能支持 4 条并行的链接。每台主机都与一台交换机直接相连，当两台主机需要通信时，该网络在两台主机之间创建一条专用的端到端的链接(end-to-end connection)。分组交换和电路交换的对比电路交换的支持者经常说分组交换不适合实时服务，因为它的端到端时延时不可预测的。而分组交换的支持者却认为分组交换提供了比电路交换更好的带宽共享；它比电路交换更加简单、更有效，实现成本更低。但是现在的趋势更多的是朝着分组交换的方向发展。分组交换网的时延、丢包和吞吐量互联网可以看成是一种基础设施，该基础设施为运行在端系统上的分布式应用提供服务。我们希望在计算机网络中任意两个端系统之间传递数据都不会造成数据丢失，这是一个极高的目标，实践中难以达到。所以，在实践中必须要限制端系统之间的吞吐量用来控制数据丢失。如果在端系统之间引入时延，也不能保证不会丢失分组问题。所以我们从时延、丢包和吞吐量三个层面来看一下计算机网络。分组交换中的时延计算机网络中的分组从一台主机（源）出发，经过一系列路由器传输，在另一个端系统中结束它的历程。在这整个传输历程中，分组会涉及到四种最主要的时延：节点处理时延(nodal processing delay)、排队时延(queuing delay)、传输时延(transmission delay)和传播时延(propagation delay)。这四种时延加起来就是节点总时延(total nodal delay)。如果用 dproc dqueue dtrans dpop 分别表示处理时延、排队时延、传输时延和传播时延，则节点的总时延由以下公式决定: dnodal = dproc + dqueue + dtrans + dpop。时延的类型下面是一副典型的时延分布图，让我们从图中进行分析一下不同的时延类型。分组由端系统经过通信链路传输到路由器 A，路由器 A 检查分组头部以映射出适当的传输链路，并将分组送入该链路。仅当该链路没有其他分组正在传输并且没有其他分组排在该该分组前面时，才能在这条链路上自由的传输该分组。如果该链路当前繁忙或者已经有其他分组排在该分组前面时，新到达的分组将会加入排队。下面我们分开讨论一下这四种时延。节点处理时延节点处理时延分为两部分，第一部分是路由器会检查分组的首部信息；第二部分是决定将分组传输到哪条通信链路所需要的时间。一般高速网络的节点处理时延都在微秒级和更低的数量级。在这种处理时延完成后，分组会发往路由器的转发队列中。排队时延在队列排队转发过程中，分组需要在队列中等待发送，分组在等待发送过程中消耗的时间被称为排队时延。排队时延的长短取决于先于该分组到达正在队列中排队的分组数量。如果该队列是空的，并且当前没有正在传输的分组，那么该分组的排队时延就是 0。如果处于网络高发时段，那么链路中传输的分组比较多，那么分组的排队时延将延长。实际的排队时延也可以到达微秒级。传输时延队列是路由器所用的主要的数据结构。队列的特征就是先进先出，先进食堂的先打饭。传输时延是理论情况下单位时间内的传输比特所消耗的时间。比如分组的长度是 L 比特，R 表示从路由器 A 到路由器 B 的传输速率。那么传输时延就是 L / R 。这是将所有分组推向该链路所需要的时间。正是情况下传输时延通常也在毫秒到微秒级。传播时延从链路的起点到路由器 B 传播所需要的时间就是传播时延。该比特以该链路的传播速率传播。该传播速率取决于链路的物理介质(双绞线、同轴电缆、光纤)。如果用公式来计算一下的话，该传播时延等于两台路由器之间的距离 / 传播速率。即传播时延是 d/s ，其中 d 是路由器 A 和 路由器 B 之间的距离，s 是该链路的传播速率。传输时延和传播时延的比较计算机网络中的传输时延和传播时延有时候难以区分，在这里解释一下，传输时延是路由器推出分组所需要的时间，它是分组长度和链路传输速率的函数，而与两台路由器之间的距离无关。而传播时延是一个比特从一台路由器传播到另一台路由器所需要的时间，它是两台路由器之间距离的倒数，而与分组长度和链路传输速率无关。从公式也可以看出来，传输时延是 L/R，也就是分组的长度 / 路由器之间传输速率。传播时延的公式是 d/s，也就是路由器之间的距离 / 传播速率。排队时延在这四种时延中，人们最感兴趣的时延或许就是排队时延了 dqueue。与其他三种时延（dproc、dtrans、dpop）不同的是，排队时延对不同的分组可能是不同的。例如，如果 10 个分组同时到达某个队列，第一个到达队列的分组没有排队时延，而最后到达的分组却要经受最大的排队时延（需要等待其他九个时延被传输）。那么如何描述排队时延呢？或许可以从三个方面来考虑：流量到达队列的速率、链路的传输速率和到达流量的性质。即流量是周期性到达还是突发性到达，如果用 a 表示分组到达队列的平均速率（ a 的单位是分组/秒，即 pkt/s）前面说过 R 表示的是传输速率，所以能够从队列中推出比特的速率（以 bps 即 b/s 位单位）。假设所有的分组都是由 L 比特组成的，那么比特到达队列的平均速率是 La bps。那么比率 La/R 被称为流量强度(traffic intensity)，如果 La/R &gt; 1，则比特到达队列的平均速率超过从队列传输出去的速率，这种情况下队列趋向于无限增加。所以，设计系统时流量强度不能大于1丢包我们在上述的讨论过程中描绘了一个公式那就是 La/R 不能大于1，如果 La/R 大于1，那么到达的排队将会无穷大，而且路由器中的排队队列所容纳的分组是有限的，所以等到路由器队列堆满后，新到达的分组就无法被容纳，导致路由器丢弃该分组，即分组会丢失。计算机网络中的吞吐量除了丢包和时延外，衡量计算机另一个至关重要的性能测度是端到端的吞吐量。假如从主机 A 向主机 B 传送一个大文件，那么在任何时刻主机 B 接收到该文件的速率就是瞬时吞吐量(instantaneous throughput)。如果该文件由 F 比特组成，主机 B 接收到所有 F 比特用去 T 秒，则文件的传送平均吞吐量(average throughput)是 F / T bps。单播、广播、多播和任播在网络通信中，可以根据目标地址的数量对通信进行分类，可以分为 单播、广播、多播和任播。单播(Unicast)单播最大的特点就是 1 对 1，早期的固定电话就是单播的一个例子，单播示意图如下。广播(Broadcast)我们一般小时候经常会跳广播体操，这就是广播的一个事例，主机和与他连接的所有端系统相连，主机将信号发送给所有的端系统。多播(Multicast)多播与广播很类似，也是将消息发送给多个接收主机，不同之处在于多播需要限定在某一组主机作为接收端。任播(Anycast)任播是在特定的多台主机中选出一个接收端的通信方式。虽然和多播很相似，但是行为与多播不同，任播是从许多目标机群中选出一台最符合网络条件的主机作为目标主机发送消息。然后被选中的特定主机将返回一个单播信号，然后再与目标主机进行通信。物理媒介网络的传输是需要介质的。一个比特数据包从一个端系统开始传输，经过一系列的链路和路由器，从而到达另外一个端系统。这个比特会被转发了很多次，那么这个比特经过传输的过程所跨越的媒介就被称为物理媒介(phhysical medium)，物理媒介有很多种，比如双绞铜线、同轴电缆、多模光纤榄、陆地无线电频谱和卫星无线电频谱。其实大致分为两种：引导性媒介和非引导性媒介。双绞铜线最便宜且最常用的引导性传输媒介就是双绞铜线，多年以来，它一直应用于电话网。从电话机到本地电话交换机的连线超过 99% 都是使用的双绞铜线，例如下面就是双绞铜线的实物图。双绞铜线由两根绝缘的铜线组成，每根大约 1cm 粗，以规则的螺旋形状排列，通常许多双绞线捆扎在一起形成电缆，并在双绞馅的外面套上保护层。一对电缆构成了一个通信链路。无屏蔽双绞线一般常用在局域网（LAN）中。同轴电缆与双绞线类似，同轴电缆也是由两个铜导体组成，下面是实物图。借助于这种结构以及特殊的绝缘体和保护层，同轴电缆能够达到较高的传输速率，同轴电缆普遍应用在在电缆电视系统中。同轴电缆常被用户引导型共享媒介。光纤光纤是一种细而柔软的、能够引导光脉冲的媒介，每个脉冲表示一个比特。一根光纤能够支持极高的比特率，高达数十甚至数百 Gbps。它们不受电磁干扰。光纤是一种引导型物理媒介，下面是光纤的实物图。一般长途电话网络全面使用光纤，光纤也广泛应用于因特网的主干。陆地无线电信道无线电信道承载电磁频谱中的信号。它不需要安装物理线路，并具有穿透墙壁、提供与移动用户的连接以及长距离承载信号的能力。卫星无线电信道一颗卫星电信道连接地球上的两个或多个微博发射器/接收器，它们称为地面站。通信中经常使用两类卫星：同步卫星和近地卫星。" }, { "title": "计算机网络发展史(二)", "url": "/posts/NetworkTwo/", "categories": "计算机网络", "tags": "学习", "date": "2023-08-28 04:17:00 +0000", "snippet": "计算机网络发展历程批处理和早期的计算机操作系统一样，最开始都要先经历批处理 Batch Processing 阶段，批处理的目的也是为了能让更多的人使用计算机。批处理就是先将数据装入卡带或者磁带，并且由计算机按照一定的顺序进行读入，如下图所示。这种计算机的价格比较昂贵，并不是每个人都能够使用的，这也就客观暗示着，只有专门的操作员才能使用计算机，用户把程序提交给操作员，由操作员排队执行程序，等...", "content": "计算机网络发展历程批处理和早期的计算机操作系统一样，最开始都要先经历批处理 Batch Processing 阶段，批处理的目的也是为了能让更多的人使用计算机。批处理就是先将数据装入卡带或者磁带，并且由计算机按照一定的顺序进行读入，如下图所示。这种计算机的价格比较昂贵，并不是每个人都能够使用的，这也就客观暗示着，只有专门的操作员才能使用计算机，用户把程序提交给操作员，由操作员排队执行程序，等一段时间后，用户再来提取结果（API 程序员的早期原型。。。。。。）这种计算机的高效性并没有很好的体现，因为涉及到各种操作不断切换，让计算机计算甚至不如手动运算快。分时系统在批处理之后出现的就是分时系统了，分时系统指的是多个终端与同一个计算机连接，允许多个用户同时使用一台计算机。分时系统的出现实现了一人一机的目的，让用户感觉像是自己在使用计算机，实际上这是一种独占性的特性，如下图所示分时系统出现以来，计算机的可用性得到了极大的改善。分时系统的出现意味着计算机越来越贴近我们的生活。还有一点需要注意：分时系统的出现促进了像是 BASIC 这种人机交互编程语言的诞生。分时系统的出现，同时促进者计算机网络的出现。计算机通信在分时系统中，每个终端与计算机相连，这种独占性的方式并不是计算机之间的通信，因为每个人还是在独立的使用计算机。到了 20 世纪 70 年代，计算机性能有了高速发展，同时体积也变得越来越小，使用计算机的门槛变得更低，越来越多的用户可以使用计算机。没有一台计算机是信息孤岛这个理念促使着计算机网络的出现和发展。计算机网络的诞生20 世纪 80 年代，一种能够互连多种计算机的网络随之诞生。它能够让各式各样的计算机相连，从大型的超级计算机或主机到小型电脑。20 世纪 90 年代，虽然实现了一人一机的环境，但是这种环境的搭建仍然价格不菲。与此同时，诸如电子邮件、万维网等信息传播方式如雨后春笋般迎来了前所未有的发展，使得互联网从大到整个公司小到每个家庭内部，都得以广泛普及。计算机网络的高速发展现如今，越来越多的终端设备接入互联网，使互联网达到了前所未有的盛世，近年来 3G、4G、5G 通信技术的发展更是互联网高速发展的产物。许多发展道路各不相同的网络技术也都逐渐向互联网靠拢。例如曾经一直作为通信基础设施、支撑通信网络的电话网。随着互联网的发展，其地位也随着时间的推移被 IP 网所取代，IP 也是互联网发展的产物。网络安全互联网也具有两面性，它的出现虽然方便了用户，但同时也方便了一些不法分子。互联网的便捷也带来了一些负面影响，计算机病毒的侵害、信息泄漏、网络诈骗层出不穷。在现实生活中，通常情况下我们挨揍了会予以反击，因为这种行为完全是动物本能驱动的。但是在互联网中，你被不法分子攻击通常情况下是无力还击的，只能防御，因为还击需要你精通计算机和互联网，这通常情况下很多人办不到。通常情况下公司和企业容易被作为不法分子获利的对象，所以，作为公司或者企业，要想不受攻击或者防御攻击，需要建立安全的互联网连接。互联网协议协议指的是大家为了达成某一目的所共同遵守的约定。协议这个名词不仅局限于互联网范畴，也体现在日常生活中，比如情侣双方约定好在哪个地点吃饭，这个约定也是一种协议，比如你应聘成功了，企业会和你签订劳动合同，这种双方的雇佣关系也是一种协议。注意自己对自己的约定不能成为协议，协议的前提条件必须是多人约定。那么网络协议是什么呢？网络协议就是网络中（包括互联网）传递、管理信息的一些规范。如同人与人之间相互交流是需要遵循一定的规矩一样，计算机之间的相互通信需要共同遵守一定的规则，这些规则就称为网络协议。没有网络协议的互联网是混乱的，就和人类社会一样，一个个体不能想怎么样就怎么样，你的行为是受到法律约束的。那么网络中的每台计算机也不能自己想发什么发什么，也是需要受到通信协议约束的。我们一般都了解过 HTTP 协议，我说一句话总结就是HTTP 是一个在计算机网络中在两点之间传输文本、图片、音频、视频等超文本数据的约定和规范。除了 HTTP 协议之外，网络协议还有很多，举几个比较有名的就是 IP、TCP、UDP、DNS 协议等。下面是一些协议的汇总和介绍。ISO 在制定标准化 OSI 模型之前，对网络体系结构相关的问题进行了充分的讨论，最终提出了作为通信协议设计指标的 OSI 参考模型。这一模型将通信协议中必要的功能分为了七层。通过这七层分层，使那些比较复杂的协议简单化。在 OSI 标准模型中，每一层协议都接收由它下一层所提供的特定服务，并且负责为上一层提供服务，上层协议和下层协议之间通常会开放接口，同一层之间的交互所遵守的约定叫做协议。OSI 标准模型上图只是简单的介绍了一下层与层之间的通信规范和上层与下层的通信规范，并未介绍具体的网络协议分层，实际上，OSI 标准模型将复杂的协议整理并分为了易于理解的七层。如下图所示互联网的通信协议都对应了七层中的某一层，通过这一点，可以了解协议在整个网络模型中的作用，一般来说，各个分层的主要作用如下： 应用层：应用层是 OSI 标准模型的最顶层，是直接为应用进程提供服务的。其作用是在实现多个系统应用进程相互通信的同时，完成一系列业务处理所需的服务。包括文件传输、电子邮件远程登录和远端接口调用等协议。 表示层: 表示层向上对应用进程服务，向下接收会话层提供的服务，表示层位于 OSI 标准模型的第六层，表示层的主要作用就是将设备的固有数据格式转换为网络标准传输格式。 会话层：会话层位于 OSI 标准模型的第五层，它是建立在传输层之上，利用传输层提供的服务建立和维持会话。 传输层：传输层位于 OSI 标准模型的第四层，它在整个 OSI 标准模型中起到了至关重要的作用。传输层涉及到两个节点之间的数据传输，向上层提供可靠的数据传输服务。传输层的服务一般要经历传输连接建立阶段，数据传输阶段，传输连接释放阶段 3 个阶段才算完成一个完整的服务过程。 网络层：网络层位于 OSI 标准模型的第三层，它位于传输层和数据链路层的中间，将数据设法从源端经过若干个中间节点传送到另一端，从而向运输层提供最基本的端到端的数据传送服务。 数据链路层：数据链路层位于物理层和网络层中间，数据链路层定义了在单个链路上如何传输数据。 物理层：物理层是 OSI 标准模型中最低的一层，物理层是整个 OSI 协议的基础，就如同房屋的地基一样，物理层为设备之间的数据通信提供传输媒体及互连设备，为数据传输提供可靠的环境。除了 OSI 标准的七层协议模型外，还出现了计算机网络的五层模型。由于会话层和表示层可以看做是应用程序中出现的两种特征，因此计算机网络的五层模型就是把表示层、会话层合并成为了应用层。除了计算机网络的五层模型，我们平常探讨更多的是 TCP/IP 的四层模型，因为我们程序员并不太关注通信的底层，这是通信工程师的特长。TCP/IP 四层模型TCP/IP 四层模型是我们程序员接触最多的协议，实际上这四层模型中所包含的协议被称为TCP/IP 协议簇，它并不特指单纯的 TCP 和 IP 协议，而是容纳了许许多多的网络协议。OSI 模型共有七层，从下到上分别是物理层、数据链路层、网络层、运输层、会话层、表示层和应用层。但是这显然是有些复杂的，所以在TCP/IP协议中，它们被简化为了四个层次和 OSI 七层网络协议的主要区别如下 应用层、表示层、会话层三个层次提供的服务相差不是很大，所以在 TCP/IP 协议中，它们被合并为应用层一个层次。 由于通信（数据）链路层和物理层的内容很相似，所以在 TCP/IP 协议中它们被归并在网络接口层一个层次里。下面我就和你聊一聊 TCP/IP 协议簇中都有哪些具体的协议：IP 协议IP 是网际互联协议，英文 Internet Protocol，位于网络层。IP 协议是整个 TCP/IP 协议簇的核心，也是构成互联网的基础。IP 能够为运输层提供数据分发，同时也能够组装数据供运输层使用。它将多个网络连接成为一个互联网，这样能够提高网络的可扩展性，实现大规模网络互联。二是分割顶层网络和底层网络之间的耦合关系。ICMP 协议ICMP 协议是 Internet 报文控制协议，英文Internet Control Message Protocol， ICMP 协议主要用于在 IP 主机、路由器之间传递控制消息。ICMP 属于网络层的协议，当遇到 IP 无法访问目标、IP 路由器无法按照当前传输速率转发数据包时，会自动发送 ICMP 消息，从这个角度来说，ICMP 协议可以看作是错误侦测与回报机制，让我们检查网络状况、也能够确保连线的准确性。ARP 协议ARP 协议是地址解析协议，英文 Address Resolution Protocol，它能够根据 IP 地址获取物理地址。主机发送信息时会将包含目标 IP 的 ARP 请求广播到局域网络上的所有主机，并接受返回消息，以此来确定物理地址。收到消息后的物理地址和 IP 地址会在 ARP 中缓存一段时间，下次查询的时候直接从 ARP 中查询即可。TCP 协议TCP 是传输控制协议，英文Transmission Control Protocol，它是一种面向连接的、可靠的、基于字节流的传输协议，TCP 协议位于传输层，TCP 协议是 TCP/IP 协议簇中的核心协议，它最大的特点就是提供可靠的数据交付。TCP 的主要特点有 慢启动、拥塞控制、快速重传、可恢复。UDP 协议UDP 协议是用户数据报协议，英文 User Datagram Protocol，UDP 也是一种传输层协议，与 TCP 相比，UDP 提供一种不可靠的数据交付，也就是说，UDP 协议不保证数据是否到达目标节点。当报文发送之后，是无法得知其是否安全完整到达的。UDP 是一种无连接的协议，传输数据之前源端和终端无需建立连接，不对数据报进行检查与修改，无须等待对方的应答，会出现分组丢失、重复、乱序等现象。但是 UDP 具有较好的实时性，工作效率较 TCP 协议高。FTP 协议FTP 协议是文件传输协议，英文File Transfer Protocol，应用层协议之一，是 TCP/IP 协议的重要组成之一，FTP 协议分为 FTP 服务器和 FTP 客户端两部分，FTP 服务器用来存储文件，FTP 客户端用来访问 FTP 服务器上的文件，FTP 的传输效率比较高，所以一般使用 FTP 来传输大文件。DNS 协议DNS 协议是域名解析协议，英文 Domain Name System，它也是应用层的协议之一，DNS 协议是一个将域名和 IP 相互映射的分布式数据库系统。DNS 缓存能够加快网络资源的访问。SMTP 协议SMTP 协议是邮件传输协议，英文 Simple Mail Transfer Protocol，应用层协议之一，SMTP 主要是用作邮件收发协议，SMTP 服务器是遵循 SMTP 协议的发送邮件服务器，用来发送或中转用户发出的电子邮件SLIP 协议SLIP 协议是指串行线路网际协议，英文 Serial Line Internet Protocol，是在串行通信线路上支持 TCP/IP 协议的一种点对点式的链路层通信协议。PPP 协议PPP 协议是点对点协议，英文Point to Point Protocol，是一种链路层协议，是在为同等单元之间传输数据包而设计的。设计目的主要是用来通过拨号或专线方式建立点对点连接发送数据，使其成为各种主机、网桥和路由器之间简单连接的一种共通的解决方案。" }, { "title": "计算机网络发展史(一)", "url": "/posts/NetworkOne/", "categories": "计算机网络", "tags": "学习", "date": "2023-08-21 04:08:00 +0000", "snippet": "如果说计算机把我们从工业时代带到了信息时代，那么计算机网络就可以说把我们带到了互联网时代。随着使用计算机人数的不断增加，计算机也经历了一系列的快速发展，从大型通用计算机 -&gt; 超级计算机 -&gt; 小型机 -&gt; 个人电脑 -&gt; 工作站 -&gt; 便携式电脑 -&gt; 智能手机终端等都是这一过程的产物。计算机网络逐渐的从独立模式演变为了网络互联模式。独立模式：计算机在联...", "content": "如果说计算机把我们从工业时代带到了信息时代，那么计算机网络就可以说把我们带到了互联网时代。随着使用计算机人数的不断增加，计算机也经历了一系列的快速发展，从大型通用计算机 -&gt; 超级计算机 -&gt; 小型机 -&gt; 个人电脑 -&gt; 工作站 -&gt; 便携式电脑 -&gt; 智能手机终端等都是这一过程的产物。计算机网络逐渐的从独立模式演变为了网络互联模式。独立模式：计算机在联网前都可以认为是独立模式，俗称单机模式，此时的计算机不会与其他计算机通信。网络互联模式：通过互联网可以将两个独立的终端连在一起，相互收发消息。如上图可以看到，在独立模式下，每个人都需要排队等待其他人在一个机器上完成工作后，其他用户才能使用。上图显示的是计算机从单机模式切换到了网络互联模式，在这种模式下，每个用户都能独立的使用计算机，计算机通过网络连接到服务器，由服务器来为客户端 A、B、C 提供服务。可以看到，这种网络互联模式就是计算机网络的雏形，客户端需要通过网络请求服务器，客户端通过网络把数据传输给客户端，来为用户服务。实际上，计算机网络就是由若干节点和链接这些节点的链路组成的网络，网络中的节点可以是计算机、交换器、集线器或者路由器等能够进行网络通信的终端设备，链路是一个节点到另外一个节点所走过的物理路线。计算机网络的组成可以分为两部分： 第一部分是计算机网络的核心部分，这一部分由大量的网络节点和连接这些网络节点的路由器组成，形成一个网络拓扑网，可以把核心部分理解为计算机网络的服务方。 第二部分是计算机网络的边缘部分，这一部分指的就是网络上的主机，也就是用户，这就相当于是计算机网络的“终点”，可以把边缘部分理解为计算机网络的请求方。计算机网络中的节点和各节点的链路可以形成各种不同范围的网络，通过网络覆盖范围的大小，可以分为局域网、城域网和广域网。局域网：一般指的是范围在几百到几公里办公楼或者校园内计算机相互连接成的计算机网络，一般用在狭小区域内的网络，一个社区、一栋楼、办公室经常使用局域网。城域网：一个城市中的计算机所相互连接形成的计算机网络。城域网是介于广域网与局域网之间的一种高速网络。城域网设计的目标是要满足几千米范围内的大量企业、机关、公司的多个局城网互联的需求，以实现大量用户之间的数据、语音、图形与视频等多种信息的传输功能。广域网：广域网中的计算机所形成的连接范围就更大了，常常是一个国家或是一个洲。其目的是为了让分市较远的各局域网互联。在介绍完计算机网络的基本构成之后，我们先来了解一下计算机网络是如何出现的呢？计算机网络发展史互联网的诞生1946 年世界上第一台计算机问世，此时还没有计算机网络这种概念，每台计算机只能单机工作，即使两台计算机的距离非常近，他们却只能像个内向的孩子一样，守着自己的那颗糖。二战之后，美苏争霸，出于军事的目的，美国组建了一个神秘的部门 ARPA，这个部门接美国国防部的要求打算研制一种分散的指挥系统，这个系统会有很多节点，每当其中某些节点被摧毁后，其它节点仍能相互通信，这个项目于 1966 年完成，ARPA 将其命名为 ARPANET（阿帕网）。ARPANET 是最早的计算机网络之一，它就是互联网的前身。ARPANET 是最早使用分组交换的计算机网络之一，通过包交换系统进行通信的数据会被格式化为带有目标机器地址的数据包，然后发送到网络上由下一台机器接收。数据包一词是由 Donald Davies 在 1965 年创造的，用于描述通过网络在计算机之间传输的数据，数据包在计算机网络中的位置举足轻重，可以说数据包是互联网的主人公。ARPANET 于1969 年正式启动。同样于 1969 年，加州大学洛杉矶分校（UCLA）的 Steve Crocker 发表了第一篇 RFC 论文，这被认为是互联网的开端。同年，第一台网络交换机实现了在 ARPANET 上的第一次数据传输，这标志着互联网的正式诞生。我们经常在某篇文章结尾引用.rfc 或者与某人沟通，他提出了他不求甚解的 idea，然后你继续追问他，他就让你自己查 RFC 文档（好像说的就是我），说的就是介个。官方一点解释 RFC 就是描述互联网和互联网系统的方法、行为、研究或者创新的官方文档。通俗一点就是：互联网协议的官方文档。TCP/IP 的诞生虽然现在能够在几个节点之间相互通信，但是当时的节点数量只有四个，很少，无法形成较大范围的网络。而且当时阿帕网有很多局限性，比如不同计算机网络之间不能互相通信，为了解决这个问题，APPA 又启动了新的研究项目，设法将不同的计算机局域网进行互联。早期的 ARPANET 采用的是一种名为NCP的网络协议，但是随着网络的发展，以及多节点接入和用户对网络需求的提高，NCP 协议已经不能充分支持 ARPANET 的发展需求。而且 NCP 还有一个非常重要的缺陷，就是它只能用于相同的操作系统环境中，不同操作系统环境中的网络无法通信。所以，ARPANET 急需一种新的协议来替换已经无法满足需求的 NCP 协议，这个任务的重担交给了 Robert E. Kahn 和 Vinton G. Cerf ，这两位大神的理论放到现在，都是空前绝后的，那么这俩老教授到底干了什么事儿呢？我只轻描淡写的讲一句：他们提出了新的通信协议，即传输控制协议——TCP（Transmission Control Protocol）。这是计算机网络两个非常著名的科学家，很多人把 Robert E. Kahn 和 Vinton G. Cerf 称为互联网之父。1974 年，这俩人在 IEEE 期刊上发表了题为《关于分组交换的网络通信协议》的论文，正式提出 TCP/IP，用以实现计算机网络之间的互联。虽然我们认为 TCP/IP 协议是一项非常伟大的发明，但在当时的背景下，却不被人们看好，而且 TCP/IP 的四层模型相比于 ISO 提出的七层模型来说，也显得比较简陋。但是功夫不负有心人，经过 4 年时间的不断改进，TCP/IP 协议终于完成了基础架构的搭建。在 1983 年，美国国防部高级研究计划局决定淘汰 NCP 协议，取而代之的是使用 TCP/IP 协议。从论文到发表，再到采纳，整整用了十年的时间。1985 年，TCP/IP 成为 UNIX 操作系统的组成部分。之后几乎所有的操作系统都逐渐支持 TCP/IP，这个协议成为主流，也是当时的未来。进一步发展20 世纪 80 年代初期，ARPANET 取得了巨大的成功，但是没有获得美国联邦机构合同的学校却不能使用。为了解决这个问题，美国国家科学基金会（NSF）开始着手建立给大学生使用的计算机科学网（CSNet）。CSNet 是在其他基础网络之上加的协议层，它使用其他网络提供的通信能力，在用户角度下它也是一个独立的网络。CSNet 采用集中控制方式，所有信息交换都经过一台中继器进行。1986 年 NSF 投资分别在五所大学建立了超级计算机中心，并形成了 NSFNET，由于 NSF 的鼓励和资助，很多大学、政府机构甚至私营的研究机构纷纷把自己的局域网并人 NSFNET 中，从 1986 年至 1991 年，NSFNET 的子网从100 个迅速增加到 3000 多个。不仅仅只有学校的加入，许多学术团体、企业、研究机构甚至个人也加入进来，于是 Internet 的使用者不再限于纯计算机专业人员。新的使用者发现计算机相互间的通讯对他们来讲更有吸引力。于是，他们逐步把 Internet 当作一种交流与通信的工具，而不仅仅只是共享 NSF 巨型计算机的运算能力。Internet 是一系列全球信息的汇总，它由无数个子网组成，每个子网中都有若干台计算机。进入 90 年代初期，Internet 已经有了非常多的子网，各个子网分别负责自己的架设和运作费用，而这些子网又通过 NSFNET 互联起来。NSFNET 连接全美上千万台计算机，拥有几千万用户，是 Internet 最主要的成员网。随着计算机网络在全球的拓展和扩散，美国以外的网络也逐渐接入 NSFNET 主干或其子网。1993 年是因特网发展过程中非常重要的一年，在这一年中 Internet 完成了到目前为止所有最重要的技术创新，WWW - 万维网和浏览器的应用使因特网上有了一个令人耳目一新的平台：人们在 Internet 上所看到的内容不仅只是文字，而且有了图片、声音和动画、甚至还有了电影。因特网演变成了一个文字、图像、声音、动画、影片等多种媒体交相辉映的新世界，更以前所未有的速度席卷了全世界。Internet 的迅速崛起、引起了全世界的瞩目，我国也非常重视信息基础设施的建设，注重与 Internet 的连接。目前，已经建成和正在建设的信息网络，对我国科技、经济、社会的发展以及与国际社会的信息交流产生着深远的影响。我国互联网发展虽然我国互联网的起步没有美国那么早，但是我国却有着全世界最快的互联网增速。我国互联网发展起源于 1987 - 1993 年，这段时期国内的科技工作者开始接触 Internet 资源。在此期间，以中科院高能物理所为首的一批科研院所与国外机构合作开展一些与 Internet 联网的科研课题，通过拨号方式使用Internet 的 E-mail 电子邮件系统，并为国内一些重点院校和科研机构提供国际 Internet 电子邮件服务。1990 年 10 月，我国正式向国际因特网信息中心登记注册了最高域名 cn，从而开通了使用自己域名的 Internet 电子邮件。1994 年 1 月，美国国家科学基金会接受我国正式接入 Internet 的要求。1994 年 3 月，我国获准加入 Internet。4月初在中美科技合作联委会上，代表我国政府向美国国家科学基金会（NSF）正式提出要求连入 Internet，并得到认可。至此，我国终于打通了最后的环节，在 4 月 20 日，以 NCFC 工程连入 Internet 国际专线为标志，我国与 Internet 全面接触。同年 5 月，我国联网工作全部完成。我国政府对 Internet 进入我国表示认可。我国网络的域名也最终确定为 cn。此事被我国新闻界评为 1994 年我国十大科技新闻之一，被国家统计公报列为我国 1994 年重大科技成就之一。从 1994 年开始至今，我国实现了和因特网的 TCP／IP 连接，从而逐步开通了因特网的全功能服务；大型电脑网络项目正式启动，因特网在我国进入了飞速发展时期。1995 年，我国电信分别在北京和上海设立专线，并通过电话线、DDN 专线以及 X.25 网面向社会提供 Internet 接入服务。1995 年 5 月，开始筹建 CHINANET 全国骨干网，1996 年 1 月，CHINANET 骨干网建成并正式开通，全国范围的公用计算机互联网络开始提供服务。标志着我国互联网进入快速发展阶段。我国陆续建造了多个全国范围内的公共计算机网络，其中最大的就是下面这几个 中国电信互联网 CHINANET 中国联通互联网 UNINET 中国移动互联网 CMNET 中国教育和科研计算机网 CERNET 中国科学技术网 CSTNET可以发现，我国互联网建设主要分为三个阶段。 第一阶段为1987—1993年，这个阶段称为启蒙阶段，或者说试验阶段，我国在这个阶段开始接触 Internet，并开展了科研课题和科技合作工作，不过阶段的网络应用仅限于小范围内的电子邮件服务。 第二阶段为 1994 年 - 1996 年，这个阶段为启动阶段，或者说铺设阶段，这个阶段我国开始架设、铺设骨干网，并接入 Internet，从此我国被国际上正式承认为有 Internet 的国家。然后 ChinaNet、CERnet、CSTnet等多个 Internet 络项目在全国范围相继启动。 第三个阶段为 1997 年至今，这个阶段面向全国范围内接入 Internet，这个阶段是我国互联网快速发展的阶段。进入 21 世纪后，CERNET2 试验网开通，CERNET2 试验网是以 2.5 Gbit/s - 10 Gbit/s 的速度连接北京、上海和广州三个 CERNET 核心节点，这标志着我国互联网已经迈入了国际先进水平。CNNIC（中国互联网络信息中心）每年都会公布我国互联网发展情况，感兴趣的小伙伴可以通过 www.cnnic.cn 查询到相关信息。" }, { "title": "从根儿上理解微服务03", "url": "/posts/MicroservicesThree/", "categories": "技术科普", "tags": "学习", "date": "2023-08-14 13:54:00 +0000", "snippet": "前言上一篇文章中我详细讲解了为什么需要微服务以及如何划分微服务。今天，咱们正式进入到微服务专栏正题 —— 微服务技术架构。大家在平常开发的时候，一定或多或少接触过负载均衡、服务发现与注册、服务熔断、服务降级等概念。但是，你有认真地梳理过这些内容吗？如果让你来开发一个微服务，你应该如何开发相关的微服务组件或引入开源解决方案？今天，我先带大家大概梳理清楚微服务技术架构脉络，使大家对于微服务有一个...", "content": "前言上一篇文章中我详细讲解了为什么需要微服务以及如何划分微服务。今天，咱们正式进入到微服务专栏正题 —— 微服务技术架构。大家在平常开发的时候，一定或多或少接触过负载均衡、服务发现与注册、服务熔断、服务降级等概念。但是，你有认真地梳理过这些内容吗？如果让你来开发一个微服务，你应该如何开发相关的微服务组件或引入开源解决方案？今天，我先带大家大概梳理清楚微服务技术架构脉络，使大家对于微服务有一个总体上的认识。如下为微服务架构思维导图服务描述在单体项目的开发过程中，很多时候我们采用主流的前后端分离开发模式。 前端请求后端接口 后端将响应 JSON 返回给前端但是，在微服务项目中，虽然前端仍然只是简单地请求后端，但是后端在处理请求的过程中，可能需要经过不同的微服务。微服务之间是需要进行通信的。那么在通信之前，我们需要解决一个问题：如何定义当前微服务？更通俗地来说，你的微服务叫什么、服务提供了什么接口、服务接口是如何定义的、服务返回的结果格式是什么、如何解析等。这一系列问题的解决方案就是「服务描述」。具体的服务描述包括三种方式： RESTful API XML 文件配置 IDL 文件配置这三种方式中，RESTful API 可能是我们日常开发中接触最多的一种形式。即使在单体应用中，我们也经常采用这一方式来规范团队接口定义格式。IDL 文件也十分常见，它是一种接口描述语言，主要用于 Thrift 与 gRPC 这类跨语言服务框架中，即使是不同语言、不同平台，也可以互相通信。服务发现与注册🌟 划重点，重要内容！当我们实现了多个微服务并定义了相关服务描述之后，应该如何实现互相调用、互相通信呢？这就是服务发现与注册需要解决的问题。当某一个服务想要调用另外一个服务时，需要知道另一个服务的地址、端口等信息，但是总不可能每一个服务都将所有服务的地址信息记录在本地，这样单服务会存在一定的负载。数据库存储较难维护，也不是一个很好的选择，因为服务的地址信息随时可能发生变化（由于扩缩容、服务下线等原因），这个变化需要被各个服务所感知。因此，我们需要将这部分信息抽取出来，集中交由某个代理进行统一管理，这就是我们所要提到的「注册中心」。 当服务上线时，需要向注册中心提交自己当前的地址信息，并向注册中心订阅自己所需的服务信息。 当服务下线时，需要通知注册中心，及时删除相关服务信息，并通知其它订阅该服务的服务。 当服务信息发生变化时，注册中心需要及时更改信息。 所有服务需要定时与注册中心进行通信（心跳信息），以确认是否存活。在日常开发中，较为常见的注册中心有如下几种： Nacos Zookeeper Eureka Consul服务通信在单体项目中，不同模块之间可以通过函数调用的方式进行交互。但是，在微服务的世界中，每个微服务最终是以分布式的方式部署于多台服务器上，是一个分布式系统。各个服务实例都是不同的进程。因此，不同服务之间必须使用 IPC 进程间通信机制 进行交互。如下为两类 IPC 技术： 异步、基于消息的通信 异步通信，客户端不会阻塞等待响应返回 采用消息系统标准协议，如：AMQP / STOMP 开源消息系统：如：RabbitMQ / Kafka / RocketMQ 等 同步的请求 &amp; 响应 同步通信，客户端等待响应时为阻塞状态 两类方式： REST，基于 HTTP 协议 Thrift，跨语言、支持多种消息格式对于服务通信，还需要了解通信的消息格式：文本与二进制 对于文本格式，有 JSON 与 XML 格式 对于二进制，可以采用开源序列化协议，如：Protocol Buffer、Kyro、Hessian等服务路由🌟 划重点，重要内容！负载均衡 在实际的微服务调用中，可能一个微服务对应几十个实例节点，那么对于服务调用者而言，应该如何从中选取节点进行调用呢？这就涉及到了服务节点的负载均衡。 负载均衡，顾名思义，就是为了使得同一服务的不同节点，较为均衡地接收并处理请求。 对于机器性能高、响应速度快、处理请求少的节点，尽可能多地处理请求，发挥最大作用 常见的负载均衡算法有如下几种： 随机算法 轮询算法 加权轮询算法 最少活跃连接算法 一致性 Hash 算法 分组调用通过对 RPC 请求打标签，实现生产环境服务与测试环境服务隔离。例如：当A、B两个工程师在共同开发一个叫做 UserService 服务时候，A 的 UserService 开发还未完成，处于自测阶段，B 的 UserService 已经开发完成，进入了测试阶段。这时，可以对服务进行分组管理，开发的 UserService 的 group 设置为 dev，B 开发的 UserService 的 group 设置为 test。全链路灰度发布当服务全量发布上线之前，我们需要先在一小部分节点上发布服务，进行局部测试。 若测试正常，没有出现问题的话，就继续扩大发布范围，逐渐实现全量更新。 若出现异常，则暂停发布，进行问题排查，这样所造成的负面影响范围较小，不会造成整体的经济损失。这种部署方式也称为「金丝雀部署」。服务稳定性治理🌟 划重点，重要内容！服务限流、降级与熔断在我们日常开发中，一定会经常接触到这三个词：限流、降级、熔断。这其实是高并发场景下最常遇到的服务治理问题。通过这三种方案，确保服务在大多数情况下的「高可用」。 服务限流：在高并发系统中，可能会遇到流量高峰，这时需要通过服务限流，以此限制 QPS，从而起到保护服务、削减流量的作用。 常见限流方式：单实例限流、分布式限流 服务降级：对于微服务项目，为了在高并发场景下保护核心服务，通常会采用 “弃车保帅” 的降级思路，通过丢弃非核心服务请求，削减流量。 服务熔断：当服务出现重大事故、Bug 时，为了防止故障扩散、全链路崩溃、整体服务宕机的情况，需要对服务上游源头进行熔断操作，即直接抛出异常信息，以此在最大程度下保护下游服务。动态超时一次微服务请求，可能涉及到几十个服务调用，调用链路十分长。为了防止长时间处理请求，造成请求资源占用的情况，需要引入超时机制。可以有效避免由于长时间的服务调用，造成服务过载，进而引起链路崩溃。请求重试当请求失败时，有可能是由于网络阻塞、服务卡顿、TCP 连接异常等原因。这时可以进行适当的请求重试，从而减少请求失败次数，提升服务整体的可用性。但是，也需要注意设置合理的重试次数，避免重试次数过大、请求累积，进而压垮整个服务。对于非幂等请求，需要额外小心，防止由于多次重试，造成服务端数据异常问题。服务可观测对于微服务系统而言，存在着复杂的链路调用，使得排查线上问题的难度增大。因此，需要建立更加可靠的「可观测系统」，也就是我们日常开发中经常接触的三个概念：日志系统、性能指标、链路追踪。微服务可观测性并不仅仅是数据的收集、展现、查询，它更加强调数据关联性，帮助我们快速发现、定位问题所在。服务安全保障在微服务系统中，我们需要确保服务安全，防止用户敏感信息泄漏。 身份认证：在请求入口（如 API 网关），需要对请求者身份进行校验，较为常规的做法是 Token 校验。 访问控制：对于不同的用户，具有的权限不同，可操控的资源不同，需要进行细粒度、精确到按钮的访问权限控制。具体方案如 RBAC 权限控制。 黑白名单机制： 对于异常流量，需要进行及时的检查与封禁。通过拦截 IP 请求，判断该 IP 是否位于黑名单中，及时拒绝访问 对于开发者、管理员等角色，直接放行，方便开发、测试或管理。服务运维微服务架构的升级，往往伴随着容器化技术与 Devops 自动化运维，它们经常搭配使用。因为当传统的单体应用升级为微服务之后，虽然降低了耦合性，但同时提高了复杂程度，其中就包括运维部署的复杂度。服务的拆分会造成多个服务的打包、测试、部署发布上线，运维的负担大大加重。这时便需要 Devops 解决这一问题。 Devops 其实是开发 Development 与运维 Operation 的组合词，这意味着开发与运维的关系变得更加紧密。有了 Devops，开发人员也可解决一键解决运维部署问题，它强调运维部署的自动化流程。容器化技术则是简化了环境配置操作，解决了环境初始化的问题，使得微服务能够在开发、测试、生产环境之间随意切换。 Docker 容器化技术能够封装、打包应用程序，将程序以及其依赖文件打包为一个镜像之后，便可在不同的环境快速运行，无需处理配置、依赖问题。总结通过本篇文章，我们基本梳理清楚了微服务技术架构的相关知识点。接下来，我们对以上知识点用「一句话」进行总结。 服务描述：定义服务接口名、接口参数信息、接口响应信息等内容。 服务发现与注册：抽取服务基本信息到注册中心中，实现服务注册、服务上下线管理、服务调用信息获取功能。 服务通信：通过异步消息队列或同步请求响应方式，实现 IPC 进程间通信。 服务路由：负载均衡、分组调用、全链路灰度发布。 服务稳定性治理：多用于高并发微服务场景，通过服务限流、降级、熔断、请求超时、请求重试等确保服务高可用。 服务可观测：从日志 Logging、性能指标 Metrics、链路追踪 Tracing 等三个角度，实现服务可观测。 服务安全保障：在 API 网关入口处，实现身份认证、访问控制、黑白名单过滤机制，保障服务安全，防止数据泄漏。 服务运维：结合容器化技术与 Devops，实现自动化运维，提升微服务运维效率，开发人员也可承担运维职责，实现敏捷开发。大家可以顺着知识点进行深入的学习与实践。" }, { "title": "从根儿上理解微服务02", "url": "/posts/MicroservicesTwo/", "categories": "技术科普", "tags": "学习", "date": "2023-08-07 14:32:00 +0000", "snippet": "前言上一篇文章中通过引入停车场项目的优化升级，介绍了微服务架构的演进。今天，咱们继续 👦小K 的故事，再向前推进一步，聊一聊微服务实际开发中无法回避的话题 —— 「服务拆分」。对于每一个部分，为了方便大家理解和兼顾趣味性，咱们先讲故事，再说结论。今天的故事从 👦小K 停车场项目发展到 「SOA 架构」 开始讲起，如果有不熟悉的同学，可以先看看上一篇文章~为什么选择微服务故事👦小K的停车场项目...", "content": "前言上一篇文章中通过引入停车场项目的优化升级，介绍了微服务架构的演进。今天，咱们继续 👦小K 的故事，再向前推进一步，聊一聊微服务实际开发中无法回避的话题 —— 「服务拆分」。对于每一个部分，为了方便大家理解和兼顾趣味性，咱们先讲故事，再说结论。今天的故事从 👦小K 停车场项目发展到 「SOA 架构」 开始讲起，如果有不熟悉的同学，可以先看看上一篇文章~为什么选择微服务故事👦小K的停车场项目团队中存在两个服务，一个是车场服务，另外一个是商城服务，采用的是 「SOA 架构」。随着业务逐渐发展壮大，团队人手逐渐变多，问题也随之暴露。👦小K具体分析思考之后，总结出如下几点问题： 对于同一个项目而言，项目成员在同一个分支上进行开发，总是互相干扰，经常出现代码冲突 服务的小程序端与后台管理系统端存在冗余重复代码，经常出现团队成员重复造轮子的情况 「车场服务」有时为了给「商城服务」提供相应的用户接口，需要不断修改自己的功能，导致二者业务边界逐渐模糊，与最初的规划渐行渐远 主从模式数据库无法满足新的需求，商城相关表与车场相关表耦合在一起，数据库再次出现瓶颈问题 由于服务粒度过粗，当车场或商城服务某一功能发生改动时，整个服务都需要重新部署，牵一发而动全身，无法做到敏捷开发 出现越来越多年久失修的 Bug，原因是担心耦合度过高，代码之间互相调用，很难进行修补。👦小K心里很清楚，要想解决这么多问题，简单的缝缝补补已经无济于事了，必须得从项目的架构入手！于是，他又开始了他的升级架构之旅～总结为什么需要微服务？简单总结就是： 技术永远为业务服务，当技术无法满足业务的快速增长时，就需要考虑进行技术升级或项目架构重构，以提高开发效率。而微服务正是为了解决企业长期以来堆积的问题，它是由业务催生的一种技术架构。导致传统单体项目向微服务架构进行转型的原因如下： 敏捷开发问题 传统单体项目耦合度过于严重，服务粒度太粗，使得开发人员无法专注于业务本身，而需要解决耦合所造成的各种问题，无法做到敏捷开发 项目庞大，提交代码时冲突严重，互相干扰 快速部署问题 传统部署方式打包时间过长，需要运维人员参与，效率低下 传统单体项目即使是很小的功能修改，也需要重新部署上线，牵一发而动全身，严重影响整体开发效率 维护一个庞大的项目，遇到生产环境问题难以定位、难以复现，严重影响经济收益 可扩展性问题： 模块可扩展性：增加新功能模块时，需要考虑是否影响原有功能模块，模块可扩展性差 资源可扩展性：即使是不同的功能模块，也要共用一套服务器、一套数据库，资源可扩展性差如何划分微服务？故事👦小K在想清楚问题之后，便着手开始思考如何升级微服务架构。既然是微服务，那么服务粒度需要再一次降低，也就是得拆分服务。那么应该如何拆分呢？👦小K现在面对的可不是 Toy Project，而是他从 0 到 1 搭建的商业化项目，所以这次升级架构一定得谨小慎微，划分服务之前需要与开发团队进行多次探讨，确定划分方案。如果随便划分一下服务架构，使得项目重构后反而带来负收益呢？这次，他查阅了很多划分微服务相关的资料，其中提到了很多划分规则。他根据实际业务场景，总结出了如下几点： 划分服务必须满足团队开发需求，以业务为导向，明确业务界限，再进行划分。 对于 👦小K的项目来说，目前业务界限比较清晰的有这几个模块：「用户服务」、「商品服务」、「车场服务」、「订单服务」、「支付服务」、「优惠券服务」、「推荐服务」、「消息服务」等十余个模块。 最大化地复用微服务，避免重复造轮子。 👦小K认为，对于「订单服务」、「支付服务」、「优惠券服务」而言，它既可以支撑车场出库需求，又可以支撑车品商城需求，二者代码有很多重复的地方。通过复用服务，减少了重复性的代码。 最小化地变更原有服务。 👦小K认为，对于车品商城中的「推荐功能」而言，它相对较为独立，处于服务的下游，暂时只被上游「商品服务」调用，只需要将相关的推荐代码抽取出来即可，并且也可为之后新业务提供推荐服务。 确定基础公共服务与业务服务，并从原有项目中抽象公共服务。 在 👦小K的项目中，除了与业务相关的服务外，还有一些基础服务模块，如「日志监控」、「权限校验」、「搜索服务」等，这些也需要从原有的庞大项目中抽取出来，作为独立的微服务。 尽可能避免循环依赖。若出现循环依赖的情况，需要设计中间层做冗余处理。 尽可能避免过度拆分微服务，必须结合实际业务场景做划分。 👦小K想到，如果拆分微服务过细，可能会不适应实际开发需求，反而产生负收益。比如，对于「车场服务」而言，如果进一步拆分「车场地图服务」、「车场详情服务」、「车场预约服务」的话，会导致团队进一步拆分，而车场相关的功能是一个整体，需要成员紧密合作、一同完成需求开发，不适合进行更细粒度的拆分。此外，过度拆分微服务，会导致出现分布式事务问题，复杂度上升，带来不必要的技术成本。想清楚这些后，👦小K快速地与团队成员进行了沟通，确定了第一版微服务拆分方案。总结如何拆分微服务？其实对于不同的项目场景，对应有不同的拆分方案。需要项目人员详细地分析项目需求、团队现状、业务边界、业务逻辑等方方面面，拆分的粒度既不能过细，也不能过粗，需要把控好尺度。如下为较为通用的拆分思路： 从业务模型切入，根据业务边界进行拆分。 确保一个微服务对应一个开发小组或团队。 确保拆分之后所带来的收益大于拆分之前，否则不进行拆分 考虑团队人员配置。在人员较少的情况，服务拆分的粒度不必过细。最后总结 不要为了架构而架构，不要为了 “微服务” 而 “微服务”，不要为了拆分而拆分。技术永远服务于业务。 微服务并不是银弹，其也存在缺点； 增加了复杂性 拆分微服务是一大难题，如何拆分不好，容易产生负收益 其引入了服务治理、负载均衡等基础设施，需要一定维护成本 微服务比单体项目更难理解与上手，并且其一般与 CICD / Devops / 容器化技术配套使用，对技术人员要求高，提升了技术与用人成本 高系统时延 引入微服务意味着调用链路更长，服务之间需要进行通信，可能带来额外的时延 安全保障问题 微服务安全性需要考虑的因素比单体架构更多，如服务之间的通信问题今天的微服务内容就到这里啦，主要讲解了为什么需要微服务、如何拆分微服务以及微服务的缺点。" }, { "title": "从根儿上理解微服务01", "url": "/posts/MicroservicesOne/", "categories": "技术科普", "tags": "学习", "date": "2023-07-31 13:17:00 +0000", "snippet": "前言​\t今天来聊一聊微服务，想来微服务大伙们都不陌生，是后端技术学习的进阶之路，想来第一次与微服务结缘是在我的第一个 SpringBoot 项目，我接触到了 Nacos 和 Gateway，从那时起，便对微服务产生了浓厚的兴趣，微服务的架构设计、服务拆分、服务治理等内容确实很有意思，这其实也是我选择从事后端开发的原因之一。​\t学了一段时间后，我想着是时候输出一些内容了，把微服务以更加通俗易懂...", "content": "前言​\t今天来聊一聊微服务，想来微服务大伙们都不陌生，是后端技术学习的进阶之路，想来第一次与微服务结缘是在我的第一个 SpringBoot 项目，我接触到了 Nacos 和 Gateway，从那时起，便对微服务产生了浓厚的兴趣，微服务的架构设计、服务拆分、服务治理等内容确实很有意思，这其实也是我选择从事后端开发的原因之一。​\t学了一段时间后，我想着是时候输出一些内容了，把微服务以更加通俗易懂的方式讲出来，让更多人轻松地了解学习微服务的核心内容。​\t话不多说，咱们进入今天的正题 —— 从根儿上理解微服务。微服务架构演进顺着互联网发展的历史，我们先来捋一捋项目的架构演进。为了使大家更能通俗易懂地理解，我们引入一个实际的停车场移动端/小程序项目开发场景，一步步地优化其项目架构。也希望大家在看的时候，想一想，如果让你来实现这个项目，应该如何做如何设计？大家可以从一个初创项目的角度出发，而不是一上来就直接考虑高大上的技术架构。因为技术永远依托于业务，服务于业务，技术选型本质是一个 Trade-off，我们需要以最小的金钱 / 技术 / 精力成本，换取局部 / 当前最优的实现方式。从 0 到 1 的思考过程，使我们能够从现实的个人开发者角度看待问题，代入到真正的开发场景，加深对于微服务架构演进的理解。为什么不用商城项目举例呢？因为商城项目太过泛滥，容易让大家产生思维定式，认为商城项目使用微服务架构是理所应当。接下来，我们以 👦小K的视角，看看他是如何从 0 到 1 开始他的停车场项目，并逐步优化为微服务架构～单实例单数据库的单体架构某一天，👦小K在某停车场停车时，发现每次出这个停车场，都得先叫醒值班室睡得正香的保安大爷。扫二维码付款后，保安大爷按下按钮，道闸抬起后才能出去。此时，👦小K陷入了深深的沉思：为什么我不能将所有与人打交道的操作放入手机中呢？他仔细想了想，最后从多个需求中提取出了核心需求： 查看附近车场位置及其它详情信息 停车前支持预约指定车场 停车后支持在手机上扫码下订单、付款 查询记录 与停车场网络摄像头、道闸等硬件设备联调，实现智能入库出库👦小K本身也是计算机科班出身，做个小程序不在话下。他立马拉了两三个技术不错的朋友，和他一起做这个项目。他在做技术选型时，想到之前某个叫什么「YKFire」的人说过，初创项目尽量不选用高大上的技术架构。于是，👦小K选择了他最熟悉的技术栈：前端 Uniapp&amp;Vue + 后端 Spring 全家桶 + MySQL 数据库。并且，他根据职责单一的设计模式，将车场详情功能、预约功能、订单功能、支付功能等拆分到了不同的模块，尽可能地降低耦合性，考虑的算是很周到啦哈哈哈～做好技术选型后，👦小K便开始方案设计、数据库设计、实现代码等一系列操作，很快啊，👦小K完成了停车场项目的第一个版本，并将其部署到了他白嫖的 2 核 2 G 云服务器上，并顺利地上线了小程序和后台管理系统网站～做完后的他也没闲着，赶快把项目的架构画了出来，便于日后项目升级时进行 review（👦小K果然深思熟虑～）多实例单数据库的单体架构👦小K在上线之前，联系了很多实体商场、停车场的老板，经过一番软磨硬泡，终于有几个老板愿意尝试用他的系统。在 👦小K的一番努力下，车场用户数越来越多，每天 DAU 上千甚至过万。但随着小程序流量越来越大，预约或付款时开始出现访问迟缓的问题。👦小K很快便意识到这一性能瓶颈。为了尽快解决，他升级了服务器的配置。虽然问题得到了一定的缓解，但是 👦小K 目光更加长远，他想控制整个城市的车场业务。为了长期的发展，他综合考虑了经济成本与可扩展性，决定不再纵向升级，而是采取横向扩容的方式，缓解单节点压力。为了实现分担不同节点压力，👦小K又在这些服务器实例智商，引入了负载均衡层。这么一番骚操作下来，性能问题立马得到了解决，赢来了初期用户的一致好评。这次，👦小K再一次画下了升级后的系统架构图。多实例多数据库的单体架构车场小程序持续运作了一年多的时间，用户数量、预约数量、订单数量越来越多，而且又有好几个停车场加入了 👦小K 的旗下，项目越做越大。总数据量逐渐达到了千万级别。此时，性能瓶颈问题又出现了。👦小K发现这次即使再次进行横向扩容，也无济于事，用户的响应时间仍然居高不下，惹来差评一片。👦小K有点慌，但他也立马定位到了问题所在 —— 数据库。👦小K立马对数据库架构进行改进，他想起之前看到过一篇文章里说到，读写分离是最低廉的解决数据库性能瓶颈的方式。说干就干，他立马搭建起了多个数据库： 一个主数据库专门用于处理写请求。当用户进行更新、删除等写操作时，请求打到这台主数据库。 其他从数据库处理读请求。当用户进行读操作时，请求平均打散到这些从数据库中。聪明的 👦小K也想到，为了保证主从数据库的一致性，主数据定期将数据同步到从数据库中。虽然问题得到了解决，但思考长远的 👦小K担心不久后再次出现这种类似的问题，他干脆一不做二不休，咬咬牙引入了 Redis 缓存，存储用户的订单数据、车场经纬度数据，加快响应速度。按照惯例，每次升级后，👦小K都会画出对应的系统架构图。（👍 爆赞）SOA 架构随着项目越做越大，嗅觉敏锐的 👦小K 逐渐发现了新的市场需求。他竟然想在他的小程序中卖货！做一个车品商城！这么一来，小程序功能数量翻倍，但他人手不太够了。他又招了几个人，一起搭建这个简单的商城。但是做着做着，他发现不对劲。每次车品商城进行测试时，都需要整体测试、并重新构建和部署，严重影响到了核心车场功能的维护与开发，压根没办法进行敏捷开发。于是，他和初创团队的人开始考虑着手进行项目重构，将两个项目分开，项目之间用消息总线 ESB 进行通信。团队也被拆分为两个小组，开发进程互不影响。 消息总线 ESB：负责协议转换、消息路由、负载均衡就这样，👦小K团队再次向前迈进了一步，从原先的单体架构升级到了 SOA 架构。微服务架构👦小K的车场小程序越做越大，在他团队几年的努力经营下，几乎实现了他当初的野心，引入了该城市的大多数车场。但他仍然不满足于此，他想要将他的停车场小程序更加现代化，他认为其它应用有的他也要有！于是，他引进了推荐系统，并且添加了很多额外的功能，如：优惠券、系统通知、加油站、电动车充电、洗车等多个业务。项目逐渐变得越来越庞大，开发人员也越来越多，团队达到了大几十人。这时，👦小K意识到原来的服务拆分粒度太 “粗”，而且原先系统的所有流量都会经过 ESB 进行分发，造成系统的可用性下降，甚至在流量峰值出现部分节点宕机的情况，这是一大隐患。于是，👦小K又开始着手进行项目改进。首先，他决定进一步拆分团队，拆分服务，将原先的车场服务与车品商城服务又进行了拆分，拆出了用户服务、商品服务、车场服务、订单服务、支付服务、优惠券服务、推荐服务等 10 余个服务。为了治理这些服务，他的团队引入了微服务架构，用微服务网关与注册中心代替原有的消息总线，并采用分布式部署。除此之外，一系列微服务治理框架与 Devops 自动化运维部署流程也随之引入，每一个微服务均通过容器化方式部署上线。 可恶的 👦小K并没有告诉我具体内容，他说等他有空再问他～总结故事讲完了，相信大家一定对微服务架构的演进，有了更加深刻的认识。今天，我们从一个车场项目的角度出发，分析了为什么要进行架构升级，并给出了每个架构的示意图。现在我们来总结一下各个架构的特点： 单体架构：所有功能都集中在同一个项目内，统一测试，统一部署，牵一发而动全身，适合初创项目进行试错，不适合大型商业项目。 在此基础做的纵向升级、横向扩容、一主多从等仍然属于单体架构，因为其仍然是单一服务、单一项目。 SOA 架构：进行了初步的服务拆分，但是服务拆分的粒度较粗，并且没有引入服务治理组件，流量集中于消息总线。 微服务架构：是 SOA 架构的升级，基本沿用了 SOA 的思想，但是服务拆分粒度更细，并且引入了服务治理组件，结合流行的容器化技术，实现 Devops 自动化部署与运维。 Devops 总是与微服务一起出现，因为 Devops 带来的是敏捷开发、更快交付，而微服务通常由各个小团队负责开发，迭代周期更快、更敏捷。 CI/CD 是 Devops 的核心。其实，仔细想想微服务架构也就那么回事，还是文章开头提到的那句话，技术永远服务于业务，正是由于互联网的扩张与业务的膨胀，才会倒逼技术架构进行升级迭代，最后有了今天的微服务。这也提醒着作为技术开发的我们，需要时刻关注业务，根据业务来决定我们的技术走向，去适应业务的发展。那么今天的微服务内容就到这里啦，大家先对微服务架构有一个基本的理解，至于本篇文章提到的负载均衡、服务治理、容器化技术等内容，我会在之后的文章中进行更详细的讲解～" }, { "title": "通过调试技术，我理清楚了b站视频播放很快的原理", "url": "/posts/rangeApplication/", "categories": "技术科普", "tags": "学习", "date": "2023-07-24 05:28:00 +0000", "snippet": "range应用b 站视频播放的是很快的，基本是点哪就播放到哪。而且如果你上次看到某个位置，下次会从那个位置继续播放。那么问题来了：如果一个很大的视频，下载下来需要很久，怎么做到点哪个位置快速播放那个位置的视频呢？前面写过一篇 range 请求的文章，也就是不下载资源的全部内容，只下载 range 对应的范围的部分。那视频的快速播放，是不是也是基于 range 来实现的呢？我们先复习下 ran...", "content": "range应用b 站视频播放的是很快的，基本是点哪就播放到哪。而且如果你上次看到某个位置，下次会从那个位置继续播放。那么问题来了：如果一个很大的视频，下载下来需要很久，怎么做到点哪个位置快速播放那个位置的视频呢？前面写过一篇 range 请求的文章，也就是不下载资源的全部内容，只下载 range 对应的范围的部分。那视频的快速播放，是不是也是基于 range 来实现的呢？我们先复习下 range 请求：请求的时候带上 range：服务端会返回 206 状态码，还有 Content-Range 的 header 代表当前下载的是整个资源的哪一部分：这里的 Content-Length 是当前内容的长度，而 Content-Range 里是资源总长度和当前资源的范围。 想要了解更多关于 Range 的介绍可以看这篇文章：[基于 HTTP Range 实现文件分片并发下载 YKFire](https://ykfire.github.io/posts/range/) 那 b 站视频是不是用 Range 来实现的快速播放呢？我们先在知乎的视频试一下：随便打开一个视频页面，然后打开 devtools，刷新页面，拖动下进度条，可以看到确实有 206 的状态码：我们可以在搜索框输入 status-code:206 把它过滤出来：这是一种叫过滤器的技巧：可以根据 method、domain、mime-type 等过滤。has-response-header：过滤响应包含某个 header 的请求method：根据 GET、POST 等请求方式过滤请求domain: 根据域名过滤status-code：过滤响应码是 xxx 的请求，比如 404、500 等larger-than：过滤大小超过多少的请求，比如 100k，1Mmime-type：过滤某种 mime 类型的请求，比如 png、mp4、json、html 等resource-type：根据请求分类来过滤，比如 document 文档请求，stylesheet 样式请求、fetch 请求，xhr 请求，preflight 预检请求cookie-name：过滤带有某个名字的 cookie 的请求当然，这些不需要记，输入一个 - 就会提示所有的过滤器：但是这个减号之后要去掉，它是非的意思。然后点开状态码为 206 的请求看一下：确实，这是标准的 range 请求。我点击进度条到后面的位置，可以看到发出了新的 range 请求：那这些 range 请求有什么关系呢？我们需要分析下 Content-Range，但是一个个点开看不直观。这时候可以自定义显示的列：右键单击列名，可以勾选展示的 header，不过这里面没有我们想要的 header，需要自定义：点击 Manage Header Columns 添加自定义的 header，输入 Content-Range:这时候就可以直观的看出这些 range 请求的范围之间的关系：点击 Content-Range 这一列，升序排列。我们刷新下页面，从头来试一下：随着视频的播放，你会看到一个个 range 请求发出：这些 range 请求是能连起来的，也就是说边播边下载后面的部分。视频进度条这里的灰条也在更新：当你直接点击后面的进度条：观察下 range，是不是新下载的片段和前面不连续了？也就是说会根据进度来计算出 range，再去请求。那这个 range 是完全随意的么？并不是。我们当前点击的是 15:22 的位置：我刷新下页面，点击 15:31 的位置：如果是任意的 range，下载的部分应该和之前的不同吧。但是你观察下两次的 range，都是 2097152-3145727也就是说，视频分成多少段是提前就确定的，你点击进度条的时候，会计算出在哪个 range，然后下载对应 range 的视频片段来播放。那有了这些视频片段，怎么播放呢？浏览器有一个 SourceBuffer 的 api，我们在 MDN 看一下：大概是这样用的：也就是说，可以一部分一部分的下载视频片段，然后 append 上去。拖动进度条的时候，可以把之前的部分删掉，再 append 新的：我们验证下，搜索下代码里是否有 SourceBuffer：按住 command + f 可以搜索请求内容：可以看到搜索出 3 个结果。在其中搜索下 SourceBuffer：可以看到很多用到 SourceBuffer 的方法，基本可以确认就是基于 SourceBuffer 实现的。也就是说，知乎视频是通过 range 来请求部分视频片段，通过 SourceBuffer 动态播放这个片段，来实现的快速播放的目的。具体的分段是提前确定好的，会根据进度条来计算出下载哪个 range 的视频。那服务端是不是也要分段存储这些视频呢？确实，有这样一种叫做 m3u8 的视频格式，它的存储就是一个个片段 ts 文件来存储的，这样就可以一部分一部分下载。不过知乎没用这种格式，还是 mp4 存储的，这种就需要根据 range 来读取部分文件内容来返回了：再来看看 b 站，它也是用的 range 请求的方式来下载视频片段：大概 600k 一个片段：下载 600k 在现在的网速下需要多久？这样播放能不快么？相比之下，知乎大概是 1M 一个片段：网速不快的时候，体验肯定是不如 b 站的。而且 b 站用的是一种叫做 m4s 的视频格式：它和 m3u8 类似，也是分段存储的，这样提前分成不同的小文件，然后 range 请求不同的片段文件，速度自然会很快。然后再 command + f 搜索下代码，同样是用的 SourceBuffer：这样，我们就知道了为什么 b 站视频播放的那么快了：m4s 分段存储视频，通过 range 请求动态下载某个视频片段，然后通过 SourceBuffer 来动态播放这个片段。总结我们分析了 b 站、知乎视频播放速度很快的原因。结论是通过 range 动态请求视频的某个片段，然后通过 SourceBuffer 来动态播放这个片段。这个 range 是提前确定好的，会根据进度条来计算下载哪个 range 的视频。播放的时候，会边播边下载后面的 range，而调整进度的时候，也会从对应的 range 开始下载。服务端存储这些视频片段的方式，b 站使用的 m4s，当然也可以用 m3u8，或者像知乎那样，动态读取 mp4 文件的部分内容返回。除了结论之外，调试过程也是很重要的：我们通过 status-code 的过滤器来过滤出了 206 状态码的请求。通过自定义列在列表中直接显示了 Content-Range：通过 command + f 搜索了响应的内容：这篇文章就是对这些调试技巧的综合运用。以后再看 b 站和知乎视频的时候，你会不会想起它是基于 range 来实现的分段下载和播放呢？" }, { "title": "基于 HTTP Range 实现文件分片并发下载", "url": "/posts/range/", "categories": "技术科普", "tags": "学习", "date": "2023-07-17 04:13:00 +0000", "snippet": "Range请求实践下载文件是一个常见的需求，只要服务端设置 Content-Disposition 为 attachment 就可以。比如这样：const express = require('express');const app = express();app.get('/aaa',(req, res, next) =&gt; { res.setHeader('Content-Di...", "content": "Range请求实践下载文件是一个常见的需求，只要服务端设置 Content-Disposition 为 attachment 就可以。比如这样：const express = require('express');const app = express();app.get('/aaa',(req, res, next) =&gt; { res.setHeader('Content-Disposition','attachment; filename=\"guang.txt\"') res.end('guangguang');})app.listen(3000, () =&gt; { console.log(`server is running at port 3000`)})设置 Cotent-Disposition 为 attachment，指定 filename。然后 html 里加一个 a 标签：&lt;!DOCTYPE html&gt;&lt;html lang=\"en\"&gt;&lt;body&gt; &lt;a href=\"http://localhost:3000/aaa\"&gt;download&lt;/a&gt;&lt;/body&gt;&lt;/html&gt;跑起静态服务器：点击链接就可以下载：如果文件比较大，比如 500M，当你下载了 499M 的时候突然断网了，这时候下载就失败了，你就要从头再重新下载一次。体验就很不爽。能不能基于上次下载的地方接着下载，也就是断点续传呢？可以的，HTTP 里有这部分协议，就是 range 相关的 header。看下 MDN 对 range 的解释：就是说你可以通过 Range 的 header 告诉服务端下载哪一部分内容。比如这样：Range: bytes=200-1000就是下载 200-1000 字节的内容（两边都是闭区间），服务端返回 206 的状态码，并带上这部分内容。可以省略右边部分，代表一直到结束：Range: bytes=200-也可以省略左边部分，代表从头开始：Range: bytes=-1000而且可以请求多段 range，服务端会返回多段内容：Range: bytes=200-1000, 2000-6576, 19000-知道了 Range header 的格式，我们来试一下吧！添加这样一个路由：app.get('/', (req, res, next) =&gt; { res.setHeader('Access-Control-Allow-Origin', '*'); res.download('index.txt', { acceptRanges: true })})设置允许跨域请求。res.download 是读取文件内容返回，acceptRanges 选项为 true 就是会处理 range 请求（其实默认就是 true）文件 index.txt 的内容是这样的：然后在 html 里访问一下这个接口：&lt;!DOCTYPE html&gt;&lt;html lang=\"en\"&gt;&lt;head&gt; &lt;script src=\"https://www.unpkg.com/axios@1.3.5/dist/axios.min.js\"&gt;&lt;/script&gt;&lt;/head&gt;&lt;body&gt; &lt;script&gt; axios.get('http://localhost:3000', { headers: { Range: 'bytes=0-4', } }).then((res) =&gt; { console.log(res.data); }).catch((err) =&gt; { console.log(err); }) &lt;/script&gt;&lt;/body&gt;&lt;/html&gt;访问页面，可以看到返回的是 206 的状态码！这时候 Content-Length 就代表返回的内容的长度。还有个 Content-Range 代表当前 range 的长度以及总长度。当然，你也可以访问 5 以后的内容&lt;!DOCTYPE html&gt;&lt;html lang=\"en\"&gt;&lt;head&gt; &lt;script src=\"https://www.unpkg.com/axios@1.3.5/dist/axios.min.js\"&gt;&lt;/script&gt;&lt;/head&gt;&lt;body&gt; &lt;script&gt; axios.get('http://localhost:3000', { headers: { Range: 'bytes=5-', } }).then((res) =&gt; { console.log(res.data); }).catch((err) =&gt; { console.log(err); }) &lt;/script&gt;&lt;/body&gt;&lt;/html&gt;返回的是这样的：这俩连接起来不就是整个文件的内容么？这样就实现了断点续传！我们再来试试如果超出 range 会怎么样：&lt;!DOCTYPE html&gt;&lt;html lang=\"en\"&gt;&lt;head&gt; &lt;script src=\"https://www.unpkg.com/axios@1.3.5/dist/axios.min.js\"&gt;&lt;/script&gt;&lt;/head&gt;&lt;body&gt; &lt;script&gt; axios.get('http://localhost:3000', { headers: { Range: 'bytes=500-600', } }).then((res) =&gt; { console.log(res.data); }).catch((err) =&gt; { console.log(err); }) &lt;/script&gt;&lt;/body&gt;&lt;/html&gt;请求 500-600 字节的内容，这时候响应是这样的：​\t返回的是 416 状态码，代表 range 不合法。Range 不是还可以设置多段么？多段内容是怎么返回的呢？我们来试一下：&lt;!DOCTYPE html&gt;&lt;html lang=\"en\"&gt;&lt;head&gt; &lt;script src=\"https://www.unpkg.com/axios@1.3.5/dist/axios.min.js\"&gt;&lt;/script&gt;&lt;/head&gt;&lt;body&gt; &lt;script&gt; axios.get('http://localhost:3000', { headers: { Range: 'bytes=0-2,4-5,7-', } }).then((res) =&gt; { console.log(res.data); }).catch((err) =&gt; { console.log(err); }) &lt;/script&gt;&lt;/body&gt;&lt;/html&gt;我分了 0-2, 4-5, 7- 这三段 range。重新访问一下，这时候报了一个跨域的错误，说是发送预检请求失败。浏览器会在三种情况下发送预检（preflight）请求： 用到了非 GET、POST 的请求方法，比如 PUT、DELETE 等，会发预检请求看看服务端是否支持 用到了一些非常规请求头，比如用到了 Content-Type，会发预检请求看看服务端是否支持 用到了自定义 header，会发预检请求为啥 Range 头单个 range 不会触发预检请求，而多个 range 就触发了呢？因为多个 range 的时候返回的 Content-Type 是不一样的，是 multipart/byteranges 类型，比较特殊。预检请求是 options 请求，那我们就支持一下：app.options('/', (req, res, next) =&gt; { res.setHeader('Access-Control-Allow-Origin', '*'); res.setHeader('Access-Control-Allow-Headers', 'Range') res.end('');});然后重新访问：这时候你会发现虽然是 206 状态码，但返回的是整个内容！这是因为 express 只做了单 range 的支持，多段 range 可能它觉得没必要支持吧。毕竟你发多个单 range 请求就能达到一样的效果。MDN 官网的图片是支持多 range 请求的，我们用那个看看：请求 3 个 range 的内容。可以看到返回的 Content-Type 确实是 multipart/byteranges，然后指定了 boundary 边界线：响应内容是这样的：以这些 boundary 分界线隔开的每一段内容都包含 Content-Type、Content-Range 基于具体的那段 range 的内容。这就是 multipart/byteranges 的格式，和我们常用的 multipart/form-data 很类似，只不过具体每段包含的 header 不同：话说回来，其实 express 只支持单段 range 问题也不大，不就是多发几个请求就能达到一样的效果。下面我们就用 range 来实现下文件的分片下载，最终合并成一个文件的功能。我们来下载一个图片吧，分成两块下载，然后下载完合并起来。app.get('/', (req, res, next) =&gt; { res.setHeader('Access-Control-Allow-Origin', '*'); res.download('guangguang.png', { acceptRanges: true })})我们写下分片下载的代码，就分两段：这个图片是 626k，也就是 626000 字节，那我们就分成 0-300000 和 300001- 两段：&lt;!DOCTYPE html&gt;&lt;html lang=\"en\"&gt;&lt;head&gt; &lt;script src=\"https://www.unpkg.com/axios@1.3.5/dist/axios.min.js\"&gt;&lt;/script&gt;&lt;/head&gt;&lt;body&gt; &lt;script&gt; axios.get('http://localhost:3000', { headers: { Range: 'bytes=0-300000', } }).then((res) =&gt; { }).catch((err) =&gt; { console.log(err); }) axios.get('http://localhost:3000', { headers: { Range: 'bytes=300001-', } }).then((res) =&gt; { }).catch((err) =&gt; { console.log(err); }) &lt;/script&gt;&lt;/body&gt;&lt;/html&gt;试一下，两个响应分别是这样的：第一个响应还能看到图片的预览，确实只有一部分：然后我们要把两段给拼起来，怎么拼呢？操作二进制数据要用 JS 的 ArrayBuffer api 了：浏览器还有个特有的 Blob api 也是用于操作二进制数据的。我们指定下响应的类型为 arraybuffer：&lt;!DOCTYPE html&gt;&lt;html lang=\"en\"&gt;&lt;head&gt; &lt;script src=\"https://www.unpkg.com/axios@1.3.5/dist/axios.min.js\"&gt;&lt;/script&gt;&lt;/head&gt;&lt;body&gt; &lt;script&gt; const p1 = new Promise((resolve, reject) =&gt; { axios.get('http://localhost:3000', { headers: { Range: 'bytes=0-300000', }, responseType: 'arraybuffer' }).then((res) =&gt; { resolve(res.data) }).catch((err) =&gt; { reject(err) }) }) const p2 = new Promise((resolve, reject) =&gt; { axios.get('http://localhost:3000', { headers: { Range: 'bytes=300001-', }, responseType: 'arraybuffer' }).then((res) =&gt; { resolve(res.data) }).catch((err) =&gt; { reject(err) }) }) Promise.all([p1, p2]).then(res =&gt; { const [buffer1, buffer2] = res; console.log(buffer1, buffeer2) }) &lt;/script&gt;&lt;/body&gt;&lt;/html&gt;两个 ArrayBuffer 怎么合并呢？ArrayBuffer 本身只是存储二进制数据的，要操作二进制数据要使用具体的 DataView 的子类。比如我们想以字节的方式操作，那就是 Uint8Array 的方式（Uint 是 unsigned integer，无符号整数）：Promise.all([p1, p2]).then(res =&gt; { const [buffer1, buffer2] = res; const arr = new Uint8Array(buffer1.byteLength + buffer2.byteLength); const arr1 = new Uint8Array(buffer1); arr.set(arr1, 0); const arr2 = new Uint8Array(buffer2); arr.set(arr2, arr1.byteLength); console.log(arr.buffer)})每个 arraybuffer 都创建一个对应的 Uint8Array 对象，然后创建一个长度为两者之和的 Uint8Array 对象，把两个 Uint8Array 设置到不同位置。最后输出合并的 Uint8Array 对象的 arraybuffer。这样就完成了合并：合并之后就是整个图片了。那自然可以作为图片展示，也可以下载。我添加一个 img 标签：&lt;img id=\"img\"/&gt;然后把 ArrayBuffer 转成 Blob 设置以对象形式设置为 img 的 urljavascript复制代码const blob = new Blob([arr.buffer]);const url = URL.createObjectURL(blob);img.src =url;现在就可以看到完整的图片了:现在我们就实现了文件的分片下载再合并！甚至，你还可以再做一步下载：const link = document.createElement('a');link.href = url;link.download = 'image.png';document.body.appendChild(link);link.click();link.addEventListener('click', () =&gt; { link.remove();});现在的全部代码如下：&lt;!DOCTYPE html&gt;&lt;html lang=\"en\"&gt;&lt;head&gt; &lt;script src=\"https://www.unpkg.com/axios@1.3.5/dist/axios.min.js\"&gt;&lt;/script&gt;&lt;/head&gt;&lt;body&gt; &lt;img id=\"img\"/&gt; &lt;script&gt; const p1 = new Promise((resolve, reject) =&gt; { axios.get('http://localhost:3000', { headers: { Range: 'bytes=0-300000', }, responseType: 'arraybuffer' }).then((res) =&gt; { resolve(res.data) }).catch((err) =&gt; { reject(err) }) }) const p2 = new Promise((resolve, reject) =&gt; { axios.get('http://localhost:3000', { headers: { Range: 'bytes=300001-', }, responseType: 'arraybuffer' }).then((res) =&gt; { resolve(res.data) }).catch((err) =&gt; { reject(err) }) }) Promise.all([p1, p2]).then(res =&gt; { const [buffer1, buffer2] = res; const arr = new Uint8Array(buffer1.byteLength + buffer2.byteLength); const arr1 = new Uint8Array(buffer1); arr.set(arr1, 0); const arr2 = new Uint8Array(buffer2); arr.set(arr2, arr1.byteLength); const blob = new Blob([arr.buffer]); const url = URL.createObjectURL(blob); img.src = url; const link = document.createElement('a'); link.href = url; link.download = 'image.jpg'; document.body.appendChild(link); link.click(); link.addEventListener('click', () =&gt; { link.remove(); }); }) &lt;/script&gt;&lt;/body&gt;&lt;/html&gt;总结文件下载是常见需求，只要设置 Content-Disposition 为 attachment 就可以。但大文件的时候，下载中断了再重新传体验不好，或者想实现分片下载再合并的功能，这时候就可以用 Range 了。在请求上带上 Range 的范围，如果服务器不支持，就会返回 200 加全部内容。如果服务器支持 Range，会返回 206 的状态码和 Content-Range 的 header，表示这段内容的范围和全部内容的总长度。如果 Range 超出了，会返回 416 的 状态码。多段 Range 的时候，会返回 multipart/byteranges 的格式，类似 multipart/form-data 一样，都是通过 boundary 分割的，每一段都包含 Content-Range 和内容。express 不支持多段 range，但我们可以通过发多个请求达到一样的效果。我们基于 Range 实现了文件的分片下载，浏览器通过 ArrayBuffer 接收。ArrayBuffer 只读，想要操作要通过 Uint8Array 来合并，之后再转为 ArrayBuffer。这样就可以通过 URL.createObjectURL 设置为 img 的 src 或者通过 a 标签的 download 属性实现下载了。" }, { "title": "GPT Prompt", "url": "/posts/gptprompt/", "categories": "技术科普", "tags": "学习", "date": "2023-07-10 13:03:00 +0000", "snippet": "​\t从今年年初，OpenAI发布的ChatGPT已摧古拉朽之势席卷全球，短短两个月注册用户数就超过1亿人，是全世界增长速度最快的应用。你如果不聊两句ChatGPT,都不好意思出门。很多人都说今年是AI元年，其实也是有一定道理的，之前的AI门槛相对较高，很多人没有机会参与其中，而类ChatGPT的出现，把AIGC的门槛几乎降到了零，让普通人也可以参与到AI的浪潮中，一个人人可以AI创业的时代到...", "content": "​\t从今年年初，OpenAI发布的ChatGPT已摧古拉朽之势席卷全球，短短两个月注册用户数就超过1亿人，是全世界增长速度最快的应用。你如果不聊两句ChatGPT,都不好意思出门。很多人都说今年是AI元年，其实也是有一定道理的，之前的AI门槛相对较高，很多人没有机会参与其中，而类ChatGPT的出现，把AIGC的门槛几乎降到了零，让普通人也可以参与到AI的浪潮中，一个人人可以AI创业的时代到来了！一、如何与ChatGPT对话，Prompt是什么？通过上一篇博客的内容，我们基本了解了ChatGPT是什么，以及他大概具有智能的原因。其实，我们最关心的并不是他多牛逼，而是，我如何才能借助这样的工具，为自身赋能、降本增效。如何与ChatGPT沟通交流呢，这里就要说到本文的重点Prompt了。遇事不决，先问GPT。我们先问一下ChatGPT,Prompt是什么？以上是ChatGPT的回答。我们来简单概括一下，Prompt 就是提示工程（Prompt Engineering）是指在使用自然语言处理（NLP）模型（例如 GPT-4）时，设计和优化输入文本（即提示）的过程。这一过程旨在更有效地引导模型生成所需的输出结果。提示工程的关键在于提高模型的表现，使其更准确、有趣或符合特定上下文要求。好了，那如何才能真正高效的与GPT沟通呢？让他能理解我们的问题，给出高质量的答案。就像与人沟通其实也是有很多技巧学问的，同样的，其实跟ChatGPT沟通也是有一些规律、模版是可以遵循的。二、如ChatGPT提问才能得到更加精准的回复我们向ChatGPT提问的目的就是为了得到想要的答案，很多朋友说GPT不好用，给的答案都是鸡肋，没有价值。大家有没有想过，这跟自己的提问技巧有关系呢?我们在生活中其实也会经常遇到一些不会提问的人，比如有些人在没交代任何背景的情况下，上来就问“如何成为一个有钱人”，让人不知道怎么答复。哪怕你问，作为一个刚毕业的大学生，如何在一个月时间挣到2W块这种问题，很多人也会给你一些建议。所以提问技巧是一个未来社会必备的技能，不光跟类GPT的AI进行交互需要，跟人交流其实也很有必要好好提升一下提问技巧。我们先来直接问问ChatGPT,如何提问才能得到高质量的答案ChatGPT列出了7条建议。其实网上关于Prompt优化的调教模版很多，我觉得说的都有道理，不过适合自己的才是最好的，大家可以在日常生活中根据自己的使用场景多去尝试，总结出适合自己的模版。网上关于ChatGPT的指令生成器非常多，比如ChatGPT指令大全：https://www.explainthis.io/zh-hans/chatgptChatGPT 快捷指令：https://www.aishort.top/我相对还是比较推荐台风大佬做的ChatGPT角色生成器这个网站（https://role.aicosplay.com.cn/），简单好用这里我也抛砖引玉，把市面上的模版做个简化，已经足够日常使用，还是那句话，只有自己实践的才是最适合自己的。我的Prompt优化调教模版主要包括四个方面1.角色在对话过程中，你希望GPT扮演什么角色，让他用一个专家的身份更加精准的回复你的问题2.背景发起指令的背景是什么？有利于ChatGPT更好的理解上下文相关信息3.任务你希望让GPT帮你干什么？你希望他如何解决你的问题4.要求你希望GPT用什么风格回复你、回复的内容的长度、内容的形式、有什么特殊要求等等下面，我们通过几个示例来学习如何使用上面的调教模版。三、通过具体示例来说明如何使用prompt优化调教模版我们先来一个全局的，让ChatGPT从上面的角色、背景、任务、要求四个维度来帮我们写几个Prompt调教示例以下是一些调教模版示例：示例一：你想让chatGPT充当用于翻译，可以这么说示例二：如果你要做表格，可以这么说：示例三：让chatGPT扮演一位心灵导师示例四：让chatGPT扮演一个文案写手示例五：AI佛祖六.其他一些需要注意的方面 ChatGPT有上下文的概念，针对同一个问题，你可以连续的向ChatGPT一直提问，GPT会记住你之前的问题。基于这个特点，就要求我们一个对话场景，尽量单独开一个新的对话窗口，通过一个角色来跟GPT对话，以免不同的角色相互影响，导致GPT回答一些不相关的问题。 及时给予ChatGPT反馈。告诉他，你是如何看待他给出的答案的。如果生成的不错，就夸一夸他，让他知道你喜欢他回复的内容；如果输出的内容不符合心意就告诉他，你不喜欢他的答案，并告诉他怎么去改进，让他继续给你生成想要的答案。你的每次反馈都决定了他接下来输出内容做如何微调。我们先随意让ChatGPT帮我们写一条小红书标题。大家看到写的非常一般，也不符合小红书的调用。所以，我们要批评他一下，让他按照我们的要求进行改进。这次改写后，感觉有点那个意思了。另外，如果你觉得回复的不错，也可以点那个大拇指给个赞，或者觉得回答的不好，点第二个大拇指给个踩，你的反馈，ChatGPT可以收到。 逆向思维因为ChatGPT会对一些信息做屏蔽，所以你直接搜是搜不到了，这里不是教大家做错事，而是说有些时候我们确实需要某些信息，如果得不到的话，可以反其道而行之。比如，我直接问，北京的色情场所有哪些，ChatGPT是拒绝回答的。但是，我反向去问，感觉有点那个啥意思了😏四、结束语当然还有很多技巧，不可能在一篇文章里全部面面俱到，后期我会在三部曲中的其他两部里尽量多介绍一些。不过，任何一个技巧、模版都不可能是万能的，很多时候，模版或技巧确实可以帮我们快速的达到及格线的水平，但是如果一直靠这些花里胡哨的东西，你永远也不可能成为一个行业顶尖的人才。很多东西是需要大家下苦功夫，在实战中一点点慢慢摸索出来的。实践才是检验真理的唯一标准。" }, { "title": "深度了解ChatGPT", "url": "/posts/gpt/", "categories": "随笔", "tags": "学习", "date": "2023-07-02 14:47:00 +0000", "snippet": "​\t从今年年初，OpenAI发布的ChatGPT已摧古拉朽之势席卷全球，短短两个月注册用户数就超过1亿人，是全世界增长速度最快的应用。你如果不聊两句ChatGPT,都不好意思出门。很多人都说今年是AI元年，其实也是有一定道理的，之前的AI门槛相对较高，很多人没有机会参与其中，而类ChatGPT的出现，把AIGC的门槛几乎降到了零，让普通人也可以参与到AI的浪潮中，一个人人可以AI创业的时代到...", "content": "​\t从今年年初，OpenAI发布的ChatGPT已摧古拉朽之势席卷全球，短短两个月注册用户数就超过1亿人，是全世界增长速度最快的应用。你如果不聊两句ChatGPT,都不好意思出门。很多人都说今年是AI元年，其实也是有一定道理的，之前的AI门槛相对较高，很多人没有机会参与其中，而类ChatGPT的出现，把AIGC的门槛几乎降到了零，让普通人也可以参与到AI的浪潮中，一个人人可以AI创业的时代到来了！一、什么是ChatGPT?ChatGPT从字面上可以分解成两个词Chat+GPT。Chat是聊天的意思，GPT是Generative Pre-trained Transformer的缩写，生成式预训练语言模型，使用Transformer架构来处理自然语言处理（NLP）任务。也就是说GPT能理解自然语言，大家能够用汉语、英语等自然语言跟GPT交流，而且它有大量的训练语料，超大规模的训练参数（上千亿），能自己生成内容，并不是像搜索引擎一样只是简单的检索，就算一个它不知道的东西，它都可以根据已掌握的数据，生成一个答案，虽然有时候可能在胡说八道，从这个角度，确实已经很像人类了。总结一下就是，他有丰富的知识库，是一个知识渊博的智者，当你向他提问时，他能听懂你的提问，并且可以非常智能的生成答案（注意这里不是检索，所以你会发现每次向GPT提问同样的问题，得到的答案都是不一样的）PS:关于为什么向GPT提同样的问题得到不同的答案这个问题，这里我简单的说一下，GPT是一个深度神经网络，里面有几百亿甚至上千亿的参数，为了得到更多的发散性，每次可能走的神经网络不会完全相同，最终的结果就不会完全相同，所以你有时候会看到GPT在一本正经的胡说八道，可能也正是因为他的这个特点，让GPT看起来更像一个人吧。二、大模型发展这么久，为什么到GPT3.5才具有了真正的智能？大家通过上面的阅读知道，GPT（Generative Pre-trained Transformer）生成式预训练语言模型。也就是这个语言模型是基于Transformer的，Transformer是一种基于注意力机制的神经网络模型，最早由谷歌公司提出，其最初目的是用于自然语言处理任务，如机器翻译、文本摘要、语音识别等。相比于传统的循环神经网络模型，如LSTM和GRU，Transformer模型具有更好的并行化能力和更短的训练时间，在处理长序列任务方面表现出色，因此在自然语言处理领域得到了广泛应用。其实GPT不是OpenAI公司的原创，而是由谷歌公司发明。是不是跟当年操作系统的图形用户界面其实是施乐公司最新发明的，却被乔布斯窃取到并应用到苹果的系统上一样。包括后来的iphone手机，大家也可以搜一下，其实所有的设计都是借鉴了其他公司的产品，但是乔布斯把他们组合并创新成了一件最伟大的艺术品，从而开启了一个全新的移动互联网时代，所以有时候并不一定什么都要原创，站在巨人的肩膀上来微创新，有时候更容易出成果。上面扯的有点远了，我们回到为什么GPT3.5才算真正的人工智能这个问题上。2018 年 OpenAI 采用 Transformer Decoder 结构在大规模语料上训练了 GPT1 模型，揭开了NLP模型预训练+微调的新范式。2019 年，OpenAI 提出了 GPT2，GPT2 拥有和 GPT1 一样的模型结构，但得益于更多和更高的数据质量以及新引入的多任务学习方式，语言生成能力得到大幅提升。之后由于 GPT 采用 Decoder 单向结构天然缺陷是无法感知上下文，Google 很快提出了 Encoder 结构的 Bert 模型可以感知上下文，效果上也明显有提升，同年 Google 采用Encoder-Decoder 结构，提出了 T5 模型，从此大规模预训练语言模型朝着三个不同方向发展。也就是说在GPT3.0之前，谷歌的Bert 模型是远超OpenAI 的GPT模型的。这里补充一个知识点，GPT3.0之前都是开源的，OpenAI由于一些商业等多方面的考虑，从GPT3.5开始，模型都是闭源的。直到2020 年 OpenAI 提出了 GPT3 将 GPT 模型提升到全新的高度，其训练参数达到了 1750 亿，训练语料超45TB，自此GPT系列模型的数据飞轮便转动起来，超大模型时代开启， NLP 任务走向了预训练+情境学习新路线。由于 GPT3 可以产生通顺的句子，但是准确性等问题一直存在，于是出现了InstructGPT、ChatGPT 等后续优化的工作，通过加入强化学习模式实现了模型可以理解人类指令的含义，会甄别高水准答案，质疑错误问题和拒绝不适当的请求等。从GPT3.5，GPT突然涌现出了“乌鸦”能力，之前的都可以理解成量变，一种鹦鹉学舌的能力，并没有真正的智能。可能是大力出奇迹，我感觉跟人脑是一个道理，一个神经元没啥智慧，一百万个、一百亿个可能也没啥智慧，不过增加到一千亿个神经元连接，突然就有智慧了，涌现出了能力。这是一件很玄学的事情，包括现在世界顶级的人工智能专业也无法解释这种现象，我们只能理解成大力出奇迹。这里拿出一点篇幅来普及一下什么“鹦鹉学舌”的假人工智障，什么是拥有“乌鸦”能力的真人工智能所谓鹦鹉学舌，就是东施效颦。没有GPT之前，几乎所有的自然语言处理都遵循着这一范式。他没有真的懂你的意思，只是一种模式匹配，比如之前的语音助手，只能识别有限的场景，比如你问他，帮我导航去天安门，他可以给你答案，但如果你让问他火星怎么去，他可能就回答不了你，因为他的数据库里没有这个问题的答案。也就是说，他只能回答在自己的数据库里有对应答案的问题，一旦你的问题超出了他的数据范围，他是没办法给你回复的。无法做到根据现有的数据生成新的数据，但是世界的问题千千万，不可能穷尽所有的可能把所有的问题答案都事先准备好，这也是之前的人工智能大家感觉并不智能的原因，因为他的底层实际上还是在做匹配。我举一个程序员都能理解的例子，比如你要实现一个不同条件得到不同结果的功能，我相信大部分程序员都是这样实现的。if($sex == '男' &amp;&amp; $age &lt; 18){echo \"小男孩\";}else if($sex == '女' &amp;&amp; $age &lt; 18){echo \"小女孩\";}else if($sex == '男' &amp;&amp; $age &gt;= 18 &amp;&amp; $age &lt;= 35){echo \"小伙子\";}else if($sex == '女' &amp;&amp; $age &gt;= 18 &amp;&amp; $age &lt;= 35){echo \"小姑娘\";}else{echo \"老年人\";}如果新增了条件，还是要新增一堆的if else才能匹配更多的情况。而乌鸦不一样，小时候我们读过乌鸦喝水的故事，乌鸦是有真正智慧的，他能真的读懂你要表达的意思。这里我们引用华人最厉害的AI学者之一朱松纯教授，在2017年写的一篇思考人工智能和智能本质的文章，通过这篇文章来理解乌鸦是如何感知、认知、推理、学习、执行的。乌鸦通过观察，自主串通了 汽车能压碎坚果 红绿灯能控制汽车 车能撞死我这三件事情，从而利用红绿灯和汽车，来帮自己达到“安全打开坚果”这一任务结果。如果类比成机器学习模型，过往“鹦鹉学舌”范式的解法，是要求所有乌鸦可以共享一个大脑，它们有很清晰的优化目标，即“保住性命的前提下打开坚果”。它们的方式是，随机尝试所有事件的组合，并向着最优解的方向不断演化。但现实世界的乌鸦无法共享大脑，也不能去冒着死亡风险去尝试所有可能。乌鸦只有一次机会，把观测到的两个现象，产生了一个新的可能性，并应用在一个全新的场景下。这里最接近的词汇可能是“inference”，是“基于证据和逻辑推演，得到结论”的过程，有的时候，还要加入很多猜测、抽象、泛化。举个例子，这篇文章把朱教授对于乌鸦的比喻，跟ChatGPT最本质的能力联系起来，就是在做inferencing这件事。但很明显，inferencing不是乌鸦智能的全部。而且在机器学习领域里，inferencing特指使用训练好的深度学习模型来预测新的数据这一件事，会产生误解。其他词汇也有类似问题，所以我们在自己文章里，会直接使用“乌鸦能力”来指代ChatGPT的新能力。在对外交流时，我们没办法每次都把乌鸦能力是什么解释一遍，所以我们会用“理解”能力来进行指代。从“乌鸦”到“理解”，当然是一个信息量损失很大的过度概括。但是好处是可以把ChatGPT的本质能力凸显出来。过往互联网的两次能力跃进一次来自于搜索，一次来自于推荐，现在ChatGPT带来了“理解”，也非常有结构感。本节最后，再给大家看一张图，让大家了解ChatGPT是如何一步步演化到目前的水平的通过上图，大家可以看到： GPT-3.5通过InstructGPT的模式 + 阅读代码，涌现了“乌鸦”能力，产生了质变。但是还没找到合适的应用界面，也不符合人类喜好 ChatGPT在RLHF的帮助下，找到了GPT-3.5和人类自然语言的合理接口，解锁了模型应用的前景（以上关于鹦鹉学舌和乌鸦能力的例子引用自”课代表立正的文章”）这里解释几个专用名词：InstructGPT：ChatGPT的交互模式，让GPT的能力，更加贴近人类真实交互方式。在in-context learning基础之上，进一步降低了prompting的门槛；一定程度解决了GPT-3生成结果与用户期望不一致的非预期输出，大幅降低了有害的、错误或偏差的输出结果，让GPT更符合人类胃口RLHFChatGPT背后的核心技术之一，让模型学习人类的偏好。全称是reinforcement learning from human feedback，通过构建人类反馈数据集，训练一个reward模型，模仿人类偏好对结果打分，是GPT-3后时代LLM越来越像人类对话的核心技术ChatGPTInstructGPT的亲戚，但一些优化方式也带来了ChatGPT的更泛化和准确能力，再次引爆了AIGC。ChatGPT总体来说和InstructGPT一样是使用RLHF进行训练，但模型是基于GPT3.5，而且数据设置上也不同。ChatGPT是一个输入，模型给出多个输出，然后人给结果排序，让模型可以学习人类的排序策略，即使是一本正经的胡说八道看起来也很合理的样子三、总结AI时代已来，面对每天海量的信息铺面而来，我想说，不要焦虑、不要担心自己会被替代，最好的方式就是保持一颗平常心，主动的拥抱AI，让AI成为你的个人助理，根据自身的情况，先从能马上提高自己工作生活效率的内容学起，躬身入局，日拱一卒，相信不久的将来，你一定会感谢今天的你的坚持！" }, { "title": "JVM详解", "url": "/posts/JVM/", "categories": "技术科普", "tags": "学习", "date": "2023-06-25 14:46:00 +0000", "snippet": " 本文主要讲述面试中常见的JVM考察题，可以帮助读者快速入门了解JVM的相关知识以及重点一、概述​\tJVM（Java Virtual Machine）是一种虚拟机，它是Java平台的核心组成部分。它是一个在物理计算机上模拟的虚拟计算机，可以解释和执行以Java语言编写的程序。JVM提供了一个平台独立的执行环境，使得Java程序可以在任何支持JVM的操作系统上运行，而不需要重新编译。JVM负...", "content": " 本文主要讲述面试中常见的JVM考察题，可以帮助读者快速入门了解JVM的相关知识以及重点一、概述​\tJVM（Java Virtual Machine）是一种虚拟机，它是Java平台的核心组成部分。它是一个在物理计算机上模拟的虚拟计算机，可以解释和执行以Java语言编写的程序。JVM提供了一个平台独立的执行环境，使得Java程序可以在任何支持JVM的操作系统上运行，而不需要重新编译。JVM负责将Java源代码编译成字节码，然后通过解释器或即时编译器将字节码转换为机器码并执行。JVM还提供了内存管理、垃圾回收、安全性和异常处理等功能，以确保Java程序的稳定性和安全性。总而言之，JVM充当了Java程序和底层硬件之间的抽象层，为Java应用程序提供了一个可移植和可扩展的运行环境。二、JVM内存JVM的内存模型有：程序计数器、虚拟机栈、本地方法栈、堆(运行时数据区)、方法区。 程序计数器（Program Counter Register）：每个线程都有自己的程序计数器，用于指示当前线程执行的字节码指令的行号，以便线程执行时能够回到正确的位置。 虚拟机栈（JVM Stack）：也称为 Java 方法栈，用于存储方法执行时的局部变量表、操作数栈、动态链接、方法出口等信息。每个线程在执行一个方法时，都会为该方法分配一个栈帧，并将该栈帧压入虚拟机栈，当方法执行完毕后，虚拟机会将其出栈。 本地方法栈（Native Method Stack）：与虚拟机栈类似，用于存储本地方法的执行信息。 堆（Heap）：用于存储对象实例，是 JVM 中最大的一块内存区域。堆是被所有线程共享的，当创建一个新对象时，对象实例存储在堆中，堆中存储的对象实例都有一个标记用于标记对象是否存活。垃圾回收器会周期性地回收那些没有被标记为存活的对象。 （新生代即老年代） 方法区（Method Area）：用于存储已经被虚拟机加载的类信息、常量、静态变量、即时编译器编译后的代码等数据。方法区也是被所有线程共享的。 (永久代)运行时常量池（Runtime Constant Pool）：是方法区的一部分，用于存储编译期间生成的各种字面量和符号引用，这些内容在类加载后进入常量池中 （字符串String也被存储其中）其中，程序计数器、虚拟机栈、本地方法栈是线程私有的，堆、方法区、运行时常量池是线程共享的。三、常见的垃圾回收机制常见的垃圾回收算法有以下几种类型： 标记-清除算法（Mark-Sweep）：分为标记和清除两个阶段。标记阶段遍历所有活动对象并打上标记，清除阶段将未被标记的对象删除。优点是不需要连续内存空间，缺点是清除后可能会产生内存碎片。 复制算法（Copying）：将可用内存分为两块，只使用其中一块，当这一块满了后，将存活对象复制到另一块未被使用的空间，然后清除使用的那块。优点是简单高效，没有内存碎片问题，缺点是需要额外的空间来存储复制后的对象。 标记-整理算法（Mark-Compact）：在标记阶段与标记-清除算法类似，但在清除阶段将存活对象整理到内存的一端，然后清除边界以外的所有对象。优点是不会产生内存碎片，缺点是比较慢。 分代收集算法（Generational）：根据对象存活的时间将内存分为几个区域，每个区域采用不同的回收策略。一般将新生代分为 Eden 区和两个 Survivor 区，采用复制算法回收；将老年代采用标记-清除或标记-整理算法回收。优点是提高了回收效率，缺点是需要额外的维护成本。这些算法各有优缺点，适用于不同的场景。标记-清除算法简单，但可能会产生内存碎片；复制算法适用于短时间内产生大量垃圾的场景，但需要额外的空间存储复制后的对象；标记-整理算法不会产生内存碎片，但比较慢；分代收集算法提高了回收效率，但需要额外的维护成本。对于一个应用程序，选择适合的垃圾回收算法需要综合考虑应用场景、内存需求、性能要求等多个因素，以便达到最佳的效果。四、JVM中什么情况下会进行垃圾回收？JVM中会在以下情况下进行垃圾回收： 空间分配失败：当JVM需要为新对象分配内存时，如果发现堆空间已经不足，就会触发垃圾回收。 调用System.gc()方法：虽然调用该方法不能保证立即进行垃圾回收，但是它能够提示JVM进行垃圾回收。 堆空间达到了设定的阈值：JVM会根据堆空间使用情况来动态调整垃圾回收的频率和方式。如果堆空间达到了设定的阈值，就会触发垃圾回收。 程序执行过程中：当程序执行过程中，JVM发现某些对象已经不再被引用，就会将这些对象标记为垃圾对象，并在垃圾回收时回收它们的内存空间。需要注意的是，JVM的垃圾回收机制是自动的，程序员无法直接控制垃圾回收的时机和方式。但是，程序员可以通过一些手段来优化程序的内存使用，如避免创建过多的临时对象、及时关闭不再使用的资源等，以减少垃圾回收的频率和影响。五、双亲委派机制双亲委派模式（Parent-Delegate Model）是 Java 类加载器（ClassLoader）在加载类时所采用的一种设计模式。这种模式的核心思想是：当一个类加载器收到类加载请求时，首先不会尝试自己加载这个类，而是将请求委派给其父类加载器。依次递归，直到最顶层的启动类加载器（Bootstrap ClassLoader）；如果父类加载器无法加载该类，子类加载器才尝试自己去加载。双亲委派模式的作用主要有以下几点： 避免类的重复加载：通过委派给父类加载器加载类，可以确保同一个类不会被多个类加载器重复加载。这有助于节省内存资源，并确保类之间的互操作性。 保护 Java 核心类库：由于双亲委派模式的存在，用户自定义的类加载器无法直接加载 Java 核心类库（如java.lang.Object等）。这有助于确保 Java 核心类库的安全性，防止恶意代码篡改或破坏Java核心类。 维护类加载器的层次结构：双亲委派模式使得各级类加载器可以按照一定的层次结构来组织和管理。这有助于降低类加载器的复杂性，简化类加载过程。双亲委派模式在 Java 类加载器中的应用是一种优秀的设计原则，它有助于确保类加载过程的稳定性、安全性和可维护性。然而，在某些特殊场景下（如 OSGi、Java 热加载等），双亲委派模式可能无法满足需求，需要采用其他类加载策略。在这些场景下，开发者需要充分了解类加载机制，以避免产生意外的问题。六、JVM真正的解析顺序大多的面经给出的都是这样的：但其实真正的解析顺序：从图中我们很容易看出: 加载与链接验证的顺序是混合执行的，并非先加载，后链接。之所以大家推崇先加载后链接的说法，是因为大家都觉得“只有加载了才能进行下一步”，因此自然而然的接受了错误的指31 链接中的解析 阶段，其实是不确定核实进行解析的。《Java虚拟机规范》之中并末规定解析阶段发生的具体时间，只要求了在执行ane-warray、checkcastgetfield、getstaticinstanceof、invokedynamic、invokeinterface、invoke-special、invokestatic、invokevirtual.ldc、ldc_w、Idc2_w、multianewarray、new、putield和putstatic这17个用于操作符号引用的字节码指令之前，先对它们所使用的符号引用进行解析。连Java虚拟机规范都未规定，是谁规定了“解析”发生在“准备”之后?为什么非要把“解析”放在准备”之后?难道单独把“解析”拿出来说不行吗?难道仅仅为了图形的完美程度? 链接-验证阶段的“符号引用验证”，一定是发生在“解析”之后的。我们一直在背诵:链接分为3步，验证-准备-解析，几乎都快成为了常识，可遗憾的是:链接-验证的第4种“符号引用验证确是发生在“解析”之后的。" }, { "title": "一个网络请求的全过程", "url": "/posts/request/", "categories": "技术科普", "tags": "学习", "date": "2023-06-18 07:19:00 +0000", "snippet": "一个网络请求的全过程一、引言​\t在浏览器上面输入一个URL后会发生什么？​\t我相信大多数人还是能够讲述出一两点关键的地方，你在搜索引擎上面搜索，能够搜索到七千多万条记录，因此本篇博客并不会以那种千篇一律的方式赘述，而是将自己类比成一个请求，切身感受到每个技术栈的具体细节，彻底从“根儿上”理解客户端请求-服务端响应的全过程。二、请求的全过程1、地址栏输入后本地会发生的事情核心：生成URL当我们...", "content": "一个网络请求的全过程一、引言​\t在浏览器上面输入一个URL后会发生什么？​\t我相信大多数人还是能够讲述出一两点关键的地方，你在搜索引擎上面搜索，能够搜索到七千多万条记录，因此本篇博客并不会以那种千篇一律的方式赘述，而是将自己类比成一个请求，切身感受到每个技术栈的具体细节，彻底从“根儿上”理解客户端请求-服务端响应的全过程。二、请求的全过程1、地址栏输入后本地会发生的事情核心：生成URL当我们在浏览器的地址栏中，输入 xxx 内容后，浏览器的进程首先会判断输入的内容： 如果是普通的字符，那浏览器会使用默认的搜索引擎去对于输入的 xxx 生成 URL 如若输入的是网址，那浏览器会拼接协议名形成完整的URL当然，在地址栏中输入某个内容后，也会进行一些额外操作，例如：安全检查、访问限制等，但总归而言，浏览器做的第一件工作则是生成URL，当按下回车后，浏览器会发送完成的URL进程发到网络进程：当网络进程收到传过来的URL后，首先并不会直接发出网络请求，而是会先查询本地缓存：观察上述流程，当网络进程收到传来的URL后，会首先通过URL作为Key，在本地缓存中进行查询： ①如果本地中是否有缓存： 没有：发起网络请求去服务器获取资源，成功后将结果渲染页面并写入缓存。 有：继续判断本地中的缓存内容是否已经过期，没有则直接使用本地缓存。 ②如果本地中的缓存已经过期，则会携带 If-Modified-Since、If-None-Match 等标识向服务器发起请求，先判断服务器中的资源是否更新过： 未更新：服务器返回304状态码，并继续读取之前的缓存内容使用。 ③如若服务器的资源更新过，那么也会向服务器发起请求获取资源。 如果在本地缓存中，无法命中缓存，或者本地缓存已过期并服务器资源更新过，那么此刻网络进程才会真正向目标网站发起网络请求。2、全新的“我”的诞生过程与前期的经历当客户端的网络进程，在查询缓存无果后，会真正开始发送网络请求，但要牢记：客户端的网络进程并非直接向目标网站发起请求的，前期还需经过一些细节处理。 当然，为了能够更直观的感受整个过程，在这里我们将自己“化身”为一个请求，站在请求的角度切身体验一段奇特的“网络旅途”。1.诞生前的准备-解析URL在网络进程发起请求之前，会首先对浏览器进程传过来的URL进行解析，一般来说完整的URL结构如下：但上述结构使用较少，通常情况下，浏览器会使用的URL的常用结构如下：URL中每个字段的释义： scheme：表示使用的协议类型，例如http、https、ftp、chrome等 domain：网站域名，经过DNS解析后会得到具体服务器的IP /path：请求路径，代表客户端请求的资源所在位置，不同层级目录之间用/区分 ?query1=value：请求参数，?后面表示请求的参数，采用K-V键值对形式 &amp;query2=value：多个请求参数，不同的参数之间用&amp;分割 #fragment：表示所定位资源的一个锚点，浏览器可根据这个锚点跳转对应的资源位置。网络进程会根据URL的结构对目标URL进行解析，其中有两个关键信息： 首先会解析得到协议名，例如http、https，这关乎到后续默认使用的端口号。 然后会解析得到域名，这个将关乎到后续具体请求的服务器地址2.我应该去哪里-DNS域名解析在上个阶段已经大概知道“我”该去往何处啦！但我具体地址该到那里呢？“我”好像不大清楚，要不找个人问问吧。\t我记得好像有个叫做DNS的“大家族”是专门负责这个的！我要去找它们问问看~不过在问DNS之前，我先来看看本地有没有域名与IP的映射缓存，好像没有~，那我只能去找DNS了，我首先找到了「本地DNS大叔」，把我要查找的域名交给了它，它让我稍等片刻，它给我找一下，让我们一起来看看「本地DNS大叔」是怎么查找的： ①首先「本地DNS大叔」找了它的「根DNS族长」，族长告诉它应该去找「顶级DNS长老」。 ②「本地DNS大叔」根据族长的示意去找了「顶级DNS长老」，然而长老又告诉它应该去找「授权DNS执事」。 ③「本地DNS大叔」又根据长老的示意找到了「授权DNS执事」，最终在「授权DNS执事」那里查到了我手里域名对应着的具体IP地址。 ④「本地DNS大叔」拿着从「授权DNS执事」那里查到的IP，最终把它交给了我，为了下次不麻烦大叔，所以我获取了IP后，将其缓存在了本地。呼~，我终于知道我该去哪儿啦！准备出发咯！3.确保路途安全-TCP与TLS握手问过DNS大叔后，获得了目的地址的我，此时已经知道该去往何处啦！但在正式出发前，由于前路坎坷，途中会存在各类危机（网络阻塞、网络延迟、第三方劫持等），因此为了我的安全出行，首先还需为我建立一条安全的通道，所以我还需要等一会儿才能出发，俺们一起来瞅瞅建立安全通道的过程是什么样的：看着挺复杂的，但实际上大体分为了两个过程： 首先是TCP的三次握手过程，听说这个阶段是为了确保目的地能够正常接收我、也是为了给我建立出一条可靠的出行通道、并且为我计算一下出行失败之后多久重新出发的时间等目的（也就是为了测试双方是否能正常通信、建立可靠连接以及预测超时时间等）其实按照之前的“交通规则”，在建立好TCP连接之后，我就可以继续走下一步啦，但现在有很多坏人，在我们出行的道路上劫持我们，然后窃取、篡改俺们携带的数据，所以如今出行变得很不安全，因此还需要还需要建立一条安全的出行通道，就是TLS大叔的安全连接~（HTTP+TLS=HTTPS）： TLS握手阶段，在这个阶段中，TLS大叔为了俺的安全出行，会通过很多手段：非对称加密、对称加密、第三方授权等，先和俺的目的地交换一个密钥，然后再通过这个密钥对我加密一下，确保我被坏人抓到了也无法得到俺护送的数据！4.诞生身体-构建请求报文经历上述过程后，安全的出行道路已经建立好啦！但此刻的我还不算完整，所以需要先构建一个“身体”，也就是HTTP请求报文：“我的身体”主要由请求行、请求头、空行以及请求主体四部分组成，里面包含了“我本次出远门的需要护送的数据和一些其他信息”。同时，为了我能够在“出行的道路上（传输介质）”安全且正常传输，我还需要经过层层封装：首先为了确保俺护送的数据安全，TLS大叔会先对我的数据进行一次加密，把我原本携带的明文数据转变为看都看不懂的密文，类似下面这个样子：经过加密后的我会紧接着来到传输层，传输层会在我的脑袋上再贴上一个传输头，如果是TCP大哥的话，它会给我贴上一个TCP头，但如果传输层的UDP大哥在的话，它给我贴的就是UDP头。但不管是谁贴的，在这个传输头内，为了防止我迷路和走丢，TCP、UDP两位大哥哥都会细心的在里面写清楚“我来自哪里，该去往何处”，也就是源地址和目的地址： 偷偷吐槽一句：TCP大哥贴的传输头里面，放了好多好多东西，让我感觉脑袋沉沉的过了传输层这一站之后，我又来到了网络层，果不其然，网络层里面最常见的还是IP大叔，IP大叔看到我之后，又在我的脑袋上贴上了一个网络头，也就是给我又加了一个IP头。哒哒哒~，我出了网络层这关之后，又来到了数据链路层，这关则是由大名鼎鼎的“以太网家族”驻守，在这里我和之前两关不同，除开在我脑袋上贴了一个链路头之外，还给我在尾巴上多加了一个链路尾。 不过刚刚出链路层的时候，好像有个人跟我说：你这个样子是无法在介质上行走的，你要记得改变一下啊！我还没听的太清楚，就来到了物理层这关，这层和之前我“家里”以及之前的关卡环境都不一样，物理层的小伙伴们好像都有实际的形态，但之前接触所有内容都是虚拟的概念形态哎~。在我对比物理层大哥们的异样差距时，一不愣神发现我的身体好像发生了“翻天覆地”的变化，整个我似乎都变为了0、1构成了，正当纳闷时，物理层的某个大哥哥告诉我说：“只有变成这样子，你才可以在出行的道路上行走哈，所以我们给你转换了一下形态，你现在已经可以出发了”。5.踏上路程-数据传输终于踏上网络之旅了！咔！我来到了第一个中转站，听别人说，好像它的名字叫做路由器，首先路由器大哥把我的身体按照之前封装的步骤层层解封了，但解封到传输层的时候，看到了我脑袋上的传输头，似乎路由器大哥发现了TCP哥哥写的目的地址，发现我的目的地还在更远的位置，然后路由器大哥又按照原本的步骤把我的身体封装回去了，然后还亲切的给我指出了接下来该往那条路走，我又该继续前行啦…. 我一边走着，一边在思考：好像路由器大哥就是负责给俺们指路的，防止俺们走丢~3、在后端服务器中多姿多彩的历程啊！路途好遥远呀，我一路走了很久很久，也遇到了很多很多的中转站，每次当我不知道怎么走时，路由器大哥都会温馨的给我指出接下来该走的路途。期间我也走过很多很多路，曾踩着双绞铜线、同轴电缆、光纤前行，当然，可不要小看俺，就算没有物理连接的情况下，我也可以通过无线电技术，通过空气前行呢！ 再次声明，文中所谓的道路，就是指数据传输的介质1.东跑西颠-接入层转发走着走着，突然前方遇到一个叫做CDN的老爷爷，它问我说要去哪里，我说要去XXX地方办事，和蔼的CDN老爷爷跟我说，我来看看我这里有没有你要的东西，如果有的话，就不用麻烦你这个小家伙一直跑下去了。可是很遗憾，老爷爷说它哪儿没有我要的东西，因此我只能继续前行下去。记不清过了多久，一路跌跌撞撞，在迷迷糊糊中我来到了一个地方，但当我还在分辨时，刷的一下，很快啊，我就被丢到了其他地方，当我回头看的时候，发现刚刚哪个地方，大写着LVS。 LVS一般会作为大型网站的网关接入层，负责提供更高的并发性能再直视前方，前方有一个东西很眼熟，难道这就是当初听说过的服务器吗？带着一脸疑惑的我慢慢走了进去，我发现内部空间很大，上面漂浮着一块大陆，名为Linux大陆，上面有好多好多的“城市（进程）”林立着，那我该去哪一座呢？让我想想！ 对了，记起来了好像！！当时出门的时候有人跟我说过：如果你到了目的地之后，不知道该找谁，那么可以根据默认的编号（端口号）去找！HTTP的默认端口是80，HTTPS的默认端口是443，我目前属于HTTPS派别的请求，那么我应该去找编号为443的城市！顺着我的推理，我来到了编号443城市的城门口，当我迈进城门后，嗖的一下，我被一个叫做Nginx的大叔抓了过去…. Nginx：小家伙，你是来干嘛的？ 我：我带了一些数据过来找地址为IP:443的地方办事！ Nginx：噢~，原来是这样啊，我就是负责监听443编号的守门将。 Nginx：小家伙，你过来让我看看….话音刚落，Nginx三下五除二的就把我的身体拆开了，然后得到了HTTP报文，然后从HTTP报文的请求行中，发现了我本次旅途的具体目标：/user/posts，然后Nginx大叔又把我组装了回去，然后根据它内部配置的规则，然后道： Nginx：小家伙，我刚才看了一下，你应该要去的具体位置是xxx.xxx.xxx.xxx:xx，快去吧。 我：你怎么知道我要去的是这里？ Nginx：我刚刚看了一下，你要去的具体位置为IP:443/user/….，根据目前的规则以及我代理的地址，你就应该去这里！ 我：大叔大叔，给我看看你代理了那些地址呗。 Nginx：你可以过来看看。 我：哇，为什么这么多！我可不可以去找其他的地址，找其他人帮我办事呀？ Nginx：不可以噢！按照规则的话，你就应该去我给你的地址哈。 我：好吧，那我去啦！ 这里的规则是什么呢？其实就是Nginx的location路由匹配规则、upstream代理集群列表以及负载均衡算法顺着Nginx大叔给的地址，我又来到了另外一台服务器，上面同样有一块Linux大陆，然后根据地址在上面找到了一个名为Gateway的东东，听它自己介绍，好像属于系统网关。但当我找它办事时，它却跟我说：“我不负责具体的业务处理，根据你的目标/user/….，你应该去找Nacos注册局，问它们要一下USER-SERVICE的具体地址，所以，小家伙你还得继续奔波哦”！ 好的好的，感谢Gateway叔叔指路，那我现在就去啦！哒哒哒~，迈着愉快的步伐我来到了Nacos注册局，然后将Gateway叔叔给我的名字：USER-SERVICE交给了它们的工作人员，它们的工作人员经过一番查询之后告诉我，这个“品牌”多有个分部，你可以去其中任意一处分部处理你的任务，你可以去：xxx.xxx.xxx.xxx:8080这个地址噢！ 这里的“品牌”是指后端的具体服务，分部是指服务集群中的每个节点好的好的，那我就去你说的这个xxx.xxx.xxx.xxx:8080地址啦！我一边在路上走着，一边想了一下刚刚过程发生的事情，然后把这个经历画成了一副逻辑图，如下：回去的时候我一定要跟小伙伴们分享一下这个有趣的经历，耶！2.遇到了一只大猫咪-tomcat根据Nacos给我的地址，我又来到了一台新的服务器面前，我记得Nacos给了我一个端口号，要我来到这里之后找编号为8080的位置，我顺着这个编号慢慢找着，突然在我的前方，出现了一只大老虎，哦不，应该是一只大猫咪，它长这个样子：它的长相似乎有些报看，但在它的脑门上正好写着我要找到8080地址，那我要找的应该就是它了吧！终于到了！我慢慢靠近了这只大猫咪，然后跟它说要找它办事，Tomcat说要看看我的数据，然后又把我的身体按照之前封装的方式逆向拆开了，从而还原了我最初的身体-HTTP请求报文，最后Tomcat说：“我确实是你本次要找的最终目标，不过要办你这件事情得到我肚子里面去噢”！ 说罢，Tomcat张开了它的血盆大口，一口将我吞了下去…..，正当我以为我完蛋的时候，我却发现Tomcat内部别有乾坤，上面似乎也有一块小陆地漂浮着，当我凑近的时候才看清楚，原来上面写的是JVM呀！我二话不说，一脚踏上了这块陆地，正当我看着上面密密麻麻的“屋子（Java方法）”迷茫时，此时我正前方就走来了一个人，然后对我做了一个自我介绍： 来自远方的尊敬客人，您好呀，欢迎光临JVM神州，我叫Thread-xxx，是线程家族的一员，您接下来的整个旅途，我终将陪伴在您左右，您需要办的所有事情，都会由我代劳，客官这边请（45度鞠身）~然后我一边走着，一边跟Thread-xxx聊着： 我：为什么是你来接我呀？ 线程：因为每位从远方到来的客人，我们线程家族都会派遣一位子弟迎接。 线程：本次轮到我了，因而由我为您本次的旅途提供服务。 我：噢噢噢，那我们接下来该去哪儿呢？ 线程：这需要看客官您本次的目的啦！可以让我看看您本次的旅程吗？ 我：可以呀，看吧，[我将请求请求行中的资源地址摆了出来]。 线程：/user/….，原来您是要去这里呀，这边请~。 线程：我们首先要去找DispatcherServlet办事处，才能继续前行。 PS：接下来是讲述Java-SpringMVC框架的执行过程，非Java开发可忽略细节随着Thread-xxx的步伐，我们找到了线程口中所说的DispatcherServlet办事处，该办事处的工作人员首先看了一下我本次的具体目的地（资源地址），然后说：您需要先去问一下HandlerMapping管理局，让它给你找一下具体负责这块业务的工作室。 紧接着线程Thread-xxx又带我来到了HandlerMapping管理局找到了其中的管理人员，该管理人员让我先把要找的资源位置给它，然后只见它拿着我的目标地址作为条件，然后输入进了查询器，一瞬间便查出来了我本次的最终目的地：UserController工作室！线程Thread-xxx道：这就是负责您本次任务的最终工作室啦！我这就带您过去 这其实本质上就是SpringMVC中，请求定位具体Java方法的逻辑 话接前文，前面经过HandlerMapping管理局的管理人员查询后，我们已经找到了本次任务处理的具体工作室了… 线程：客官，咱们到了！ 线程：这个工作室中已经写明了您本次任务如何处理的具体步骤，接下的事情都将由我为您效劳。 线程：您要随我一起去看看具体的处理过程嘛？ 我：好呀，好呀，一起去！随着线程的工作开始，我们一路走过了service层、dao/mapper层，在service层办事时，我们遇到了强大的Redis哥哥，Redis哥哥看到我们之后，问清楚了我们本次到来的目的，然后它说：“来自远方的贵客，请稍等，让我先看看我这里有没有您需要的东西！” 这个场景似曾相识哎，我记得来的路上也有个CDN老爷爷跟我说过同样的话~ Redis：来自远方的客人，很抱歉我这里没有您要的东西。 Redis：您本次的路途还需继续前行，您可以去找一下MyBatis哪小子，它也许能够帮到您。根据Redis的指示，线程Thread-xxx领着我最终见到了MyBatis，它长这个样子：MyBatis简单看了一下我本次的任务： 鸟叔：来自远方的贵客，这件事我确实可以帮到您，请稍等。 然后“鸟叔”一顿操作，竟鼓捣出了一个我看不懂的东西，然后递给了我。 鸟叔：这个叫做SQL代码，是你您次任务的必须之物。 鸟叔：你现在可以拿着它，让Thread-xxx去带您找一下JDBC哪个老家伙。慢慢的，线程又带我找到了“鸟叔”口中所说的JDBC老爷爷，JDBC老爷爷见到我的到来，眼神中并没有丝毫的意外之情，似乎早已经习以为然，只见JDBC老爷爷抬起消瘦的右手，指着一个地址： jdbc:mysql://xxx.xxx.xxx.xxx:3306/db_xxxxx然后道：“小家伙，你又需要再跑一段远路咯，而且只能你去，Thread-xxx只能在这里等你”。 我：好吧好吧，那我去啦！又是孤身一人的旅途，难免有些孤独感袭来，但还好我早已习惯啦！随着一路奔波，我来到了JDBC老爷爷给出的地址，这里同样是位于另外一台服务器的Linux大陆上，我通过3306这个编号找到了一座叫做MySQL的城池，当我踏入之后发现，与之前踏上JVM神州相同，在我刚踏入MySQL这座大城的时候，有一个自称为DB连接家族的弟子接待了我。 DB连接：您好呀，是JVM神州上那位JDBC前辈介绍过来办事的，对吗？ 我：对对对，是的，是的。 DB连接：好的，那请把您手中的SQL给我噢。 DB连接：那是您本次需要做的任务清单，麻烦交给我一下，由我帮你代劳。 我：昂，那给给你啦[递过去]~ DB连接：好的，这边有冰阔乐和西瓜，请您稍等片刻，我去去便回。 这里便不再讲诉DB中的具体流程了正当我吃完一块西瓜、喝完一瓶冰阔乐时，DB连接家族的哪位弟子便回来了，同时怀里抱着一大堆东西（数据），然后丢给了我，道：“这便是您本次需要的数据啦，您本次的任务我都按照清单（SQL）上的记录，给您一一处理了噢”。我：好的，万分感谢，那我走啦！顺着来时的原路，我飞速的赶回了JVM神州所在的位置，然后映入眼帘的第一眼就是：Thread-xxx哪个家伙在原地站着，老老实实的等候着我的回归，我悄悄的绕到了Thread-xxx身后，然后从背后拍了一巴掌： 我：嘿，我回来啦！等了我这么久，有没有想我~、 线程：并未，我是在履行线程家族该有的职责。 我：额….，无趣。 我：我事情已经办好了，我要走了噢。 线程：好的，那由我来送您。一路跟随着Thread-xxx的脚步，兜兜转转的我们最终又回到了DispatcherServlet办事处，经过它们内部人员的一顿操作之后，我就打算返航啦！一路走走停停，我走到了JVM神州的边缘。 线程：远方的客人，我只能送您到这里啦。 我：就要说再见了吗？ 线程：是的，按照我们Java线程家族的规则，正常情况下我是不能踏出JVM神州的。 我：好吧好吧，那就再见啦，Thread-xxx~，我会记得你的。 线程：好的，那祝您归途一路顺风，期待您的下次光临！再见啦！ 我：拜拜[挥手]~我告别了Thread-xxx，也从此离开了JVM神州，最终我从Tomcat这只大猫咪的口中飞了出来，正式踏上了归途。4、大功告成原路返回诸多经历过后，现在的我携带着本次任务的结果踏上了回家之路，首先我又路过了Gateway叔叔那里，然后我又回到了Nginx大叔所在的城池，不过Nginx大叔把我的身体改为了应答报文结构，并且往其中还写入了一些东西，听说是让我回去交给浏览器老大的。 然而在我返航之前，似乎这边也有加密层、传输层、网络层、链路层、物理层这些关卡，和我当时出发的过程一样，我身上被一层一层的贴了很多东西，并且最终也被改为了0、1组成的身体结构，这个过程是多么的熟悉呐！ 我又踏上了哪不知有多遥远的路途，与来时的路一样，其中也遇到了很多中转站，也走过各种各样的道路，当然，为了防止我迷路，在Nginx大叔那里，也在我的脑袋上贴了一个TCP头，里面写清楚了我来自那里，该去向何方…..在迷迷糊糊中不断前行，终于看到了我的出生地，看到了网络进程和浏览器老大~，哦豁！我回来啦！在进入家门之前，我又会经历物理层、链路层、网络层、传输层、TLS层依次解封的过程，主要是为了将我从后端带回来的数据解析出来。网络进程在解析到数据后，我的使命就此完成啦！紧接着网络进程会将数据交给浏览器老大，然后老大会派遣一个小弟（渲染进程）对数据进行处理，我瞅了几眼，大体过程是这样的：首先渲染小弟会根据HTML、CSS数据生成DOM结构树和CSS规则树。然后结合结构树和规则树生成渲染树，再根据渲染树计算每一个节点的布局。最后根据计算好的布局绘制页面，绘制完成后通知另一个小弟（呈现器）显示内容。最后，因为我至此已经正常返航了，所以为了节省资源开销，会将我出发前构建的安全通道（TCP、TLS连接）关闭，这个过程会由TCP大哥去经过四次挥手完成，如下：三、总结综上所述，用户在浏览器地址栏输入内容后，我们站在一个“网络请求”的角度，切身感受了一场奇妙的网络之旅，从客户端发送请求到服务端返回响应，整个流程咱们都“亲身”体验了一回，最后写个流程总结：​\t地址栏输入生成url – 查询本地缓存是否有该请求的资源 – 否则发送网络请求(浏览器解析url 和 DNS域名解析获取目标地址) – 建立安全传输通道(TCP与TLS握手) – 构建请求报文与层层封装(每一层都为其添加信息) – 经过各种中转站(路由器，解封获取目标信息再封装) – 到达CDN(判断是否拥有数据，否则继续寻找服务器) – 到达Nginx服务器(反向代理) – 到达目标服务器(gateway、nacos…) – 到达对应的服务端口8080：tomcat – 到达JVM(分配线程进行执行) – DispatcherServlet、HandlerMapping、controller(根据使用的对应框架进行变动) – service、dao层(还涉及redis、mq等其他中间件技术) – 到达mysql(分配新的工作线程)获取数据 – 之后再按之前的路程返回到达浏览器(TCP四次挥手) – 浏览器渲染数据 本文以以ava技术栈为例，具体情况还是需要具体分析" }, { "title": "研发工程师的职业规划", "url": "/posts/Plan/", "categories": "随笔", "tags": "职业规划", "date": "2023-06-11 13:01:00 +0000", "snippet": " 现在已经入职差不多一个月，也算的上是一个初级研发工程师，但还是会感到焦虑与烦躁，会开始思考自己的职业规划。人总是这样，害怕不确定性，在校时期就会焦虑实习的事情，工作之后就会焦虑职业规划的事情，等哪天达到目标之后，就又会焦虑其他种种…​\t不管那一个行业，想走的更远，都要考虑到未来职业发展，比如研发来举例子: 假如你目前处在初级研发工程师阶段，下一步你的目标，就是要成为中高级研发工程师，最终...", "content": " 现在已经入职差不多一个月，也算的上是一个初级研发工程师，但还是会感到焦虑与烦躁，会开始思考自己的职业规划。人总是这样，害怕不确定性，在校时期就会焦虑实习的事情，工作之后就会焦虑职业规划的事情，等哪天达到目标之后，就又会焦虑其他种种…​\t不管那一个行业，想走的更远，都要考虑到未来职业发展，比如研发来举例子: 假如你目前处在初级研发工程师阶段，下一步你的目标，就是要成为中高级研发工程师，最终要成为一名研发架构师。​\t但是在进一步地追问后，大多数研发同学对自身技术发展的认知，仅停留在学习了哪种新的技术掌握了哪种新的开发框架，觉得这样就能把技术做好，就能成为架构师。​\t可是现实情况是:你觉得技术满足应聘部门的要求，可还是面不到想要的职位。这其实与技术认知不足有很大关系，你达不到一个高级研发或者是架构师该有的思维层次，在面试时，自然很难讲出自己的技术价值与亮点，就会影响面试竞争力。​\t而今天的分享，会从架构设计认知、分析问题的认知、能力边界认知三个角度出发，讲解研发工程师如何提高自己的技术认知，在面试的过程中更加体现价值和竞争力，进而获得满意的 Ofer。对架构设计的认知​\t面试官通常会在考察完候选人基础技术能力之后，再问一些关于系统架构设计上的问题，这时如果你回答得比较好，很容易得到面试官的认可，也会掩盖个别技术问题上回答的不足。​\t但实际上，很多研发同学对架构设计的掌握和理解是欠缺经验的，系统设计问题只能回答出表层的技术名词，落地没有实际经验，拔高没有理论支撑。那你怎么回答面试中的架构设计问题呢?​\t关于架构设计的问题，一定要立足于点、连接成线、扩散成面，用这样的思路回答才能让面试官满意。下面就通过一个例子，来帮你理解什么是回答架构设计问题该有的认知。例子​\t你是一名研发工程师，去参加面试，在介绍过往经历时，自己最近在重构一个负责交易流程的系统时，将其拆分成报价系统、促销系统，以及订单系统，而当时你们只有两个人负责交易系统的开发工作。​\t针对你的经历，面试官问题是：你们只有两个人负责这个交易系统，为什么还要做系统架构拆分？而且拆分之后会带来其他的复杂度，你是怎么考虑的？系统拆分的架构设计问题，在面试中很常见，你可能有所准备，针对第一个问题给出了四个层面的回答。【1】从订单系统层面来看，由于交易流程中的订单系统相对来说业务稳定，不存在很多的迭代需求，如果耦合到整个交易系统中，在其他功能发布上线的时候会影响订单系统，比如订单中心的稳定性。基于这样的考虑，需要拆分出一个独立的子系统。【2】从促销系统层面来看，由于促销系统是交易流程中的非核心系统，出于保障交易流程稳定性的考虑，将促销系统单独拆分出来，在发生异常的时候能让促销系统具有可降级的能力。【3】从报价系统层面来看，报价是业务交易流程中最为复杂和灵活的系统，出于专业化和快速迭代的考虑，拆分出一个独立的报价系统，目的就是为了快速响需求的变化。【4】从复杂度评估层面来看，系统拆分虽然会导致系统交互更加复杂，但在规范了 API 的格式定义和调用方式后，系统的复杂度可以维持在可控的范围内。​\t这样的回答很好地表达了应聘者对系统设计的思考与理解。因为你说出了原有系统中关于订单、促销和报价功能耦合在一起带来的实际问题，这是立足于点，又从交易流程的角度做系统设计串联起三个系统的拆分逻辑，这是连接成线，最后从复杂度和成本考量的方向夯实了设计的原则，这是扩展成面。案例分析​\t如果你是这名应聘者，会怎么回答呢？很多研发同学一提到架构设计就说要做拆分，将一个系统拆分成两个系统，将一个服务拆分成两个服务，甚至觉得架构就是做系统拆分，但其实并不理解拆分背后的深层原因，所以往往只能回答得比较表面，无法深入背后的底层设计逻辑，那这个问题的底层逻辑到底是什么呢？有这样四点。●为什么做架构拆分？通常最直接目的就是做系统之间解耦、子系统之间解耦，或模块之间的解耦。●为什么要做系统解耦？系统解耦后，使得原本错综复杂的调用逻辑能有序地分布到各个独立的系统中，从而使得拆封后的各个系统职责更单一，功能更为内聚。●为什么要做职责单一？因为职责单一的系统功能逻辑的迭代速度会更快，会提高研发团队响应业务需求的速度，也就是提高了团队的开发效率。●为什么要关注开发效率？研发迭代效率的提升是任何一家公司在业务发展期间都最为关注的问题，所以从某种程度上看，架构拆分是系统提效最直接的手段。所以，架构拆分其实是管理在技术上提效的一种手段，认识到这一点后，就不难理解为什么很多架构师在做系统架构时，会做系统设计上的拆分，甚至认为架构的本质就是拆分了。对分析问题的认知​\t在实际工作中，技术人员在做系统设计时需要与公司或部门的战略定位对齐，才能让你的技术有价值。因为对于系统技术架构升级的问题，业务方、管理者和技术人员的关注点是不同的。●业务方的诉求是在技术升级后，系统有能力迭代功能来满足市场的要求，所以关注点在系统能力。●管理者的诉求是在技术升级后，系统研发团队的开发效能得到提升，所以关注点在人效管理。●作为技术人员的你，需要找到自己做系统设计的立足点，来满足不同人对技术的诉求，而这个立足点通常就是系统设计原则。所以你应该认识到，系统的设计原则不是乱提出来的，而是针对系统现阶段业务发展带来的主要矛盾提出，才会更有价值且被认可。例子​\t之前在前东家做过一个对原有老系统进行架构改造的系统设计，当时的背景是这样的。​\t早期，业务发展比较简单，团队规模也不是很大，单体系统可以支撑业务的早期规模，但当业务不断发展，团队规模越来越大时，之前的一个业务团队逐渐发展成了多个业务团队，这时每个业务团队都会提出自己的功能需求。​\t然而，系统现状仍然是单体架构，研发同学都在同一个系统里进行开发，使得系统逻辑复杂，代码耦合，功能迭代和交付变得非常缓慢，牵一发而动全身，研发同学都不敢轻易修改代码。​\t这个时期系统的主要矛盾就变成了：多人协作进行复杂业务，导致速度缓慢，但业务需求又快速迭代。说白了，就是研发效率不能匹配业务发展的速度，并且单靠加人不能解决问题。对于这样的一个系统，此阶段的系统架构核心原则就不能随便定义为要保证高性能和高可用。​\t那么应该怎么做呢？针对这样的问题，我们需要对原有系统进行合理的系统边界拆分，让研发人员有能力提速，来快速响应需求变化，这就要求架构师对业务领域和团队人员有足够的了解。​\t类似这样的情况也是面试中经常出现的考题，比如面试官在问你历史项目经历的时候，要重点关注你是如何解决系统核心问题的，所以不要一张口就是高性能、高可用，这会让有经验的面试官觉得你很初级。案例分析​\t面试中，研发人员在回答系统设计问题的时候，要根据系统所处阶段的主要矛盾来回答架构设计问题，在 20 世纪 60 年代，《人月神话》的作者就分析，软件复杂性来源于两点：本质复杂度和偶然复杂度。开发工具、开发框架、开发模式，以及高性能和高可用这些仅是偶然复杂性，架构最重要的是要解决本质复杂性，这包括人的复杂性和业务的复杂性。​\t技术是静态的，业务和用户是变化的，具体问题要从具体的业务领域出发。这时有人可能会说，我只想做技术，不想做业务，然而你会慢慢发现，在职业生涯中处理的最有价值的事情，一般都是利用技术解决了业务领域的某阶段的主要问题，这也是最复杂的。而一个优秀的应聘者，在回答中应该向面试官展现出这样的技术认知。对能力边界的认知​\t平时在公司搬砖学习的时候，也会看一下研发晋升评审资料，有很多人会问相同的一个问题：你觉得一个高级研发工程师和一个架构师的区别在哪？ 这个问题很多研发同学回答得都不是很好，有些人说需要足够的技术经验，懂得高性能、高可用，也有些人说需要懂得管理，带过团队。​\t这些能力固然重要，但不是作为架构师最核心的能力。下面我通过一个例子，来帮你理解一个高级研发工程师和一个架构师的本质区别在哪儿。例子我们先来看一下互联网一些大厂的中高级研发工程师晋升架构师的标准，如下图所示：可以看出，晋升架构师需要掌握架构知识体系以及互联网的设计经验。​\t那么是不是可以这么理解：想要成为架构师，需要在掌握原有技术框架原理与开发基础之上，再懂得分布式高性能、高可用的设计知识，这样就可以了？如果你真是这么认为的，那就存在一个技术认知的问题。​\t可以这样思考，一个中级或高级研发工程师就不需要懂高性能、高可用的设计手段了吗？这些在网上应该也不难找到通用的解决方案，那么他就可以成为架构师了吗？​\t其实不然，掌握互联网架构设计中的高性能、高可用、高扩展这些非功能性设计方案是基础，但还要看你是站在哪个角色上考虑的。互联网大厂职级体系晋升的一个很重要规则，就是你所做的事情的边界，所能影响到的范围。比如，研发工程师和架构师能驾驭的边界可以如下概括：●一个中高级研发工程师对系统的驾驭边界至少是模块或者子系统层面；●一个架构师对系统的驾驭边界至少是全系统层面；●一个高级架构师对系统的驾驭边界至少是某一领域层面。案例分析​\t我们常说，屁股决定脑袋，不在那个位置就不会真正体会到那个位置带来的问题。没有触达多系统层面的设计，就不会掌握多系统层面带来的复杂度和解决问题的思考逻辑。但是往往研发同学意识不到这样的问题存在，即便能碰到一个通盘考虑架构设计的机会，但价值、眼界、认知的形成，也不是一朝一夕的事儿。​\t那么，你要怎么做才能让自己更快速地成长呢？你要在工作中养成归纳总结的习惯，形成自己的知识体系，沉淀自己的方法论，提高自己的认知能力，并且跳出舒适区，多争取扩展自己能驾驭系统的边界的机会。总结今天这篇分享，给大家，讲解了研发工程师在面试中如何提高竞争力，可以总结为三点。●首先要提高你对系统架构设计的认知能力，一个好的架构师的架构设计不是仅仅停留在技术解决方案上。●其次要提高你分析系统问题的认知能力，做架构设计要具备根据现阶段的主要矛盾来分析问题的能力。●最后你要扩大自己能够驾驭系统的边界，因为只有这样才能遇到之前没经历过的问题层次，注意我这里说的是问题层次，而不是问题数量。 日常在技术社区中闲逛，看到一位p9大佬编写的关于职业规划的文章，阅读之后也是受益匪浅，以下便转载到笔者博客之中" }, { "title": "使用@Autowired为什么会被IDEA警告？", "url": "/posts/Autowired/", "categories": "技术科普", "tags": "学习", "date": "2023-06-04 15:19:00 +0000", "snippet": "一、导致警告的原因1、初始化问题Java初始化类的顺序： 父类的静态字段 &gt; 父类静态代码块 &gt; 子类静态字段 &gt; 子类静态代码块 &gt; 父类成员变量 &gt; 父类构造代码块 &gt; 父类构造器 &gt; 子类成员变量 &gt; 子类构造代码块 &gt; 子类构造器而Autowired注入，则要排队到子类构造器以后了，SpringIOC并不会对依赖的bean是否为...", "content": "一、导致警告的原因1、初始化问题Java初始化类的顺序： 父类的静态字段 &gt; 父类静态代码块 &gt; 子类静态字段 &gt; 子类静态代码块 &gt; 父类成员变量 &gt; 父类构造代码块 &gt; 父类构造器 &gt; 子类成员变量 &gt; 子类构造代码块 &gt; 子类构造器而Autowired注入，则要排队到子类构造器以后了，SpringIOC并不会对依赖的bean是否为null做判断，JVM编译时同样也不会有问题，但如果使用不当，运行起来时或许会因为出现空指针异常。2、对IOC容易依赖过强@Autowired由Spring提供，而@Resource是JSR-250提供的，它是Java标准。前者会警告，而后者不警告，就是因为前者导致了应用与框架的强绑定，若是换成其他IOC框架，则不能够成功注入了。其实对于这方面，我认为在大多数情况时是不会有什么问题的。3、其他方面我看到网络上有一些其他方面的总结，比如：依赖过多却不够明显，违反了单一职责原则；不能像构造器那样注入不可变的对象等，这类问题需要结合个人实际开发进行判断。对于@Autowired使用方面，它虽然是将业务代码和框架进行了强绑定，但字段注入确实大幅简化了代码。追求完完全全的松耦合其实也过于理想化，应该在实际使用中追求平衡，否则将为了过度追求松耦合而得不偿失。二、解决方式1、@Resource注解除了使用@Autowired以外，我们其实也有几种好用的方式。使用@Resource替代@Autiwired方法是其中一种，只需要改变一个注解，这里就不展示了。2、set方法@RestControllerpublic class TestController2 { ITestService testService; /* * 基于set注入 * */ @Autowired public void setTestService(ITestService iTestService) { this.testService = iTestService; } @GetMapping(\"/status2\") public Result&lt;?&gt; status() { return testService.status(); }}这种方法也使用了@Autowired注解，但是它是作用于成员变量的Setter函数上，而不是像Fied注入一样作用于成员变量上。3、构造器@RestControllerpublic class TestController1 { ITestService testService; /* * 基于构造方法的注入 * */ public TestController1(ITestService iTestService) { this.testService = iTestService; } @GetMapping(\"/status1\") public Result&lt;?&gt; status() { return testService.status(); }}它的好处在于，采用了构造方法注入，这种方式对对象创建的顺序会有要求，它将避免循环依赖问题。是最可靠的方法。4、构造器简化版(推荐)首先，需要引入lombok依赖&lt;dependency&gt; &lt;groupId&gt;org.projectlombok&lt;/groupId&gt; &lt;artifactId&gt;lombok&lt;/artifactId&gt; &lt;version&gt;1.18.2&lt;/version&gt;&lt;/dependency&gt;随后，我们在创建时就可以使用@RequiredArgsConstructor注解，它将帮我们创建构造器，final关键字必不可少。@RestController@RequiredArgsConstructorpublic class TestController3 { /* * 用@RequiredArgsConstructor注解，这个使用方式也可以应用于service层 * */ private final ITestService testService; @GetMapping(\"/status3\") public Result&lt;?&gt; status() { return testService.status(); }}在网上有博主总结了一张表，但因为到处能看到，不知原来出处是哪里。三、总结​\t在使用中，使用构造方法是比较可行的，加上lombok，其实也可以到达非常简便。​\t但最终还是要根据项目的结构、业务，选择平衡解决的落地方式。" }, { "title": "jar包与war包的区别", "url": "/posts/jarandwar/", "categories": "技术科普", "tags": "学习", "date": "2023-06-04 11:54:00 +0000", "snippet": "一、会产生的奇怪问题 我的一个springboot项目，用mvn install打包成jar，换一台有jdk的机器就直接可以用java -jar 项目名.jar的方式运行，没任何问题，为什么这里不需要tomcat也可以运行了？ 然后我打包成war放进tomcat运行，发现端口号变成tomcat默认的8080（我在server.port中设置端口8090）项目名称也必须加上了。也就是说我在...", "content": "一、会产生的奇怪问题 我的一个springboot项目，用mvn install打包成jar，换一台有jdk的机器就直接可以用java -jar 项目名.jar的方式运行，没任何问题，为什么这里不需要tomcat也可以运行了？ 然后我打包成war放进tomcat运行，发现端口号变成tomcat默认的8080（我在server.port中设置端口8090）项目名称也必须加上了。也就是说我在原来的机器的IDEA中运行，项目接口地址为 ip:8090/listall,打包放进另一台机器的tomcat就变成了ip:8080/项目名/listall。这又是为什么呢？ 通过jar运行实际上是启动了内置的tomcat,所以用的是应用的配置文件中的端口 直接部署到tomcat之后，内置的tomcat就不会启用，所以相关配置就以安装的tomcat为准，与应用的配置文件就没有关系了 至于为什么会造成这种问题，就要涉及到之前的技术发展进程了总体来说吧，很多年前，Sun 还在世的那个年代，在度过了早期用 C++写 Html 解析器的蛮荒时期后，有一批最早的脚本程序进入了 cgi 时代，此时的 Sun 决定进军这个领域，为了以示区别并显得自己高大上，于是研发了 servlet 标准，搞出了最早的 jsp。并给自己起了个高大上的称号 JavaEE （ Java 企业级应用标准，其实就是一堆服务器以 http 提供服务…）。既然是企业级标准那自然得有自己的服务器标准。于是 Servlet 标准诞生，以此标准实现的服务器称为 Servle 容器服务器，Tomcat 就是其中代表，被 Sun 捐献给了 Apache 基金会，那个时候的 Web 服务器还是个高大上的概念，当时的 Java Web 程序的标准就是 War 包(其实就是个 Zip 包)，这就是 War 包的由来。后来随着服务器领域的屡次进化，人们发现我们为什么要这么笨重的 Web 服务器，还要实现一大堆 Servlet 之外的管理功能，简化一下抽出核心概念 servlet 不是更好吗，最早这么干的似乎是 Jetty，出现了可以内嵌的 Servelet 服务器。去掉了一大堆非核心功能。后来 tomcat 也跟进了，再后来，本来很笨重的传统 JavaEE 服务器 Jboss 也搞了个 undertow 来凑热闹。正好这个时候微服务的概念兴起，“ use Jar，not War ”。要求淘汰传统 Servlet 服务器的呼声就起来了二、两者的主要区别1、war是一个web模块，其中需要包括WEB-INF，是可以直接运行的WEB模块；jar一般只是包括一些class文件，在声明了Main_class之后是可以用java命令运行的。2、war包是做好一个web应用后，通常是网站，打成包部署到容器中；jar包通常是开发时要引用通用类，打成包便于存放管理。3、war是Sun提出的一种Web应用程序格式，也是许多文件的一个压缩包。这个包中的文件按一定目录结构来组织；classes目录下则包含编译好的Servlet类和Jsp或Servlet所依赖的其它类（如JavaBean）可以打包成jar放到WEB-INF下的lib目录下。4、JAR文件格式以流行的ZIP文件格式为基础。与ZIP文件不同的是，JAR 文件不仅用于压缩和发布，而且还用于部署和封装库、组件和插件程序，并可被像编译器和 JVM 这样的工具直接使用。【格式特点】： 安全性　可以对 JAR 文件内容加上数字化签名。这样，能够识别签名的工具就可以有选择地为您授予软件安全特权，这是其他文件做不到的，它还可以检测代码是否被篡改过。 减少下载时间　如果一个 applet 捆绑到一个 JAR 文件中，那么浏览器就可以在一个 HTTP 事务中下载这个 applet 的类文件和相关的资源，而不是对每一个文件打开一个新连接。 压缩 JAR 格式允许您压缩文件以提高存储效率。 传输平台扩展 Java 扩展框架（Java Extensions Framework）提供了向 Java 核心平台添加功能的方法，这些扩展是用 JAR 文件打包的（Java 3D 和 JavaMail 就是由 Sun 开发的扩展例子）。 WAR文件就是一个Web应用程序，建立WAR文件，就是把整个Web应用程序（不包括Web应用程序层次结构的根目录）压缩起来，指定一个war扩展名。【建立的条件】： 需要建立正确的Web应用程序的目录层次结构。 建立WEB-INF子目录，并在该目录下建立classes与lib两个子目录。 将Servlet类文件放到WEB-INF\\classes目录下，将Web应用程序所使用Java类库文件（即JAR文件）放到WEB-INF\\lib目录下。 将JSP页面或静态HTML页面放到上下文根路径下或其子目录下。 建立META-INF目录，并在该目录下建立context.xml文件。三、两者的主要特点Java打包构建后，可以生成两种不同类型的包：JAR包和WAR包。JAR（Java Archive）包是一种用于打包Java类、资源文件和配置文件的标准Java归档文件格式。它通常用于构建独立的Java应用程序，可以直接通过Java虚拟机（JVM）运行。WAR（Web Application Archive）包则是一种用于打包JavaWeb应用程序的归档文件格式。它包含了Web应用程序的所有相关资源，例如Servlet、JSP、HTML、CSS、JavaScript等文件，以及配置文件和库文件。WAR包可以被应用服务器（如Tomcat、JBoss等）识别和部署。主要区别如下： 文件结构：JAR包适用于独立的Java应用程序，而WAR包适用于Web应用程序； 主要内容：JAR包主要包含类文件（.class）、资源文件（.properties、.xml）等，而WAR包除了包含类文件和资源文件外，还包含Web相关文件（Servlet、JSP、HTML、CSS、JavaScript等）； 部署方式：JAR包只需通过Java命令行或者双击运行即可，而WAR包需要将其部署到应用服务器中才能运行； 运行环境：JAR包可以在任何支持Java运行环境（JRE）的操作系统上运行，而WAR包需要在支持JavaWeb应用服务器的操作系统上进行部署和运行。总的来说，JAR包适用于独立的Java应用程序，而WAR包适用于Web应用程序。选择哪种打包方式取决于你的项目需求和运行环境。四、打包方法1、jar包 配置构建工具如maven、gradle，使用其进行构建 使用命令行进行打包 jar cf 文件名.jar 文件目录 使用开发idea 的 build Artifact 进行打包2、war包 配置构建工具如maven、gradle，使用其进行构建 需要ServletInitializer类 使用开发idea 的 build Artifact 进行打包五、idea相关的功能1、idea build Artifact在IntelliJ IDEA中，“Build Artifact ”是用于构建和生成项目输出文件（例如JAR包、WAR包等）的工具。通过Build Artifact，可以将项目编译为可运行的二进制格式，并将其包装为一个或多个文件，便于部署和分发。通常情况下，构建Artifact是项目的最终产品。在IntelliJ IDEA中配置Build Artifact的步骤如下： 打开项目，然后点击菜单栏的”File”，选择”Project Structure”（或者按下快捷键Ctrl+Alt+Shift+S）。 在Project Structure窗口中，选择”Artifacts”选项卡。 点击”+”按钮，选择你要构建的Artifact类型（例如JAR、WAR等）。 配置Artifact的设置，包括输出路径、依赖关系、源码文件等。 点击”Apply”或”OK”保存配置。一旦Build Artifact配置完成，你可以使用Build菜单或快捷键，将项目编译为指定的Artifact。IntelliJ IDEA会自动将相关文件打包，并生成Artifact文件。使用Build Artifact可以让你方便地构建和输出项目的可执行文件，对于部署和发布项目非常有用。2、idea build project在IntelliJ IDEA中，“Build Project”是用于编译整个项目的操作。它会执行以下任务： 检查代码错误：Build Project会对项目的Java源代码进行编译，并检查代码中的语法错误、命名问题、类型错误等。如果代码中存在错误，IDEA会在编译过程中将这些错误显示给你。 生成字节码：Build Project将Java代码编译为字节码（.class文件），这些文件是能够在Java虚拟机（JVM）上执行的二进制文件。 构建依赖：当你修改了项目的源代码、配置文件或资源文件时，Build Project还会重新构建项目的依赖关系。这意味着如果你修改了一个类，那么与该类相关的其他类也会被重新编译。 更新项目输出：Build Project会更新项目的输出文件（例如JAR包、WAR包等），以便能够进行部署和分发。通常情况下，当你对项目的源代码进行修改后，IntelliJ IDEA会自动执行Build Project操作。你也可以手动触发Build Project，方法是点击菜单栏的”Build”，然后选择”Build Project”（或者按下快捷键Ctrl+F9）。通过Build Project，你可以确保项目的代码被正确地编译，并且依赖关系也得到正确的构建，以便进行后续的开发、测试和部署。" }, { "title": "如何解决数据库与缓存数据一致的问题?", "url": "/posts/DataConsistency/", "categories": "技术实践", "tags": "学习", "date": "2023-05-27 07:18:00 +0000", "snippet": " 当我们在项目中引入redis缓存后，我们就必须考虑“数据库与缓存的数据一致性”这个问题，本文笔者将去剖析产生这个问题的原因以及给出整理后的解决方法与最佳实践一、常用的解决方法先给出解决方法： 先更新缓存，再更新数据库 先更新数据库，再更新缓存 先删除缓存，再更新数据库 先更新数据库，再删除缓存在更新的时候，操作缓存和数据库无疑就是以上四种可能之一二、每种情况下会产生的...", "content": " 当我们在项目中引入redis缓存后，我们就必须考虑“数据库与缓存的数据一致性”这个问题，本文笔者将去剖析产生这个问题的原因以及给出整理后的解决方法与最佳实践一、常用的解决方法先给出解决方法： 先更新缓存，再更新数据库 先更新数据库，再更新缓存 先删除缓存，再更新数据库 先更新数据库，再删除缓存在更新的时候，操作缓存和数据库无疑就是以上四种可能之一二、每种情况下会产生的数据不一致问题1.先更新缓存，再更新数据库如果我成功更新了缓存，但是在执行更新数据库的那一步，服务器突然宕机了，那么此时，我的缓存中是最新的数据，而数据库中是旧的数据。脏数据就因此诞生了，并且如果我缓存的信息（是单独某张表的），而且这张表也在其他表的关联查询中，那么其他表关联查询出来的数据也是脏数据，结果就是直接会产生一系列的问题。2.先更新数据库，在更新缓存只有等到缓存过期之后，才能访问到正确的信息。那么在缓存没过期的时间段内，所看到的都是脏数据。从上面两张图中，大家也能看出，无论咋样，只要执行第二步时失败了，就必然会产生脏数据。如果两步都能执行成功？能保证数据一致性吗？ 其实也不能，因为还有Java常考的并发。3.不建议更新缓存并发情况下的思考第一种情况第二种情况在这里可以看到当执行时序被改变，那么就必然会产生脏数据。看到这里，也许学过 Java 锁知识的小伙伴可能会说，咱们可以加锁啊，这样就不会产生这样的问题啦~在这里确实可以加锁，以保证用户的请求顺序，来达到数据一致性。虽然加锁确实可以通过牺牲一些性能来保证一定数据一致性，但我还是不推荐更新缓存的方式。原因如下： 首先加入缓存的主要作用是提高系统性能。 其次更新缓存的代价并不低。 复杂场景下：比如可能更新了某个表的一个字段，然后其对应的缓存，是需要查询另外两个表的数据并进行运算，才能计算出缓存最新的值的。 缓存利用率问题。一个频繁更新的缓存，它是否会被频繁的访问呢？ 一个缓存在很短的时间内，更新10次，20次或者更多，但是实际访问次数只有1、2次，这其实也是一种浪费。 如果采用删除缓存就不会这样，删除了缓存，那么就只会等到有人要使用缓存的时候，才会重新查询数据，放入缓存中。这其实也是懒加载的思想，等到要使用了，再加载。 当然业务场景确实有这样的场景，这么使用也未免不可， 一切都要实事求是，而并非空谈。 当然，难道先删除缓存，再更新数据库，或者是先更新数据库，再删除缓存就没有问题了吗？4.先删除缓存，再更新数据库如果只有第一步执行成功，而第二步失败，那么只有缓存中的数据被删除了，但是数据库没有更新，那么在下一次进行查询的时候，查不到缓存，只能重新查询数据库，构建缓存，这样其实也是相对做到了数据一致性。但如果是处于读写并发的情况下，还是会出现数据不一致的情况：执行完成后，明显可以看出，1号用户所构建的缓存，并不是最新的数据，还是存在问题的~5.先更新数据库，再删除缓存如果更新数据库成功了，而删除缓存失败了，那么数据库中就会是新数据，而缓存中是旧数据，数据就出现了不一致情况。和之前一样，如果两段代码都执行成功，在并发情况下会是什么样呢？还是会造成数据的不一致性。但是此处达成这个数据不一致性的条件明显会比起其他的方式更为困难 ： 时刻1：读请求的时候，缓存正好过期 时刻2：读请求在写请求更新数据库之前查询数据库， 时刻3：写请求，在更新数据库之后，要在读请求成功写入缓存前，先执行删除缓存操作。这通常是很难做到的，因为在真正的并发开发中，更新数据库是需要加锁的，不然没一点安全性~一定程度上来讲，这种方式还是解决了一定程度上的数据不一致性问题的。不过在这四种选择中，平常都会优先考虑后两种方式。并且市面上对于这后两种选择，也已经有一些解决方案。在谈解决方案之前，我们先看看需要解决的问题： 1.我们要如何保证这两段代码一起执行成功 (即更新数据库 删除缓存这两个操作) 2.【先删除缓存，再更新数据库】在读写并发时，会产生一个缓存旧数据，而数据库是新数据的问题，这该如何解决呢？3.加锁可以解决并发情况下出现的不一致问题吗？三、数据一致性的补充简单说，只要使用缓存，那么必然就会产生缓存和数据库数据不一致的问题。在这首先我们要明确一个问题，就是我们的系统是否一定要做到“缓存+数据库”完全一致性？是否能够接受偶尔的数据不一致性问题？能够接受最长时间的数据不一致性？强一致性如果缓存和数据库要达到数据的完全一致，那么就只能读写都加锁，变成串行化执行，系统吞吐量也就大大降低了，一般不是必须达到强一致性，不采用这样的方式。并且实在过于要求强一致性，会采用限流+降级，直接走MySQL，而不是特意加一层 Redis 来处理。弱一致性（最终一致性）一般而言，大都数项目中，都只是要求最终一致性，而非强一致性。最终一致性是能忍受一定时间内的数据不一致性的，只要求最后的数据是一致的即可。例如缓存一般是设有失效时间的，失效之后数据也会保证一致性，或者是下次修改时，没有并发，也会让数据回到一致性等等。四、数据一致性的解决 同时也是更新数据库和缓存时，会造成数据不一致的原因 ①两个操作不是原子 ②多个线程并发访问1.我们要如何保证这两段代码一起执行成功重试机制 异步 联想到消息队列 采用消息队列进行解决我们可以把第二步操作交由消息队列去做，达到一个异步重试的效果。并且引入消息队列来实现，代价并非想象中的那么大。当然大家也会说，如果发送消息也失败呢？有这种可能，但真的不算高，另外消息队列自身是很好的支持高可用的。 首先消息队列在高并发的场景下，可以毋庸置疑的说是一个非常重要的组件啦，所以引入消息队列以及维护消息队列，其实都不能算是额外的负担。 其次消息队列具有持久化，即使项目重启也不会丢失。 最后消息队列自身可以实现可靠性 保证消息成功发送，发送到交换机； 保证消息成功从交换机发送至队列； 消费者端接收到消息，采用手动ACK确认机制，成功消费后才会删除消息，消费失败则重新投递~ Canal 订阅日志实现消息队列虽然已经比较简单，但是仍然要手动的进行代码的编写，以及写一个消费者来进行监听，可以说还是比较麻烦，每个地方都还要引入消息队列，发送一个消息~，有没有办法省去这一步呢？有的勒，偷懒的人大有人在勒现有的解决方案中，可以使用 alibaba 的开源组件 Canal，订阅数据库变更日志，当数据库发生变更时，我们可以拿到具体操作的数据，然后再去根据具体的数据，去删除对应的缓存。2.在读写并发时，会产生一个缓存旧数据，而数据库是新数据的问题，这该如何解决呢？解决这样的问题，其实最好的方式就是在执行完更新数据库的操作后，先休眠一会儿，再进行一次缓存的删除，以确保数据一致性，这也就是市面上给出的主流解决方案–延时双删。3.加锁可以解决并发情况下出现的不一致问题吗？加分布式锁解决！五、最佳实践对于一些读多写少、写操作并发竞争不是特别激烈且对一致性要求不是特别高的情况下，可以采用事务（高隔离级别） + 先更新数据库再更新缓存的方式来达到数据一致的诉求。在对并发性能要求极高的情况下，可以考虑非事物类的其余方式来实现，如重试机制、或异步补偿机制、或多者结合方式等。实际使用场景中，对于一致性要求不是特别高、且并发量不是特别大的场景，可以选择基于数据库事务保证的先更新数据库再更新/删除缓存。而对于并发要求较高、且数据一致性要求较好的时候，推荐选择先更新数据库，再删除缓存，并结合删除重试 + 补偿逻辑 + 缓存过期TTL等综合手段。" }, { "title": "Rabbitmq详解", "url": "/posts/Rabbitmq/", "categories": "技术科普", "tags": "学习", "date": "2023-05-15 12:49:00 +0000", "snippet": " 本篇文章可以帮助你快速了解消息队列以及对应的应用场景与问题。第八小节结合场景阅读更佳一、基本概述RabbitMQ是一种开源的消息中间件，基于AMQP（高级消息队列协议）实现。它的基本原理如下： 生产者向RabbitMQ发送消息，消息会被发送到交换机（Exchange）。 交换机接收到消息后，根据规则（Routing Key）将消息路由到一个或多个队列（Queue）。 消费者订阅队列...", "content": " 本篇文章可以帮助你快速了解消息队列以及对应的应用场景与问题。第八小节结合场景阅读更佳一、基本概述RabbitMQ是一种开源的消息中间件，基于AMQP（高级消息队列协议）实现。它的基本原理如下： 生产者向RabbitMQ发送消息，消息会被发送到交换机（Exchange）。 交换机接收到消息后，根据规则（Routing Key）将消息路由到一个或多个队列（Queue）。 消费者订阅队列，一旦有消息到达队列，就会将消息取出进行处理。 RabbitMQ支持多种消息传递模式，包括点对点（Point-to-Point）和发布/订阅（Publish/Subscribe）。 RabbitMQ采用消息确认机制，保证消息的可靠性传递。当消费者处理完消息后，会向RabbitMQ发送确认消息，告知RabbitMQ该消息已经被处理。 RabbitMQ还支持消息持久化、消息优先级、消息过期等高级特性，可根据实际需求进行配置。总之，RabbitMQ提供了一个可靠、灵活、可扩展的消息传递机制，为分布式系统提供了重要的基础设施。二、组成部分RabbitMQ的组成部分包括以下几个方面： Broker：RabbitMQ的核心组件，负责接收、存储和转发消息。Broker由Exchange、Queue和Binding三个部分组成。 Exchange：消息交换机，负责接收生产者发送的消息，并根据Binding规则将消息路由到相应的Queue中。 Queue：消息队列，消息在Broker中存储的地方，消费者从队列中获取消息并进行处理。 Binding：Exchange和Queue之间的绑定关系，定义了消息的路由规则。 Connection：连接，指生产者或消费者与RabbitMQ Broker之间的TCP连接。 Channel：通道，每个连接可以创建多个通道，通道是进行消息传递的逻辑通道，可以在通道中进行消息的发送和接收。 Virtual Host：虚拟主机，用于将RabbitMQ Broker划分为多个逻辑部分，每个虚拟主机拥有自己的Exchange、Queue和Binding等。 Producer：消息生产者，负责向Exchange发送消息。 Consumer：消息消费者，负责从Queue中获取消息并进行处理。以上是RabbitMQ的主要组成部分，每个组件都有自己的作用和功能，共同构成了一个完整的消息传递系统。三、应用场景 异步处理 - 相比于传统的串行、并行方式，提高了系统吞吐量。 应用解耦 - 系统间通过消息通信，不用关心其他系统的处理。 流量削锋 - 可以通过消息队列长度控制请求量；可以缓解短时间内的高并发请求。 日志处理 - 解决大量日志传输。 消息通讯 - 消息队列一般都内置了高效的通信机制，因此也可以用在纯的消息通讯。比如实现点对点消息队列，或者聊天室等。四、使用消息队列会带来什么问题？ 系统可用性降低本来系统运行好好的，现在你非要加入个消息队列进去，那消息队列挂了，你的系统不是呵呵了。因此，系统可用性会降低； 系统复杂度提高加入了消息队列，要多考虑很多方面的问题，比如：一致性问题、如何保证消息不被重复消费、如何保证消息可靠性传输等。因此，需要考虑的东西更多，复杂性增大。 一致性问题A 系统处理完了直接返回成功了，人都以为你这个请求就成功了；但是问题是，要是 BCD 三个系统那里，BD 两个系统写库成功了，结果 C 系统写库失败了，咋整？你这数据就不一致了。五、AMQP协议是什么？​\tAdvanced Message Queue，高级消息队列协议。它是应用层协议的一个开放标准，为面向消息的中间件设计，基于此协议的客户端与消息中间件可传递消息，并不受产品、开发语言等条件的限制。​\tAMQP协议的设计目标是提供一个标准的、可靠的、高效的消息传递机制，以满足不同应用场景下的需求。它支持点对点（Point-to-Point）和发布/订阅（Publish/Subscribe）两种消息传递模式，同时还提供了丰富的消息属性、消息路由规则、消息确认机制等高级特性，可满足复杂的业务需求。六、exchange有哪些类型？RabbitMQ中的Exchange有四种类型，分别是：Direct Exchange、Fanout Exchange、Topic Exchange和Headers Exchange。 Direct Exchange：直连交换机，它根据消息的Routing Key将消息路由到相应的Queue中。当Exchange收到一条消息时，会将消息的Routing Key与Binding Key进行比较，如果相同，则将消息路由到对应的Queue中。 Fanout Exchange：扇形交换机，它将消息广播到所有绑定到该Exchange上的Queue中，不考虑消息的Routing Key。当Exchange收到一条消息时，会将消息路由到所有绑定的Queue中。 Topic Exchange：主题交换机，它根据消息的Routing Key和Binding Key的匹配规则将消息路由到相应的Queue中。当Exchange收到一条消息时，会将消息的Routing Key与Binding Key进行模糊匹配，如果匹配成功，则将消息路由到对应的Queue中。 Headers Exchange：头交换机，它根据消息的Header信息将消息路由到相应的Queue中。当Exchange收到一条消息时，会将消息的Header信息与Binding中的参数进行比较，如果匹配成功，则将消息路由到对应的Queue中。以上四种Exchange类型各有特点，可以根据实际业务需求选择合适的类型。七、queue有哪些类型？在RabbitMQ中，Queue有以下几种类型： Classic Queue：经典队列，是最常见的队列类型，消息会被顺序存储在队列中，消费者从队列中接收消息。 Lazy Queue：懒惰队列，懒惰队列是一种特殊的经典队列，它是为了解决经典队列在存储大量消息时可能会出现的性能问题而设计的。懒惰队列会将消息存储在磁盘中，只有当消息需要被消费者消费时才会从磁盘中读取。 Priority Queue：优先级队列，它允许消息设置优先级，高优先级的消息会被优先消费。 Quorum Queue：法定队列，是RabbitMQ 3.8版本新增的队列类型，它提供了更高的可靠性和可用性，支持多个节点之间的数据同步，可以在节点故障时快速恢复。 TTL Queue：过期队列，消息在队列中存活的时间有限，如果消息在规定时间内没有被消费者消费，则会被自动删除。 Shovel Queue：隧道队列，是一种特殊的队列类型，它可以将消息从一个RabbitMQ服务器传输到另一个RabbitMQ服务器。以上是RabbitMQ中常见的Queue类型，每种类型都有自己的特点和使用场景，可以根据实际业务需求选择合适的类型。八、常见的业务问题 核心中的核心 这一模块好好理解使用RabbitMQ时，常见的业务问题包括：1、如何保证mq中的消息都会被消费到？导致消息无法被消费的原因： 1.在消息传递过程中，可能会发生消息丢失的情况，导致消息无法被消费者接收 2.消费者消费失败的场景 3.消费者消费完成后没有进行确认 解决方法：1、设置消息发送表:在消息传递过程中，设置消息发送表来判断消息是否有发送到rabbitmq之中，同时可以设置重传机制2、消息持久化：在消息传递过程中，可能会发生RabbitMQ节点宕机等异常情况，导致消息丢失。因此，需要将消息进行持久化，确保即使在RabbitMQ节点宕机的情况下，消息也能够被恢复。3、消费者的高可用性：消费者也可能会出现宕机等异常情况，导致消息无法被正常消费。因此，需要确保消费者的高可用性，例如采用集群部署、负载均衡等方式，确保即使某个消费者宕机，其他消费者也能够正常消费消息。4、消息重试机制：如果在消息处理过程中发生异常或错误，需要进行消息重试，确保消息能够被正常消费。5、TTL机制：可以通过设置消息的TTL（Time To Live）来保证消息不会一直在队列中等待消费，避免消息一直占用队列资源。6、消息确认机制：消费者在消费完消息后需要向RabbitMQ发送确认消息，告知RabbitMQ消息已经被成功消费。如果RabbitMQ没有收到确认消息，就会认为消息未被消费成功，将会重新发送该消息，直到被消费者确认为止。2、如何保证mq的消息不被重复消费?导致消息重复消费的原因： 由于网络等原因，消息可能会被重复发送，导致消费者重复消费同一条消息解决方法：1. 消息去重：在消息消费前，可以先判断该消息是否已经被消费过，如果已经被消费过，则直接忽略该消息，避免重复消费。可以使用 Redis、MySQL 等数据库或者缓存来记录已经消费过的消息。2. 消息幂等性：在消费消息时，需要保证消息的幂等性，即多次消费同一条消息，最终的结果都是一致的。可以在消息处理过程中使用唯一标识符来保证幂等性，例如使用数据库的主键、全局唯一标识符（UUID）等。3. 消息确认机制：在消费者消费完消息后，需要向 RabbitMQ 发送确认消息，告知 RabbitMQ 消息已经被成功消费。如果 RabbitMQ 没有收到确认消息，就会认为消息未被消费成功，将会重新发送该消息，从而导致消息重复消费。因此，需要确保消费者正确地发送确认消息。4. 消息过期时间：在消息发送时，可以设置消息的过期时间，如果消息在规定时间内未被消费，则该消息会过期，从而避免消息被重复消费3、消息堆积导致消息堆积的原因： 当消息的消费速度低于生产速度时，就会导致消息堆积的情况，如果消息堆积过多，会导致系统性能下降 解决方法：1. 增加消费者：可以通过增加消费者的数量，来提高消息的消费速度，从而避免消息在队列中积累过多。可以采用集群部署、负载均衡等方式，实现消费者的高可用性和负载均衡。2. 增加队列容量：可以通过增加队列的容量，来缓解消息堆积的问题。可以通过 RabbitMQ 的管理界面或者命令行工具来修改队列的容量参数。3. 设置消息 TTL：可以通过设置消息的 TTL（Time To Live），来避免消息在队列中长时间积累。可以将过期时间设置为较短的时间，从而保证消息能够及时被消费。4. 设置死信队列：可以通过设置死信队列，来处理一些无法被消费的消息，例如消息过期、队列满等情况。4、垃圾消息问题导致垃圾消息问题的原因： 消费者重复多次发送同样的消息 解决方法：1、为消费者的重传机制设置上限，每次在job重试时，需要先判断一下消息发送表中该消息的发送次数是否达到最大限制，如果达到了，则直接返回。5、RabbitMQ节点宕机导致RabbitMQ节点宕机的原因： 由于硬件故障或其他原因，RabbitMQ节点可能会宕机，导致消息无法被正常处理。 解决方法：1、集群部署RabbitMQ 以增加 RabbitMQ 节点，实现其的高可用性，但同时要实现多节点之间的消息共享和负载均衡。九、其他概念1、死信队列是干什么的？死信队列是消息队列中的一种特殊队列，用于存储无法被正常消费的消息。当一个消息在队列中被处理多次仍然无法被消费者处理时，该消息就会被发送到死信队列中。死信队列通常用于处理以下情况： 消息无法被正确处理：例如，消息的格式不正确或者消息所需的资源不足等情况。 消息被拒绝：例如，消费者拒绝处理某些消息，或者消息被设置为不可重试。 消息过期：例如，消息在队列中等待太久，超过了设置的过期时间。死信队列可以帮助开发人员更好地管理消息队列中的消息，避免消息丢失或者无法被处理的情况，并且可以方便地对无法被处理的消息进行分析和处理。2、消息的TTL是指啥？消息的TTL（Time-To-Live）指的是消息在消息队列中的最长存活时间。当消息在队列中等待时间超过TTL时，消息将被标记为过期并被丢弃或者发送到死信队列中。TTL通常是在发送消息时设置的，可以根据业务需求设置不同的TTL。例如，对于一些实时性要求较高的消息，可以设置较短的TTL，以确保消息能够及时被消费；对于一些不太紧急的消息，可以设置较长的TTL，以确保消息可以在一定时间内得到处理。TTL是消息队列中一个非常重要的概念，可以帮助开发人员更好地管理消息队列中的消息，避免消息过期或者无法被处理的情况，并且可以方便地对过期的消息进行分析和处理。" }, { "title": "Java集合详解", "url": "/posts/JavaCollection/", "categories": "技术科普", "tags": "学习", "date": "2023-05-13 11:31:00 +0000", "snippet": " 最近重新复习了Java基础，把集合模块的知识好好地整理一遍，本文为笔者复习时的笔记集合类存放于java.util包中，主要有三种：set、list(列表中包含Queue、stack)、map一、List、Set、Map三种集合的特点与区别1、特点 List（列表）： 特点：List是一个有序的集合，允许元素重复。它维护了元素的插入顺序，并且可以根据索引访问和操作元...", "content": " 最近重新复习了Java基础，把集合模块的知识好好地整理一遍，本文为笔者复习时的笔记集合类存放于java.util包中，主要有三种：set、list(列表中包含Queue、stack)、map一、List、Set、Map三种集合的特点与区别1、特点 List（列表）： 特点：List是一个有序的集合，允许元素重复。它维护了元素的插入顺序，并且可以根据索引访问和操作元素。 适用场景：当需要按照插入顺序存储元素，并且允许重复元素时，可以选择List。常用的List实现类有ArrayList和LinkedList。 线程安全的类有：CopyOnWriteArrayList Set（集合）： 特点：Set是一个不允许重复元素的集合，每个元素在Set中是唯一的。Set不保证元素的顺序，即插入顺序不被保留。 适用场景：当需要存储一组唯一的元素，并且不关心元素的顺序时，可以选择Set。常用的Set实现类有HashSet、TreeSet和LinkedHashSet。 线程安全的类有：CopyOnWriteArraySet、ConcurrentSkipListSet、ConcurrentHashMap的键的集合视图，即keySet()方法返回的Set集合 Map（映射）： 特点：Map是一种键值对（Key-Value）的集合，每个键是唯一的。可以通过键快速查找、插入和删除与之关联的值。 适用场景：当需要按照键值对的方式存储和检索数据时，可以选择Map。常用的Map实现类有HashMap、TreeMap和LinkedHashMap。 线程安全的类有： ConcurrentHashMap 2、区别 List和Set都是集合，但List允许元素重复，而Set不允许重复元素。 List和Set都继承自Collection接口，而Map则是独立的接口。 List和Set都维护了元素的顺序，可以根据索引（List）或迭代顺序（Set）访问元素，而Map则是通过键来访问值。 List使用有序的索引来操作元素，支持重复元素，适合需要按顺序存储和访问元素的场景。Set不保证元素的顺序，并且不允许重复元素，适合需要存储唯一元素的场景。Map通过键值对存储数据，可以根据键快速查找和操作对应的值，适合需要根据键检索数据的场景。二、Set集合怎么去重Set集合是自动去重的，即不允许包含重复的元素。当向Set集合中添加元素时，如果元素已经存在于集合中，那么添加操作将不会生效，集合中仍然保持原有的唯一性。是如何实现的?原理是什么？Set集合实现去重的原理是基于元素的唯一性。当向Set集合中添加元素时，集合会根据元素的哈希值（hashCode）和相等性（equals）来判断是否已经存在相同的元素。具体实现的步骤如下： 当向Set集合中添加元素时，集合会首先调用元素的hashCode()方法获取元素的哈希值。 Set集合内部使用哈希表（Hash Table）或平衡二叉树（如红黑树）来存储元素。 首先，Set集合会根据元素的哈希值，将元素放入合适的桶（bucket）或树节点中。 如果桶或树节点为空，直接将元素存入其中；如果不为空，则进行相等性判断。 集合会调用元素的equals()方法来比较新元素和已存在的元素是否相等。 如果equals()方法返回true，表示新元素和已存在的元素相等，则新元素不会被添加到集合中；如果返回false，则将新元素添加到集合中。通过使用哈希值和相等性的判断，Set集合可以保证其中的元素是唯一的。这种实现方式可以高效地进行去重操作，并且不需要手动进行遍历和比较元素。因此，Set集合是一种非常方便的数据结构，适用于存储唯一元素的场景。 面试时这里可能就会涉及hashcode与equals的区别三、hashset与treeset如何去重和排序HashSet和TreeSet在去重和排序方面有一些不同的特点。HashSet： 去重：HashSet通过元素的哈希值来确定元素的存储位置，当向HashSet中添加元素时，先计算元素的哈希值，然后将元素放入相应的哈希桶中。如果新元素的哈希值与已存在的元素的哈希值相同，HashSet会调用元素的equals()方法进行相等性比较，如果返回true，则认为元素已存在，不会被添加到集合中。 排序：HashSet不会对元素进行排序，元素在HashSet中的存储顺序由哈希值决定，不保证插入顺序或其他特定的顺序。TreeSet： 去重：TreeSet通过比较元素的大小来确定元素的存储位置。当向TreeSet中添加元素时，TreeSet会调用元素的compareTo()方法或自定义的比较器来确定元素的顺序**。如果新元素与已存在的元素相等，TreeSet会认为新元素是重复的，不会被添加到集合中。 排序：TreeSet会对元素进行排序，元素按照自然顺序或自定义比较器定义的顺序进行排序。在添加元素时，TreeSet会根据元素的比较结果将元素放入合适的位置，从而实现了排序。总结： HashSet通过哈希值和相等性来去重，不进行排序。 TreeSet通过比较元素的大小来去重，并且会对元素进行排序。因此，如果只需要去重功能而不关心元素的顺序，可以选择HashSet。如果需要去重并且希望元素按照一定的顺序进行存储和遍历，可以选择TreeSet，并根据需要实现Comparable接口或传入自定义的比较器。 自定义排序：实现Comparable接口或传入自定义的比较器四、如何实现set集合的有序性？如果需要实现有序的Set集合，可以使用TreeSet类。TreeSet是基于红黑树（红黑二叉搜索树）实现的Set集合，它可以按照元素的自然顺序或自定义的比较器来进行排序。 面试的时候提redis的zset集合也是一种不错的思路五、集合底层实现1、List集合Java中的List集合有多种实现，每种实现都有不同的底层数据结构。以下是一些常见的List集合实现及其底层数据结构： ArrayList： 底层数据结构：ArrayList使用动态数组实现。它内部使用数组来存储元素，并根据需要动态调整数组的大小。当元素数量超过数组容量时，ArrayList会创建一个更大的数组，并将元素复制到新数组中。 LinkedList： 底层数据结构：LinkedList使用双向链表实现。它内部的节点包含了元素本身以及指向前一个节点和后一个节点的引用。LinkedList通过节点的链接来实现元素的插入、删除和访问。 Vector： 底层数据结构：Vector也使用动态数组实现，与ArrayList类似。它是Java早期提供的线程安全的List集合实现，但在性能上不如ArrayList。由于其线程安全的特性，现在一般更推荐使用ConcurrentArrayList或CopyOnWriteArrayList来替代Vector。 Stack： 底层数据结构：Stack是Vector的子类，它基于动态数组实现。Stack是一种后进先出（LIFO）的数据结构，支持压栈（push）和出栈（pop）操作。 CopyOnWriteArrayList： 底层数据结构：CopyOnWriteArrayList也使用动态数组实现，类似于ArrayList。但与ArrayList不同的是，CopyOnWriteArrayList在修改操作时会创建一个新的数组，并将元素复制到新数组中，从而实现读写分离，保证了读操作的并发性。 这些List集合实现使用不同的底层数据结构，每种数据结构都有不同的特点和适用场景。选择合适的List集合实现取决于具体的需求，例如对性能、线程安全性或特定操作的需求。一般情况下，ArrayList是使用最广泛的List集合实现，它提供了快速的随机访问和修改操作。2、Map集合Java中的Map集合也有多种实现，每种实现都有不同的底层数据结构。以下是一些常见的Map集合实现及其底层数据结构： HashMap： 底层数据结构：HashMap使用哈希表（散列表）实现。它通过键的哈希值来确定键值对的存储位置，并使用链表或红黑树处理键的哈希冲突。在Java 8及之后的版本，当链表长度超过一定阈值时，会将链表转换为红黑树，以提高查找效率。 TreeMap： 底层数据结构：TreeMap使用红黑树（红黑二叉搜索树）实现。它根据键的自然顺序或自定义的比较器来对键进行排序，并将键值对存储在树节点中。红黑树的特点保证了键值对的有序性。 LinkedHashMap： 底层数据结构：LinkedHashMap是HashMap的子类，它使用哈希表和双向链表实现。除了哈希表的快速查找特性外，LinkedHashMap还维护了键值对的插入顺序或访问顺序（可配置），通过双向链表来保持顺序。 ConcurrentHashMap： 底层数据结构：ConcurrentHashMap是哈希表的线程安全实现，内部使用分段锁（Segment）来实现并发访问。它在多线程环境下提供了高并发的读操作，并且支持并发的读写操作。 Hashtable： 底层数据结构：Hashtable是早期Java版本提供的线程安全的哈希表实现，与HashMap类似。不同之处在于Hashtable的操作是同步的，保证了线程安全，但也导致了性能上的一些损耗。现在一般更推荐使用ConcurrentHashMap来替代Hashtable。 这些Map集合实现使用不同的底层数据结构，每种数据结构都有不同的特点和适用场景。选择合适的Map集合实现取决于具体的需求，例如对性能、线程安全性、有序性或特定操作的需求。一般情况下，HashMap是使用最广泛的Map集合实现，它提供了快速的查找、插入和删除操作。3、Set集合Java中的Set集合也有多种实现，每种实现都有不同的底层数据结构。以下是一些常见的Set集合实现及其底层数据结构： HashSet： 底层数据结构：HashSet使用哈希表（散列表）实现。它通过元素的哈希值来确定元素的存储位置，并使用链表或红黑树处理哈希冲突。在Java 8及之后的版本，当链表长度超过一定阈值时，会将链表转换为红黑树，以提高查找效率。 TreeSet： 底层数据结构：TreeSet使用红黑树（红黑二叉搜索树）实现。它根据元素的自然顺序或自定义的比较器来对元素进行排序，并将元素存储在树节点中。红黑树的特点保证了元素的有序性。 LinkedHashSet： 底层数据结构：LinkedHashSet是HashSet的子类，它使用哈希表和双向链表实现。除了哈希表的快速查找特性外，LinkedHashSet还维护了元素的插入顺序或访问顺序（可配置），通过双向链表来保持顺序。 ConcurrentSkipListSet： 底层数据结构：ConcurrentSkipListSet使用跳表数据结构实现。它是一种有序的并发数据结构，支持快速的查找、插入和删除操作，并且支持并发访问。 CopyOnWriteArraySet： 类型：它是基于CopyOnWriteArrayList的线程安全Set集合实现。 特点：CopyOnWriteArraySet使用CopyOnWrite机制，在修改操作时会创建一个新的底层数组，并将元素复制到新数组中。这种机制保证了读操作的并发性，并且不会导致读取操作的异常或修改操作的冲突这些Set集合实现使用不同的底层数据结构，每种数据结构都有不同的特点和适用场景。选择合适的Set集合实现取决于具体的需求，例如对性能、有序性、并发性或特定操作的需求。一般情况下，HashSet是使用最广泛的Set集合实现，它提供了快速的查找、插入和删除操作。" }, { "title": "封装自己的反射工具类！", "url": "/posts/reflect/", "categories": "技术实践", "tags": "学习", "date": "2023-05-06 01:53:00 +0000", "snippet": "一、基本概念与作用​\tJava反射机制是指在运行时动态地获取类的信息并操作类的属性、方法和构造函数等的能力。通过反射机制，可以在程序运行时动态地创建对象、调用方法、访问属性等，而不需要在编译时确定这些类的信息。反射机制主要包括以下几个重要的类： Class类：表示类的类型，可以获取类的信息，如类名、父类、接口、构造函数、方法、属性等。 Constructor类：表示类的构造函数，可以通过...", "content": "一、基本概念与作用​\tJava反射机制是指在运行时动态地获取类的信息并操作类的属性、方法和构造函数等的能力。通过反射机制，可以在程序运行时动态地创建对象、调用方法、访问属性等，而不需要在编译时确定这些类的信息。反射机制主要包括以下几个重要的类： Class类：表示类的类型，可以获取类的信息，如类名、父类、接口、构造函数、方法、属性等。 Constructor类：表示类的构造函数，可以通过它创建对象。 Method类：表示类的方法，可以通过它调用方法。 Field类：表示类的属性，可以通过它访问属性。​\t反射机制的优点是可以实现动态加载类和动态调用类的方法，提高了程序的灵活性和扩展性。但是，反射机制的使用也需要注意一些问题，如性能问题、安全问题等。/** * 反射机制实践 * * @author YKFire */public class ReflectionExample { public static void main(String[] args) { String className = \"java.lang.String\"; //加载类 try { //通过调用Class.forName方法加载类 Class&lt;?&gt; clazz = Class.forName(className); //简单地获取类信息 String name = clazz.getName(); System.out.println(\"Class name：\" + name); } catch (ClassNotFoundException e) { e.printStackTrace(); } }}二、支撑反射的重要核心类1、Class类​\tJava中的Class类是反射机制的核心类，它代表了一个类或接口在运行时的信息，包括类的名称、父类、接口、构造方法、方法、属性等信息。在Java中，每个类都有一个与之对应的Class对象。获取Class对象的方式有三种： 使用Class.forName方法：该方法会返回与指定类名对应的Class对象，如果指定的类不存在，则会抛出ClassNotFoundException异常。Class&lt;?&gt; clazz = Class.forName(\"com.example.Person\"); 使用类字面常量：在编译期间就会将类字面常量转换为对应的Class对象，不需要进行运行时的类加载。Class&lt;?&gt; clazz = Person.class; 使用对象的getClass方法：该方法会返回该对象所属类的Class对象。Person person = new Person(); Class&lt;?&gt; clazz = person.getClass();Class类提供了许多有用的方法，例如： getConstructors：获取类的所有公共构造方法； getDeclaredConstructors：获取类的所有构造方法，包括私有构造方法； getMethods：获取类的所有公共方法，包括从父类继承的方法； getDeclaredMethods：获取类的所有方法，包括私有方法； getFields：获取类的所有公共属性，包括从父类继承的属性； getDeclaredFields：获取类的所有属性，包括私有属性。​\t通过Class类提供的方法，我们可以获取类的构造方法、方法、属性等信息，并通过反射机制来创建对象、调用方法、访问属性等。2、Constructor类​\tJava中的Constructor类是反射机制中表示构造方法的类，它封装了构造方法的信息，包括参数类型、修饰符等信息。通过Constructor类，我们可以在运行时动态地创建一个类的对象。获取Constructor对象的方式有两种： 使用Class类的getConstructor或getDeclaredConstructor方法：这些方法会返回指定参数类型的公共构造方法或私有构造方法的Constructor对象。getConstructor方法只能获取公共构造方法，而getDeclaredConstructor方法可以获取私有构造方法。Class&lt;?&gt; clazz = Person.class; Constructor&lt;?&gt; publicConstructor = clazz.getConstructor(String.class, int.class); Constructor&lt;?&gt; privateConstructor = clazz.getDeclaredConstructor(String.class); 使用Class类的getConstructors或getDeclaredConstructors方法：这些方法会返回类的所有公共构造方法或私有构造方法的Constructor对象数组。Class&lt;?&gt; clazz = Person.class; Constructor&lt;?&gt;[] publicConstructors = clazz.getConstructors(); Constructor&lt;?&gt;[] privateConstructors = clazz.getDeclaredConstructors();Constructor类提供了许多有用的方法，例如： newInstance：创建一个对象； getParameterTypes：获取参数类型数组； getModifiers：获取修饰符。通过Constructor类提供的方法，我们可以在运行时动态地创建一个类的对象，例如：Constructor&lt;Person&gt; constructor = Person.class.getConstructor(String.class, int.class); Person person = constructor.newInstance(\"Tom\", 20);上述代码中，我们通过反射机制获取Person类中的指定构造方法，并创建一个Person对象。3、Method类​\tJava中的Method类是反射机制中表示方法的类，它封装了方法的信息，包括方法名、参数类型、返回值类型、修饰符等信息。通过Method类，我们可以在运行时动态地调用一个类的方法。获取Method对象的方式有两种： 使用Class类的getMethod或getDeclaredMethod方法：这些方法会返回指定名称和参数类型的公共方法或私有方法的Method对象。getMethod方法只能获取公共方法，而getDeclaredMethod方法可以获取私有方法。Class&lt;?&gt; clazz = Person.class; Method publicMethod = clazz.getMethod(\"publicMethod\", String.class); Method privateMethod = clazz.getDeclaredMethod(\"privateMethod\", int.class); 使用Class类的getMethods或getDeclaredMethods方法：这些方法会返回类的所有公共方法或私有方法的Method对象数组。Class&lt;?&gt; clazz = Person.class;Method[] publicMethods = clazz.getMethods(); Method[] privateMethods = clazz.getDeclaredMethods();Method类提供了许多有用的方法，例如： invoke：调用该方法； getName：获取方法名； getReturnType：获取返回值类型； getParameterTypes：获取参数类型数组； getModifiers：获取修饰符。通过Method类提供的方法，我们可以在运行时动态地调用一个类的方法，例如：Person person = new Person(); Method method = person.getClass().getMethod(\"sayHello\", String.class); method.invoke(person, \"Tom\");上述代码中，我们通过反射机制获取Person类中的sayHello方法，并调用该方法，传入一个字符串参数。4、Field类Java中的Field类是反射机制中表示属性的类，它封装了属性的信息，包括属性名、类型、修饰符等信息。通过Field类，我们可以在运行时动态地访问一个类的属性。获取Field对象的方式有两种： 使用Class类的getField或getDeclaredField方法：这些方法会返回指定名称的公共属性或私有属性的Field对象。getField方法只能获取公共属性，而getDeclaredField方法可以获取私有属性。Class&lt;?&gt; clazz = Person.class; Field publicField = clazz.getField(\"publicField\"); Field privateField = clazz.getDeclaredField(\"privateField\"); 使用Class类的getFields或getDeclaredFields方法：这些方法会返回类的所有公共属性或私有属性的Field对象数组。Class&lt;?&gt; clazz = Person.class; Field[] publicFields = clazz.getFields();Field[] privateFields = clazz.getDeclaredFields();Field类提供了许多有用的方法，例如： get：获取指定对象上该属性的值； set：设置指定对象上该属性的值； getName：获取属性名； getType：获取属性类型； getModifiers：获取修饰符。通过Field类提供的方法，我们可以在运行时动态地访问一个类的属性，例如：Person person = new Person();Field field = person.getClass().getDeclaredField(\"age\");field.setAccessible(true); int age = (int) field.get(person); field.set(person, age + 1);上述代码中，我们通过反射机制获取Person类中的age属性，并动态地修改该属性的值。注意，访问私有属性需要先调用setAccessible方法来设置可访问性。5、代码实践/** * 反射机制 四大核心类实践 实践 * * @author YKFire */public class ReflectionExample2 { public static void main(String[] args) throws Exception { //通过常用的类进行创建对象 反射机制主要包括的四个类：Class类 Constructor类 Method类 Field类 //加载类信息 Class&lt;?&gt; clazz = Class.forName(\"com.ykfire.node.utils.Person\"); //创建构造器对象 Constructor&lt;?&gt; constructor = clazz.getConstructor(String.class, int.class); //使用构造器对象创建实例 Object person = constructor.newInstance(\"Tom\", 20);// Person person1 = new Person(\"yk\",12); //输出对象 System.out.println(person); //调用方法 //使用clazz对象调用方法 获取对应的Method类对象 Method method = clazz.getMethod(\"sayHello\"); method.invoke(person); //获取方法的名字 System.out.println(method.getName()); //获取方法的返回类型 System.out.println(method.getReturnType()); //获取该类的所有公共方法 返回一个数组 Method[] methods = clazz.getMethods(); for (int i = 0; i &lt; methods.length; i++) { System.out.println(methods[i].getName()); } //访问属性 //使用clazz对象获取属性 获取对应的Field类对象 Field filed = clazz.getDeclaredField(\"age\"); //访问私有属性需要先调用setAccessible方法来设置可访问性 filed.setAccessible(true); int age = (int) filed.get(person); System.out.println(\"Age：\" + age); //获取该类的所有私有属性 返回一个数组 Field[] declaredFields = clazz.getDeclaredFields(); for (int i = 0; i &lt; declaredFields.length; i++) { System.out.println(declaredFields[i].getName()); } }}class Person { private String name; private int age; public Person() { } public Person(String name, int age) { this.name = name; this.age = age; } public void sayHello() { System.out.println(\"Hello, my name is \" + name); } public String toString() { return \"Person [name=\" + name + \", age=\" + age + \"]\"; }}输出结果如下：三、封装自己的反射工具类 经过上述对反射相关核心类的了解，以下便实现封装自己的反射工具类1、反射工具类​\t封装反射机制可以让我们更方便地使用反射，提高代码的可读性和可维护性。下面是一个简单的反射工具类的示例，可以用来获取类的属性、方法和构造方法等信息，以及创建对象和调用方法等操作：/** * 封装反射机制 * * @author YKFire */public class ReflectionUtils { /** * 获取指定类的构造方法 * @param clazz 指定类 * @param parameterTypes 参数类型数组 * @return 构造方法 */ public static Constructor&lt;?&gt; getConstructor(Class&lt;?&gt; clazz, Class&lt;?&gt;... parameterTypes) throws NoSuchMethodException { return clazz.getConstructor(parameterTypes); } /** * 获取指定类的属性 * @param clazz 指定类 * @param fieldName 属性名 * @return 属性 */ public static Field getField(Class&lt;?&gt; clazz, String fieldName) throws NoSuchFieldException { return clazz.getField(fieldName); } /** * 获取指定类的方法 * @param clazz 指定类 * @param methodName 方法名 * @param parameterTypes 参数类型数组 * @return 方法 */ public static Method getMethod(Class&lt;?&gt; clazz, String methodName, Class&lt;?&gt;... parameterTypes) throws NoSuchMethodException { return clazz.getMethod(methodName, parameterTypes); } /** * 无参构造(前提目标类要有无参构造方法) * 创建指定类的对象 * @param clazz 指定类 * @param args 参数数组 * @return 对象 */ public static Object newInstance(Class&lt;?&gt; clazz, Object... args) throws Exception { Constructor&lt;?&gt; constructor = clazz.getConstructor(); return constructor.newInstance(args); } /** * 有参构造 * 创建指定类的对象 * @param clazz 指定类 * @param parameterTypes 参数数组 * @param args 参数数组 * @return 对象 */ public static Object newInstance(Class&lt;?&gt; clazz, Class&lt;?&gt;[] parameterTypes, Object[] args) throws Exception { Constructor&lt;?&gt; constructor = clazz.getDeclaredConstructor(parameterTypes); return constructor.newInstance(args); } /** * 调用指定对象的方法 * @param obj 指定对象 * @param methodName 方法名 * @param args 参数数组 * @return 方法返回值 */ public static Object invokeMethod(Object obj, String methodName, Object... args) throws Exception { Class&lt;?&gt;[] parameterTypes = new Class&lt;?&gt;[args.length]; for (int i = 0; i &lt; args.length; i++) { parameterTypes[i] = args[i].getClass(); } Method method = obj.getClass().getMethod(methodName, parameterTypes); return method.invoke(obj, args); } /** * 设置指定对象的属性值 * @param obj 指定对象 * @param fieldName 属性名 * @param value 属性值 */ public static void setFieldValue(Object obj, String fieldName, Object value) throws Exception { Field field = obj.getClass().getDeclaredField(fieldName); //访问私有属性需要先调用setAccessible方法来设置可访问性 field.setAccessible(true); field.set(obj, value); } /** * 获取指定对象的属性值 * @param obj 指定对象 * @param fieldName 属性名 * @return 属性值 */ public static Object getFieldValue(Object obj, String fieldName) throws Exception { Field field = obj.getClass().getDeclaredField(fieldName); //访问私有属性需要先调用setAccessible方法来设置可访问性 field.setAccessible(true); return field.get(obj); }}2、工具类实践 以下代码中的Person类复用上一章节中的内容/** * 接下里使用封装的工具类 实现对应的功能 * * @author YKFire */public class ReflectionExample3 { public static void main(String[] args) throws Exception { Class&lt;?&gt;[] parameterTypes = {String.class, int.class}; Object[] arg = {\"Tom\", 20}; Person person = (Person) ReflectionUtils.newInstance(Person.class, parameterTypes ,arg); String name = (String) ReflectionUtils.getFieldValue(person, \"name\"); System.out.println(name); ReflectionUtils.setFieldValue(person, \"age\", 21); int age = (int) ReflectionUtils.getFieldValue(person, \"age\"); System.out.println(age); ReflectionUtils.invokeMethod(person, \"sayHello\", args); Method sayHello = ReflectionUtils.getMethod(Person.class, \"sayHello\"); System.out.println(sayHello.getName()); }}输出结果如下：" }, { "title": "AOP实践", "url": "/posts/aop/", "categories": "技术实践", "tags": "学习", "date": "2023-04-28 12:36:00 +0000", "snippet": " 背”八股“的时候，Spring的AOP特性是绕不过的坎，但纸上得来终觉浅，绝知此事要躬行。于是就打开idea编写了个小demo，体会也更加深刻了 本文记录下Spring框架中AOP特性的简单实践，权限认证与日志记录的应用一、简介​\tAOP（Aspect-Oriented Programming，面向切面编程）是一种编程范式，它旨在解决软件开发中的横切关注点（cross-cutting ...", "content": " 背”八股“的时候，Spring的AOP特性是绕不过的坎，但纸上得来终觉浅，绝知此事要躬行。于是就打开idea编写了个小demo，体会也更加深刻了 本文记录下Spring框架中AOP特性的简单实践，权限认证与日志记录的应用一、简介​\tAOP（Aspect-Oriented Programming，面向切面编程）是一种编程范式，它旨在解决软件开发中的横切关注点（cross-cutting concerns）问题。横切关注点是那些分布于多个模块或对象的功能，例如日志记录、安全检查、事务管理等。AOP通过将横切关注点与业务逻辑分离，从而提高了代码的模块化程度，使得开发更加简洁、易于维护。二、实现方式实现AOP的方式主要有以下几种： 动态代理：通过代理模式，为目标对象生成一个代理对象，然后在代理对象中实现横切关注点的织入。动态代理可以分为JDK动态代理（基于接口）和CGLIB动态代理（基于类）。 编译时织入：在编译阶段，通过修改字节码实现AOP。AspectJ的编译时织入就是这种方式。 类加载时织入：在类加载阶段，通过修改字节码实现AOP。AspectJ的加载时织入就是这种方式。三、应用场景​\tAOP（面向切面编程）是Spring框架的一个重要特性，它可以帮助我们将应用程序的业务逻辑与横切关注点分离开来，从而提高代码的可维护性和重用性。以下是AOP的几个应用场景： 日志记录：在应用程序中添加日志记录是一个很常见的需求，但是在每个方法中都添加日志记录代码会导致代码重复和臃肿。使用AOP可以将日志记录代码抽象为一个切面，在需要添加日志的方法上添加相应的切点，从而实现日志记录的功能。 安全检查：在应用程序中添加安全检查是一个很重要的需求，但是在每个方法中都添加安全检查代码会导致代码重复和臃肿。使用AOP可以将安全检查代码抽象为一个切面，在需要进行安全检查的方法上添加相应的切点，从而实现安全检查的功能。 性能监控：在应用程序中添加性能监控是一个很重要的需求，但是在每个方法中都添加性能监控代码会导致代码重复和臃肿。使用AOP可以将性能监控代码抽象为一个切面，在需要进行性能监控的方法上添加相应的切点，从而实现性能监控的功能。 事务管理：在应用程序中添加事务管理是一个很重要的需求，但是在每个方法中都添加事务管理代码会导致代码重复和臃肿。使用AOP可以将事务管理代码抽象为一个切面，在需要进行事务管理的方法上添加相应的切点，从而实现事务管理的功能。 面试时比较好的回答：开发者可以将一些通用代码，例如日志管理，事务等从业务中抽离出来，定义为切面，使其更易于管理，解耦合，提高了代码的可重用性。（我们可以将这些与业务逻辑无关的横切关注点（Cross-cutting Concerns）定义为切面（Aspect），并将它们织入到业务逻辑中，从而实现了业务逻辑与横切关注点的解耦）四、应用实践1、权限认证编写自定义注解 @AuthCheck/** * 权限校验 * * 该注解的使用： 在方法上标记 @AuthCheck() * * @author YKFire */@Target(ElementType.METHOD) //表示该注解可以标注在方法上@Retention(RetentionPolicy.RUNTIME) //表示该注解在运行时可以被保留，因为需要在运行时进行权限校验。public @interface AuthCheck { /** * 有任何一个角色 * * @return */ String[] anyRole() default \"\"; /** * 必须有某个角色 * * @return */ String mustRole() default \"\";}编写权限验证逻辑切面：/** * 权限校验 AOP * * @author YKFire */@Aspect //表示该类是一个切面，用于定义切点或拦截器@Componentpublic class AuthInterceptor { @Resource private UserService userService; /** * 执行拦截 * * @param joinPoint * @param authCheck * @return */ @Around(\"@annotation(authCheck)\") // 表示拦截标注有 AuthCheck 注解的方法，并且在方法执行前后进行拦截和处理 //在 @Around 方法中，可以调用 ProceedingJoinPoint 的 proceed() 方法来继续执行目标方法，也可以在此处抛出异常来中断目标方法的执行，或者改变目标方法的返回值等操作。 public Object doInterceptor(ProceedingJoinPoint joinPoint, AuthCheck authCheck) throws Throwable { //获取 anyRole 属性 并将其类型从 字符串数组 转换为 list集合 List&lt;String&gt; anyRole = Arrays.stream(authCheck.anyRole()).filter(StringUtils::isNotBlank).collect(Collectors.toList()); //获取 mustRole 属性 String mustRole = authCheck.mustRole(); //获取当前请求的属性 RequestAttributes requestAttributes = RequestContextHolder.currentRequestAttributes(); HttpServletRequest request = ((ServletRequestAttributes) requestAttributes).getRequest(); // 当前登录用户 User user = userService.getLoginUser(request); // 拥有任意权限即通过 if (CollectionUtils.isNotEmpty(anyRole)) { String userRole = user.getUserRole(); if (!anyRole.contains(userRole)) { throw new BusinessException(ErrorCode.NO_AUTH_ERROR); } } // 必须有对应权限才通过 如管理员 if (StringUtils.isNotBlank(mustRole)) { String userRole = user.getUserRole(); if (!mustRole.equals(userRole)) { throw new BusinessException(ErrorCode.NO_AUTH_ERROR); } } // 通过权限校验，放行 return joinPoint.proceed(); }}在对应的方法标记 @AuthCheck 即系统管理员才可以查看2、日志记录编写日志记录切面：/** * 请求响应日志 AOP * * @author YKFire **/@Aspect@Component@Slf4jpublic class LogInterceptor { /** * 执行拦截 * * 对 com.ykfire.project.controller 包下的所有方法进行拦截，并实现了对请求和响应日志的记录以及计时等操作 * * 代码中使用了 StopWatch 类来计时，记录请求的开始时间和结束时间 */ @Around(\"execution(* com.ykfire.project.controller.*.*(..))\") public Object doInterceptor(ProceedingJoinPoint point) throws Throwable { // 计时 StopWatch stopWatch = new StopWatch(); stopWatch.start(); // 获取请求路径 RequestAttributes requestAttributes = RequestContextHolder.currentRequestAttributes(); HttpServletRequest httpServletRequest = ((ServletRequestAttributes) requestAttributes).getRequest(); // 生成请求唯一 id String requestId = UUID.randomUUID().toString(); String url = httpServletRequest.getRequestURI(); // 获取请求参数 Object[] args = point.getArgs(); String reqParam = \"[\" + StringUtils.join(args, \", \") + \"]\"; // 输出请求日志 log.info(\"request start，id: {}, path: {}, ip: {}, params: {}\", requestId, url, httpServletRequest.getRemoteHost(), reqParam); // 执行原方法 Object result = point.proceed(); // 输出响应日志 stopWatch.stop(); long totalTimeMillis = stopWatch.getTotalTimeMillis(); log.info(\"request end, id: {}, cost: {}ms\", requestId, totalTimeMillis); return result; }}五、拓展1、Spring AOP 和 AspectJ AOP 有什么区别？Spring AOP和AspectJ AOP是两种不同的AOP实现： Spring AOP：是Spring框架中的AOP实现，基于动态代理实现。Spring AOP主要用于解决Spring容器中Bean的横切关注点问题。由于它使用了动态代理，所以只支持方法级别的切面（即横切关注点只能织入方法的执行）。Spring AOP的性能略逊于AspectJ，但对于大部分应用来说，性能影响不大。 AspectJ AOP：是一个独立的、功能更强大的AOP实现，不仅支持方法级别的切面，还支持字段、构造器等其他切面。AspectJ可以通过编译时织入（编译时修改字节码）或加载时织入（在类加载时修改字节码）的方式实现AOP。Spring可以与AspectJ结合使用，以提供更强大的AOP功能。2、AOP切面与过滤器的区别AOP切面和过滤器都是在程序运行时对指定的方法或对象进行拦截、处理或增强的技术。但是它们之间有以下几点区别： AOP切面是基于面向切面编程的思想，通过在指定的方法或对象的前、后、或者环绕执行时动态添加或删除一些操作，比如日志、事务、权限校验等。而过滤器则是基于面向对象编程的思想，通过在请求到达服务器之前或响应离开服务器之后进行拦截处理，实现类似于拦截器的功能。 AOP切面通常是针对整个应用程序的某个特定功能进行的，比如日志、安全、事务等，而过滤器通常是针对某个特定的Web资源进行的，比如对某个请求路径进行拦截。 AOP切面是通过代理机制实现的，而过滤器则是通过特定的Servlet API实现的。 AOP切面可以对多个方法或对象进行拦截处理，而过滤器只能对一个特定的Web资源进行拦截处理。 AOP切面可以对方法或对象进行增强操作，而过滤器只能对请求或响应进行修改或过滤操作。3、AOP切面与拦截器器的区别AOP切面和拦截器都是在程序运行时对指定的方法或对象进行拦截、处理或增强的技术。但是它们之间有以下几点区别： AOP切面是基于面向切面编程的思想，通过在指定的方法或对象的前、后、或者环绕执行时动态添加或删除一些操作，比如日志、事务、权限校验等。而拦截器则是基于面向对象编程的思想，通过在方法调用前后进行拦截处理，实现类似于过滤器的功能。 AOP切面通常是针对整个应用程序的某个特定功能进行的，比如日志、安全、事务等，而拦截器通常是针对某个具体的对象或方法进行的，比如对用户登录进行拦截。 AOP切面是通过代理机制实现的，而拦截器则是通过实现特定的接口或继承特定的类来实现的。 AOP切面可以对多个方法或对象进行拦截处理，而拦截器只能对一个方法或对象进行拦截处理。" }, { "title": "Java线程池详解", "url": "/posts/threadpool/", "categories": "技术科普", "tags": "学习", "date": "2023-04-22 11:59:00 +0000", "snippet": "Java线程池详解一、作用​\t线程池（ThreadPool）是⼀种基于池化思想管理和使用线程的机制。它是将多个线程预先存储在⼀个“池子”内，当有任务出现时可以避免重新创建和销毁线程所带来性能开销，只需要从“池子”内取出相应的线程执行对应的任务即可。线程我们可以使用 new 的方式去创建，但如果并发的线程很多，每个线程执行的时间又不长，这样频繁的创建线程会大大的降低系统处理的效率，因为创建和销...", "content": "Java线程池详解一、作用​\t线程池（ThreadPool）是⼀种基于池化思想管理和使用线程的机制。它是将多个线程预先存储在⼀个“池子”内，当有任务出现时可以避免重新创建和销毁线程所带来性能开销，只需要从“池子”内取出相应的线程执行对应的任务即可。线程我们可以使用 new 的方式去创建，但如果并发的线程很多，每个线程执行的时间又不长，这样频繁的创建线程会大大的降低系统处理的效率，因为创建和销毁进程都需要消耗资源，线程池就是用来解决类似问题。线程池实现了一个线程在执行完一段任务后，不销毁，继续执行下一段任务。用《Java并发编程艺术》提到线程池的优点： 降低资源的消耗：使得线程可以重复使用，不需要在创建线程和销毁线程上浪费资源 提高响应速度：任务到达时，线程可以不需要创建即可以执行 线程的可管理性：线程是稀缺资源，如果无限制的创建会严重影响系统效率，线程池可以对线程进行管理、监控、调优。 Java线程池的核心作用：通过复用线程，避免了线程创建和销毁的开销，提高了程序的性能和可靠性二、组成部分 线程池管理器：用于创建并管理线程池 工作线程：线程池中的线程 任务接口：每个任务都必须实现的接口，用于工作线程调度其运行 任务队列：用于存放待处理的任务，提供一种缓冲机制三、实现原理及使用​\tjava线程池是通过Executor框架实现的，该框架中用到了 Executor，ExecutorsExecutorService，ThreadPoolExecutor ，Callable 和 Future、 FutureTask 这几个类 主要是通过ThreadPoolExecutor类来实现 下面详细介绍Java线程池的使用方法与注意事项1.创建线程池创建线程池时，需要指定线程池的核心线程数、最大线程数、线程空闲时间、任务队列、拒绝策略等参数。例如：ThreadPoolExecutor executor = new ThreadPoolExecutor( corePoolSize, // 核心线程数 maximumPoolSize, // 最大线程数 keepAliveTime, // 线程空闲时间 TimeUnit.SECONDS, // 时间单位 new LinkedBlockingQueue&lt;Runnable&gt;() // 任务队列 handler //拒绝策略);2.提交任务将任务提交给线程池时，可以调用execute()方法或submit()方法。例如：executor.execute(new Runnable() { @Override public void run() { // 执行任务 }});Future&lt;?&gt; future = executor.submit(new Callable&lt;Object&gt;() { @Override public Object call() throws Exception { // 执行任务，返回结果 return null; }});3.线程池状态线程池会检查自身的状态，如果状态不是RUNNING，那么线程池会拒绝任务，或者直接抛出异常。线程池的状态有以下几种： RUNNING：线程池正在运行，可以接收任务。 SHUTDOWN：线程池正在关闭，不再接收新任务，但会执行已提交的任务。 STOP：线程池正在关闭，不再接收新任务，也不会执行已提交的任务。 TIDYING：线程池正在清理任务队列。 TERMINATED：线程池已经关闭，所有任务已经执行完毕。4.任务队列如果线程池中的线程都在执行任务，新的任务就会被放入任务队列中，等待线程空闲后再执行。常用的任务队列有以下几种： ArrayBlockingQueue：基于数组的有界队列，按照FIFO原则对任务进行排序。 LinkedBlockingQueue：基于链表的无界队列，按照FIFO原则对任务进行排序。 SynchronousQueue：不存储任务的阻塞队列，将任务直接交给线程执行。5.线程池管理线程线程池会根据任务数量和线程池状态来管理线程，如果任务数量超过了线程池的最大线程数，新的任务就会被拒绝或者等待。线程池管理线程的方法有以下几种： prestartCoreThread()：预启动一个核心线程。 prestartAllCoreThreads()：预启动所有核心线程。 allowCoreThreadTimeOut(boolean)：设置核心线程是否允许超时回收。 setMaximumPoolSize(int)：设置线程池的最大线程数。 setKeepAliveTime(long, TimeUnit)：设置线程空闲时间。 setRejectedExecutionHandler(RejectedExecutionHandler)：设置任务拒绝策略。6.关闭线程池当不再需要线程池时，可以调用shutdown()方法关闭线程池，线程池会拒绝新任务，并等待已提交的任务执行完成。如果希望立即关闭线程池，可以调用shutdownNow()方法。例如：executor.shutdown();7.清理线程池当所有任务执行完成后，线程池会清理所有线程，并释放资源。如果希望等待所有任务执行完成后再关闭线程池，可以调用awaitTermination()方法。例如：executor.awaitTermination(Long.MAX_VALUE, TimeUnit.NANOSECONDS);​\t总的来说，Java线程池通过复用线程，避免了线程创建和销毁的开销，提高了程序的性能和可靠性。但是，在使用Java线程池时，需要注意避免线程安全问题，并合理设置线程池的参数，以达到最优的性能和可靠性。四、工作流程 Java线程池是一种并发编程技术，它允许开发者将多个任务分配给一组线程，从而提高程序的性能和可靠性。1.创建线程池后，开始等待请求2.当调用execute()方法添加一个请求任务时，线程池会做以下判断： 如果正在运行的线程数量小于corePoolSize，马上创建线程执行任务 如果正在运行的线程数量大于等于corePoolSize，将该任务放入等待队列 如果等待队列已满，但正在运行线程数量小于max，创建非核心线程执行任务 如果队列满了且正在运行的线程数量大于max，线程池会启动饱和拒绝策略3.当一个线程完成任务时，会从等待队列中取下一个任务来执行4.当空闲线程超过keepAliveTime定义时间，会判断： 如果当前运行线程大于corePoolSize，该线程销毁 所有线程执行完任务后，线程个数恢复到corePoolSize大小图解：五、拒绝策略​\t线程池中的线程已经用完了，无法继续为新任务服务，同时，等待队列也已经排满了，再也塞不下新任务了。这时候我们就需要拒绝策略机制合理的处理这个问题。JDK 内置的拒绝策略如下: AbortPolicy : 直接抛出异常，阻止系统正常运行 CallerRunsPolicy: 只要线程池未关闭，该策略直接在调用者线程中，运行当前被丢弃的任务。显然这样做不会真的丢弃任务，但是，任务提交线程的性能极有可能会急剧下降 DiscardOldestPolicy : 丢弃最老的一个请求，也就是即将被执行的一个任务，并尝试再次提交当前任务。 DiscardPolicy :该策略默默地丢弃无法处理的任务，不予任何处理。如果允许任务丢失，这是最好的一种方案​\t若以上策略仍无法满足实际以上内置拒策略均实现了 ReiectedExecutionHandler 接需要，完全可以自己扩展 ReiectedExecutionHandler 接口" }, { "title": "Docker Command", "url": "/posts/DockerCommand/", "categories": "实用工具", "tags": "学习", "date": "2023-04-13 09:03:00 +0000", "snippet": "Docker Command 最近在整理自己开发过的项目，同时也把项目都部署上线下。把好久没用的docker复习了遍，于是想写篇博客记录下部署过程中常用的命令文件以及需要了解的定义，以免时间一长又忘了~一、常用命令镜像操作 docker images：查看镜像 docker rmi：删除镜像 docker pull：从服务器拉取镜像 docker push：推送镜像到服务器 do...", "content": "Docker Command 最近在整理自己开发过的项目，同时也把项目都部署上线下。把好久没用的docker复习了遍，于是想写篇博客记录下部署过程中常用的命令文件以及需要了解的定义，以免时间一长又忘了~一、常用命令镜像操作 docker images：查看镜像 docker rmi：删除镜像 docker pull：从服务器拉取镜像 docker push：推送镜像到服务器 docker save：保存镜像为一个压缩包 docker load：加载压缩包为镜像 docker build：构建镜像容器操作 容器的三个状态 运行：进程正常运行 暂停：进程暂停，CPU不再运行，并不释放内存 停止：进程终止，回收进程占用的内存、CPU等资源- docker run：创建并运行一个容器，处于运行状态 docker pause：让一个运行的容器暂停 docker unpause：让一个容器从暂停状态恢复运行 docker stop：停止一个运行的容器 docker start：让一个停止的容器再次运行 docker rm：删除一个容器docker compose操作 docker-compose up：执行docker-compose.yml文件 前台启动 (-d 后台启动) docker-compose logs：输出日志 docker-compose ps：列出工程中正在运行的容器 (-a 所有容器） docker-compose exec nginx bash：进入工程中指定服务的容器 docker-compose restart：重启工程中所有服务的容器 docker-compose start：启动工程中所有服务的容器 docker-compose stop：停止工程中所有服务的容器 docker-compose rm：删除工程中所有服务的容器 docker-compose images：打印所有服务的容器所对应的镜像二、Dockerfile​\tDockerfile 是一个文本文件，其内包含了一条条的指令(Instruction)，用于构建镜像。每一条指令构建一层镜像，因此每一条指令的内容，就是描述该层镜像应当如何构建​\t一般搭配docker build命令使用后端模板FROM maven:3.5-jdk-8-alpine as builder# Copy local code to the container image.WORKDIR /appCOPY ./yupao-backend-0.0.1-SNAPSHOT.jar ./yupao-backend-0.0.1-SNAPSHOT.jarEXPOSE 8080# Run the web service on container startup.CMD [\"java\",\"-jar\",\"/app/target/yupao-backend-0.0.1-SNAPSHOT.jar\",\"--spring.profiles.active=prod\"]前端模板 前端编写Dockerfile文件，还需要nginx配置文件 进行代理FROM nginxWORKDIR /usr/share/nginx/html/USER rootCOPY ./nginx.conf /etc/nginx/conf.d/default.confCOPY ./dist /usr/share/nginx/html/EXPOSE 80CMD [\"nginx\", \"-g\", \"daemon off;\"]server { listen 80; # gzip config gzip on; gzip_min_length 1k; gzip_comp_level 9; gzip_types text/plain text/css text/javascript application/json application/javascript application/x-javascript application/xml; gzip_vary on; gzip_disable \"MSIE [1-6]\\.\"; root /usr/share/nginx/html; include /etc/nginx/mime.types; location / { try_files $uri /index.html; }}构建镜像 t：打上版本好 Dockerfile文件应该位于当前命令执行的命令下docker build -t 项目名:v0.0.1 .创建容器 前面为宿主端口 后面为容器内端口docker run -p 80:80 -d xxx:v0.0.1docker run -p 8081:8080 -d xxx:v0.0.1三、docker-compose.yml​\tDocker Compose是一个用于定义和运行多容器应用程序的工具。 通过compose，我们可以使用yaml文件来配置应用程序的服务，然后使用一个命令来创建和启动所有已配置的服务。​\t主要用于一键部署所有的容器，当你的项目存在多个容器时，使用docker compose可以将创建每个容器并执行的命令整合起来，以及定义网络和数据集的挂载，甚至连构建镜像的过程也可以一起包含，可以简化大量重复的手动工作。 ​\tdocker-compose.yml文件是一个定义服务、 网络和卷的 YAML 文件 。Compose 文件的默认路径是 ./docker-compose.yml模板version: '3.9'services: web-ui: image: partner-dating-ui:v0.0.1 restart: always container_name: part-ui ports: - \"80:80\" web: image: partner-dating:v0.0.2 restart: always container_name: part ports: - \"8081:8080\" 更多关于docker-compose.yml文件的解析与使用：https://blog.csdn.net/qq_37768368/article/details/120583141" }, { "title": "ORM框架的选择", "url": "/posts/ORM/", "categories": "技术科普", "tags": "学习", "date": "2023-04-08 15:07:00 +0000", "snippet": "ORM框架的选择ORM：是一种为了解决面向对象与关系型数据库中数据类型不匹配的技术，它通过描述Java对象与数据库表之间的映射关系，自动将java应用程序中的对象持久化到关系型数据库的表中；使用ORM框架后，应用程序不再直接访问底层数据库，而是以面向对象的方式来操作持久化对象，而ORM框架则会通过映射关系将这些面向对象的操作转换成底层的SQL操作。 人们常用且熟悉的ORM莫过于mybati...", "content": "ORM框架的选择ORM：是一种为了解决面向对象与关系型数据库中数据类型不匹配的技术，它通过描述Java对象与数据库表之间的映射关系，自动将java应用程序中的对象持久化到关系型数据库的表中；使用ORM框架后，应用程序不再直接访问底层数据库，而是以面向对象的方式来操作持久化对象，而ORM框架则会通过映射关系将这些面向对象的操作转换成底层的SQL操作。 人们常用且熟悉的ORM莫过于mybatis与hibernate，接下来将讲述两者之间的区别以及各自的应用场景。 mybatis与hibernate两者本身并没有什么优劣之分，重要的还是取决于具体的应用场景与需求。1、概述Hibernate：是一个全表映射的框架。通常开发者只需定义好持久化对象到数据库表的映射关系，就可以通过Hibernate提供的方法完成持久层操作。开发者并不需要熟悉地掌握SQL语句的编写，Hibernate会根据制定的存储逻辑，自动的生成对应的SQL，并调用JDBC接口来执行，所以其开发效率会高于Mybatis。然而Hibernate自身也存在着一些缺点，例如它在多表关联时，对SQL查询的支持较差；更新数据时，需要发送所有字段；不支持存储过程；不能通过优化SQL来优化性能等。这些问题导致其只适合在场景不太复杂且对性能要求不高的项目中使用Mybatis：是一个半自动映射的框架。这里所谓的”半自动”是相对于Hibernate全表映射而言，Mybatis需要手动匹配提供POJO、SQL和映射关系，而Hibernate只需要POJO和映射关系即可。与Hibernate相比，虽然使用Mybatis手动编写SQL要比使用Hibernate的工作量大，但Mybatis可以配置动态SQL并优化SQL，可以通过配置决定SQL的映射规则，它还支持存储过程等。对于一些复杂的和需要优化性能的项目来说，显然使用Mybatis更加合适2、各自的特点Mybatis技术特点：1、 通过直接编写SQL语句，可以直接对SQL进行性能的优化；2、 学习门槛低，学习成本低。只要有SQL基础，就可以学习mybatis，而且很容易上手；3、 由于直接编写SQL语句，所以灵活多变，代码维护性更好。4、 不能支持数据库无关性，即数据库发生变更，要写多套代码进行支持，移植性不好。5、 需要编写结果映射。Hibernate技术特点：1、 标准的orm框架，程序员不需要编写SQL语句。2、 具有良好的数据库无关性，即数据库发生变化的话，代码无需再次编写。3、 学习门槛高，需要对数据关系模型有良好的基础，而且在设置OR映射的时候，需要考虑好性能和对象模型的权衡。4、 程序员不能自主的去进行SQL性能优化。3、区别 hibernate是全自动的，封装完整，而mybatis是半自动的，灵活更高 hibernate和mybatis一样都是orm数据库框架，但二者还是有很大区别的，hibernate完全可以通过对象关系模型实现对数据库的操作，拥有完整的JavaBean对象与数据库的映射结构来自动生成sql，而mybatis仅有基本的字段映射，对象数据以及对象实际关系任然需要通过手写sql来实现和管理 hibernate的数据库移植性远大于mybatis hibernate通过他强大的映射结构和hql语言，大大降低了对象与数据库（oracle和mysql）的耦合性，而mybatis由于需要手写sql,因此与数据库的耦合性直接取决于程序员些sql的方法，如果sql不具通用性而用了很多某数据库特有的sql语句的话，移植性也会随之降低很多，成本很高 hibernate拥有完整的日志系统，mybatis则欠缺一些 hibernate日志系统非常健全，涉及广泛，包括：sql记录、关系异常、优化警告、缓存提示、脏数据警告等；而mybatis除了基本记录功能外，功能薄弱很多 mybatis可以自主进行SQL性能优化，而hibernate不行 hibernate配置要比mybatis复杂的多，学习成本也比mybatis高，但也正因为mybatis使用简单，才导致它要比hibernate关系很多技术细节。mybatis由于不用考虑很多细节，开发模式上与传统jdbc区别很小，因此很容易上手并开发项目但忽略细节会导致前期bug比较多，因而开发出相对稳定的软件很慢，而开发软件却很快。Hibernate则正好与之相反。但是如果使用hibernate很熟练的话，实际上开发效率丝毫不差于甚至超越mybatis。 4、应用场景 mybatis适用于需求多变的互联网项目 hibernate适用于需求明确、业务固定的项目 如：OA、ERP项目 OA项目是指办公自动化项目，它帮助企业处理内部事务，例如公文、内部沟通、协同办公、员工请假、审批流程等 ERP项目是指企业资源计划项目，它主要管理原材料生产制造的软件，如员工、生产、制造、财务、销售、采购、仓储、分销、质量等等5、总结mybatis：小巧、方便、高效、简单、直接、半自动hibernate：强大、方便、高效、复杂、绕弯子、全自动mybatis： 入门简单，即学即用，提供了数据库查询的自动对象绑定功能，而且延续了很好的SQL使用经验，对于没有那么高的对象模型要求的项目来说，相当完美。 可以进行更为细致的SQL优化，可以减少查询字段。 缺点就是框架还是比较简陋，功能尚有缺失，虽然简化了数据绑定代码，但是整个底层数据库查询实际还是要自己写的，工作量也比较大，而且不太容易适应快速数据库修改。 mybatis-plus插件提供了一些基础通用的api，很好的解决了这项问题 二级缓存机制不佳。 hibernate： 功能强大，数据库无关性好，O/R映射能力强，如果你对Hibernate相当精通，而且对Hibernate进行了适当的封装，那么你的项目整个持久层代码会相当简单，需要写的代码很少，开发速度很快，非常爽。 有更好的二级缓存机制，可以使用第三方缓存。 缺点就是学习门槛不低，要精通门槛更高，而且怎么设计O/R映射，在性能和对象模型之间如何权衡取得平衡，以及怎样用好Hibernate方面需要你的经验和能力都很强才行。举个形象的比喻：mybatis：机械工具，使用方便，拿来就用，但工作还是要自己来作，不过工具是活的，怎么使由我决定。﻿﻿hibernate：智能机器人，但研发它（学习、熟练度）的成本很高，工作都可以摆脱他了，但仅限于它能做的事" }, { "title": "Java优先队列PriorityQueue", "url": "/posts/PriorityQueue/", "categories": "数据结构", "tags": "学习", "date": "2023-04-01 12:54:00 +0000", "snippet": "Java优先队列PriorityQueue一、概述​\tJava中PriorityQueue 实现的是Queue 接口，可以使用Queue的方法和自定义方法；其通过完全二叉树构造的小顶堆实现（任意一个非叶子节点的权值，都不大于其左右子节点的权值）。对于可比较的元素（natural ordering）直接进行比较；对于自定义类的比较，通过构造时传入的比较。 优先队列的默认空间大小为11​\t优先...", "content": "Java优先队列PriorityQueue一、概述​\tJava中PriorityQueue 实现的是Queue 接口，可以使用Queue的方法和自定义方法；其通过完全二叉树构造的小顶堆实现（任意一个非叶子节点的权值，都不大于其左右子节点的权值）。对于可比较的元素（natural ordering）直接进行比较；对于自定义类的比较，通过构造时传入的比较。 优先队列的默认空间大小为11​\t优先队列的作用是能保证每次取出的元素都是队列中权值最小的。这里牵涉到了大小关系，元素大小的评判可以通过元素本身的自然顺序（natural ordering），也可以通过构造时传入的比较（Comparator，类似于C++的仿函数）二、接口实现PriorityQueue 实现的是 Queue 接口 ，可以使用 Queue 提供的方法，以及自带的方法三、常用方法public boolean add(E e); //在队尾插入元素，插入失败时抛出异常，并调整堆结构public boolean offer(E e); //在队尾插入元素，插入失败时抛出false，并调整堆结构public E remove(); //获取队头元素并删除，并返回，失败时前者抛出异常，再调整堆结构public E poll(); //获取队头元素并删除，并返回，失败时前者抛出null，再调整堆结构public E element(); //返回队头元素（不删除），失败时前者抛出异常public E peek()；//返回队头元素（不删除），失败时前者抛出nullpublic boolean isEmpty(); //判断队列是否为空public int size(); //获取队列中元素个数public void clear(); //清空队列public boolean contains(Object o); //判断队列中是否包含指定元素（从队头到队尾遍历）public Iterator&lt;E&gt; iterator(); //迭代器四、堆结构调整每次插入或删除元素后，都对队列进行调整，使得队列始终构成最小/大堆。具体调整如下： 插入元素后，从堆底到堆顶调整堆； 删除元素后，将队尾元素复制到队头，并从堆顶到堆底调整堆。小根堆结构调整插入（ add()和offer()方法 ）元素后，向上调整堆：//siftUp()private void siftUp(int k, E x) { while (k &gt; 0) { int parent = (k - 1) &gt;&gt;&gt; 1;//parentNo = (nodeNo-1)/2 Object e = queue[parent]; if (comparator.compare(x, (E) e) &gt;= 0)//调用比较器的比较方法 break; queue[k] = e; k = parent; } queue[k] = x;}删除（ remove()和poll()方法 ）元素后，向下调整堆：//siftDown()private void siftDown(int k, E x) { int half = size &gt;&gt;&gt; 1; while (k &lt; half) { //首先找到左右孩子中较小的那个，记录到c里，并用child记录其下标 int child = (k &lt;&lt; 1) + 1;//leftNo = parentNo*2+1 Object c = queue[child]; int right = child + 1; if (right &lt; size &amp;&amp; comparator.compare((E) c, (E) queue[right]) &gt; 0) c = queue[child = right]; if (comparator.compare(x, (E) c) &lt;= 0) break; queue[k] = c;//然后用c取代原来的值 k = child; } queue[k] = x;}五、自定义类的比较PriorityQueue采用：Comparble和Comparator两种方式：1、实现Comparble接口该方法是默认的内部比较方式，如果用户插入自定义类型对象时，该类对象必须要实现Comparble接口，并重载compareTo方法import java.util.Comparator;import java.util.PriorityQueue;public class CompTest { class Comp implements Comparable&lt;Comp&gt; { int val; ListNode ptr; Comp(int val, ListNode ptr) { this.val = val; this.ptr = ptr; } public int compareTo(Comp b) { return this.val - b.val; } } public static void main(String[] args) { PriorityQueue&lt;Comp&gt; queue = new PriorityQueue&lt;Comp&gt;(); }}2、实现Comparator接口实现Comparator比较器并重载compare方法：import java.util.Comparator;import java.util.PriorityQueue;public class CompTest { public static void main(String[] args) { PriorityQueue&lt;ListNode&gt; queue = new PriorityQueue&lt;&gt;(new Comparator&lt;ListNode&gt;() { @Override public int compare(ListNode o1, ListNode o2) { return o1.val-o2.val; } }); } }} 由于源码中新入队元素x是在第1个参数的位置，因此最大/最小优先队列主要根据第1个参数的大小关系来判断 可以直接使用下面方法进行实现 最小堆：return o1.compareTo(o2); /return o1-02; (队头为最小值,后面乱序) 最大堆：return o2.compareTo(o1); /return o2-o1; (队头为最大值,后面乱序)六、应用场景前K个高频元素 给定一个整数数组nums和一个整数k，请返回其中出现频率前k高的元素，可以接受任意顺序返回元素//本题采用数组代替hashmappublic class TopK { public int[] topKFrequent(int[] nums, int k) { //创建hashmap用于记录数组每个数出现的次数 HashMap&lt;Integer,Integer&gt; map = new HashMap&lt;Integer,Integer&gt;(); //遍历数组 记录次数 for (int num : nums) { map.put(num,map.getOrDefault(num,0)+1); //对不存在的值提供默认值 } //创建优先队列构建小顶堆 存储数组 int[] 的第一个元素代表数组的值，第二个元素代表了该值出现的次数 //这里创建了一个匿名内部类作为比较器，实现了 Comparator 接口中的 compare 方法。 // 在 compare 方法中，根据两个元素的第二个元素（索引为 1）的大小关系来进行比较， // 返回一个负数表示 m 的第二个元素小于 n 的第二个元素，返回 0 表示 m 的第二个元素 // 等于 n 的第二个元素，返回一个正数表示 m 的第二个元素大于 n 的第二个元素。 // 这样，优先队列中队首元素的第二个元素就是最小的。 PriorityQueue&lt;int[]&gt; queue = new PriorityQueue&lt;&gt;(new Comparator&lt;int[]&gt;() { public int compare(int[] m, int[] n) { return m[1] - n[1]; } }); //对一个 Map&lt;Integer, Integer&gt; 对象中的每个键值对进行遍历操作 for (Map.Entry&lt;Integer, Integer&gt; entry : map.entrySet()) { //获取map的键和值 键代表数组内的数 值代表其出现的次数 int num = entry.getKey(), count = entry.getValue(); //如果队列内的元素等于k，那么就需要判断当前元素是否可以加入队列 if (queue.size() == k){ if (queue.peek()[1] &lt; count){ queue.poll(); queue.offer(new int[]{num,count}); } } //否则 直接加入队列 else{ queue.offer(new int[]{num,count}); } } //创建结果数组 int[] res = new int[k]; for (int i = 0; i &lt; k; i++) { res[i] = queue.poll()[0]; } return res; }}" }, { "title": "内网穿透软件ZeroTier", "url": "/posts/ZeroTier/", "categories": "实用工具", "tags": "学习", "date": "2023-03-25 09:19:00 +0000", "snippet": "内网穿透软件ZeroTier 在本学期云计算开发课程中，导师要求我们完成一个实验：如何在不访问公网ip的情况下完成服务之间的访问与调用思来想去，决定使用内网穿透技术将各主机都加入到一个局域网中已实现实验。本文将介绍一个内网穿透软件ZeroTier一、定义​\tZeroTier是一款用于构建异地虚拟局域网的工具，通过网页后台创建虚拟网络并进行管理，然后通过电脑上的ZeroTier客户端连接各个...", "content": "内网穿透软件ZeroTier 在本学期云计算开发课程中，导师要求我们完成一个实验：如何在不访问公网ip的情况下完成服务之间的访问与调用思来想去，决定使用内网穿透技术将各主机都加入到一个局域网中已实现实验。本文将介绍一个内网穿透软件ZeroTier一、定义​\tZeroTier是一款用于构建异地虚拟局域网的工具，通过网页后台创建虚拟网络并进行管理，然后通过电脑上的ZeroTier客户端连接各个异地电脑到虚拟局域网，从而实现组网。简单来说，ZeroTier 是一款简单易用的，在任何有网络的地方，都可以连回家里、公司、学校，而无需公网 IP 和复杂端口转发配置，基于 p2p 技术的内网穿透工具。 更形象一点的说法是，ZeroTier 就像虚拟的交换机，所有 ZeroTier 的客户端节点都可以互访互通。二、创建客户端到达 ZeroTier官网 进行创建账号，建议直接使用github账号进行登陆点击按钮直接创建网络保持默认设置，查看Members模块，授权其他主机加入局域网，加入后的各主机可以得到一个Managed IP，在局域网内的主机可以通过该id进行互相访问三、安装1、Linux系统安装1、安装zerotier软件(如未安装curl工具请先安装)curl -s https://install.zerotier.com| sudo bash 2、添加开机自启 $ sudo systemctl enable zerotier-one.service 3、启动zerotier-one.service $ sudo systemctl start zerotier-one.service 4、加入网络 $ sudo zerotier-cli join ID2、windows安装根据官网指示进行安装安装：Download – ZeroTier同样安装后加入network如果是windows系统，会提示你让选择是否开启网络发现，一定要选择是：四、Zerotier的优点 节点之间属于 P2P UDP 直连，无需服务器中转流量，互联速度仅仅取决于你的和其他节点的直连上传带宽（当然握手之初是需要经过中心服务器来当媒婆的，之后就是UDP直连了！）； 节点之间的流量是加密和压缩的，所以有带宽放大的功能； 配置简单，只需要填入一个 Network ID ，然后管理员在 Zerotier 官网管理页面允许通过一次，以后就直接允许连上了。 连上后各个虚拟网卡相当于同一局域网内，无应用的限制了——至于有哪些应用场景就需要读者你自己脑洞大开了。 关键的一点，只要你想要的连接的节点在25以内，所有功能都是免费的！" }, { "title": "Java8新特性", "url": "/posts/Java8/", "categories": "学习笔记", "tags": "学习", "date": "2023-03-19 10:53:00 +0000", "snippet": "Java8新特性 以下是笔者学习时做的笔记，过了段时间重新复习了下，并决定将其整理好并发布一、lambda表达式1、作用Lambda 表达式，也可称为闭包，它是推动 Java 8 发布的最重要新特性。Lambda 允许把函数作为一个方法的参数（函数作为参数传递进方法中）。使用 Lambda 表达式可以使代码变的更加简洁紧凑。2、基本语法Lambda 表达式由箭头 “-&gt;” 分为两部分...", "content": "Java8新特性 以下是笔者学习时做的笔记，过了段时间重新复习了下，并决定将其整理好并发布一、lambda表达式1、作用Lambda 表达式，也可称为闭包，它是推动 Java 8 发布的最重要新特性。Lambda 允许把函数作为一个方法的参数（函数作为参数传递进方法中）。使用 Lambda 表达式可以使代码变的更加简洁紧凑。2、基本语法Lambda 表达式由箭头 “-&gt;” 分为两部分。箭头左侧是参数列表，可以是空的或非空的；箭头右侧是方法体，可以是表达式或代码块。3、函数式接口lambda 表达式只能用于函数式接口，函数式接口是指只包含一个抽象方法的接口。函数式接口的定义：在Java 8中，函数式接口是指仅具有单个抽象方法的接口。它是使用Lambda表达式和方法引用的基础。Lambda表达式可以被解释为实现函数式接口的实例。因此，函数式接口提供了一种简单的方法来实现代码组件的传递和消费，而不需要定义实现类。在Java中，函数式接口的标志是使用@FunctionalInterface注解。这个注解可以让编译器检查接口是否确实只包含一个抽象方法。如果接口有多个抽象方法或者没有任何抽象方法，编译器将会报错。4、Lambda 表达式的多行代码块Lambda 表达式可以包含多行代码块，如果方法体是多行代码块，则需要使用花括号将它们括起来。例如，以下是一个带有多行代码块的 Lambda 表达式：(int x, int y) -&gt; { int sum = x + y; System.out.println(“The sum of “ + x + “ and “ + y + “ is “ + sum); return sum; }5、变量作用域lambda 表达式只能引用标记了 final 的外层局部变量，这就是说不能在 lambda 内部修改定义在域外的局部变量，否则会编译错误。6、重要特征 可选类型声明：不需要声明参数类型，编译器可以统一识别参数值。 可选的参数圆括号：一个参数无需定义圆括号，但多个参数需要定义圆括号。 可选的大括号：如果主体包含了一个语句，就不需要使用大括号。 可选的返回关键字：如果主体只有一个表达式返回值则编译器会自动返回值，大括号需要指定表达式返回了一个数值。总结Lambda 表达式是 Java 8 中最重要的新特性之一，它可以简化代码，并提高代码的可读性和可维护性。掌握 Lambda 表达式的使用方法，有助于提高 Java 编程技能。示例代码如下：import java.util.function.BiFunction;import java.util.function.IntBinaryOperator;public class LambdaTest { public static void main(String[] args) { //最基本的lambda表达式：箭头左侧是参数列表，可以是空的或非空的；箭头右侧是方法体，可以是表达式或代码块 //1、不带参数的lambda表达式 Runnable runnable = () -&gt; System.out.println(\"今天晚上吃了猪杂饭\"); //lambda 表达式只能用于函数式接口，函数式接口是指只包含一个抽象方法的接口 runnable.run(); //2、带参数lambda表达式 BiFunction&lt;Integer, Integer, Integer&gt; add = (x, y) -&gt; x+y; //lambda 表达式只能用于函数式接口，函数式接口是指只包含一个抽象方法的接口 Integer result = add.apply(1,2); System.out.println(result); //3、Lambda 表达式的多行代码块 AddResult addResult = (int x, int y) -&gt; { int sum = x + y; System.out.println(\"The sum of \" + x + \" and \" + y + \" is \" + sum); return sum; }; int i = addResult.is(1,2); System.out.println(i); }}interface AddResult{ int is (int x,int y);}二、方法引用1、作用在Java 8中，方法引用是一种用来简化Lambda表达式的语法的新特性。它可以让你直接引用一个已经存在的方法，而不是通过Lambda表达式来定义一个新的方法体。方法引用是一种更加简洁、易读、易维护的代码风格，可以使得代码更加简单、清晰。即方法引用通过方法的名字来指向一个方法。方法引用可以使语言的构造更紧凑简洁，减少冗余代码。方法引用使用一对冒号 :: 。2、方法引用的四种形式 静态方法引用：类名::staticMethodName(静态方法名) 实例方法引用：实例对象名::instanceMethodName(即实例对象方法名) 构造方法引用：类名::new 数组引用：类型[]::new方法引用的具体使用取决于需要传递的参数类型和Lambda表达式中需要执行的代码。对于一些简单的情况，方法引用可以取代Lambda表达式，从而使代码更加简洁易读。示例代码import java.util.ArrayList;import java.util.List;import java.util.function.BiFunction;import java.util.function.Function;import java.util.function.Supplier;public class MethodReference { public static void main(String[] args) { //1、静态方法引用 //lambda表达式// Function&lt;String , Integer&gt; toInteger = (s) -&gt; Integer.parseInt(s); //方法引用 Function&lt;String , Integer&gt; toInteger = Integer::parseInt; //字符串转化为整型 Integer a = toInteger.apply(\"6\"); System.out.println(a); //2、实例方法引用 //Lambda表达式// BiFunction&lt;String, String, Boolean&gt; startsWith = (s1, s2) -&gt; s1.startsWith(s2); //字符串比较 //方法引用 BiFunction&lt;String, String, Boolean&gt; startsWith = String::startsWith; Boolean apply = startsWith.apply(\"a\", \"a\"); System.out.println(apply); //3、构造方法引用 //Lambda表达式// Supplier&lt;List&lt;String&gt;&gt; listSupplier = () -&gt; new ArrayList&lt;&gt;(); //方法引用 Supplier&lt;List&lt;String&gt;&gt; listSupplier = ArrayList::new; List&lt;String&gt; list = listSupplier.get(); list.add(\"1\"); list.add(\"3\"); System.out.println(list); //4、数组引用 //Lambda表达式// Function&lt;Integer, int[]&gt; arrayCreator = size -&gt; new int[size]; //方法引用 Function&lt;Integer, int[]&gt; arrayCreator = int[]::new; int[] array = arrayCreator.apply(6); System.out.println(array.length); //方法引用方式输出list集合内的元素 list.forEach(System.out::println); }}三、函数式接口1、定义函数式接口(Functional Interface)就是一个有且仅有一个抽象方法，但是可以有多个非抽象方法的接口。函数式接口可以被隐式转换为 lambda 表达式。Lambda 表达式和方法引用（实际上也可认为是Lambda表达式）上。注：在Java中，函数式接口的标志是使用@FunctionalInterface注解。这个注解可以让编译器检查接口是否确实只包含一个抽象方法。如果接口有多个抽象方法或者没有任何抽象方法，编译器将会报错。2、java函数式接口 1 BiConsumer代表了一个接受两个输入参数的操作，并且不返回任何结果 2 BiFunction代表了一个接受两个输入参数的方法，并且返回一个结果 3 BinaryOperator代表了一个作用于于两个同类型操作符的操作，并且返回了操作符同类型的结果 4 BiPredicate代表了一个两个参数的boolean值方法 5 BooleanSupplier代表了boolean值结果的提供方 6 Consumer代表了接受一个输入参数并且无返回的操作 7 DoubleBinaryOperator代表了作用于两个double值操作符的操作，并且返回了一个double值的结果。 8 DoubleConsumer代表一个接受double值参数的操作，并且不返回结果。 9 DoubleFunction代表接受一个double值参数的方法，并且返回结果 10 DoublePredicate代表一个拥有double值参数的boolean值方法 11 DoubleSupplier代表一个double值结构的提供方 12 DoubleToIntFunction接受一个double类型输入，返回一个int类型结果。 13 DoubleToLongFunction接受一个double类型输入，返回一个long类型结果 14 DoubleUnaryOperator接受一个参数同为类型double,返回值类型也为double 。 15 Function接受一个输入参数，返回一个结果。 16 IntBinaryOperator接受两个参数同为类型int,返回值类型也为int 。 17 IntConsumer接受一个int类型的输入参数，无返回值 。 18 IntFunction接受一个int类型输入参数，返回一个结果 。 19 IntPredicate：接受一个int输入参数，返回一个布尔值的结果。 20 IntSupplier无参数，返回一个int类型结果。 21 IntToDoubleFunction接受一个int类型输入，返回一个double类型结果 。 22 IntToLongFunction接受一个int类型输入，返回一个long类型结果。 23 IntUnaryOperator接受一个参数同为类型int,返回值类型也为int 。 24 LongBinaryOperator接受两个参数同为类型long,返回值类型也为long。 25 LongConsumer接受一个long类型的输入参数，无返回值。 26 LongFunction接受一个long类型输入参数，返回一个结果。 27 LongPredicateR接受一个long输入参数，返回一个布尔值类型结果。 28 LongSupplier无参数，返回一个结果long类型的值。 29 LongToDoubleFunction接受一个long类型输入，返回一个double类型结果。 30 LongToIntFunction接受一个long类型输入，返回一个int类型结果。 31 LongUnaryOperator接受一个参数同为类型long,返回值类型也为long。 32 ObjDoubleConsumer接受一个object类型和一个double类型的输入参数，无返回值。 33 ObjIntConsumer接受一个object类型和一个int类型的输入参数，无返回值。 34 ObjLongConsumer接受一个object类型和一个long类型的输入参数，无返回值。 35 Predicate接受一个输入参数，返回一个布尔值结果。 36 Supplier无参数，返回一个结果。 37 ToDoubleBiFunction接受两个输入参数，返回一个double类型结果 38 ToDoubleFunction接受一个输入参数，返回一个double类型结果 39 ToIntBiFunction接受两个输入参数，返回一个int类型结果。 40 ToIntFunction接受一个输入参数，返回一个int类型结果。 41 ToLongBiFunction接受两个输入参数，返回一个long类型结果。 42 ToLongFunction接受一个输入参数，返回一个long类型结果。 43 UnaryOperator接受一个参数为类型T,返回值类型也为T。 3、案例Predicate 接口是一个函数式接口，它接受一个输入参数 T，返回一个布尔值结果。该接口包含多种默认方法来将Predicate组合成其他复杂的逻辑（比如：与，或，非）。该接口用于测试对象是 true 或 false。实例代码：import java.util.Arrays;import java.util.List;import java.util.function.Predicate;public class FunInte { public static void main(String args[]){ List&lt;Integer&gt; list = Arrays.asList(1, 2, 3, 4, 5, 6, 7, 8, 9); // Predicate&lt;Integer&gt; predicate = n -&gt; true // n 是一个参数传递到 Predicate 接口的 test 方法 // n 如果存在则 test 方法返回 true System.out.println(\"输出所有数据:\"); // 传递参数 n eval(list, n-&gt; true); //lambda的使用 // Predicate&lt;Integer&gt; predicate1 = n -&gt; n%2 == 0 // n 是一个参数传递到 Predicate 接口的 test 方法 // 如果 n%2 为 0 test 方法返回 true System.out.println(\"输出所有偶数:\"); eval(list, n-&gt; n%2 == 0 ); // Predicate&lt;Integer&gt; predicate2 = n -&gt; n &gt; 3 // n 是一个参数传递到 Predicate 接口的 test 方法 // 如果 n 大于 3 test 方法返回 true System.out.println(\"输出大于 3 的所有数字:\"); eval(list, n-&gt; n &gt; 3 ); } public static void eval(List&lt;Integer&gt; list, Predicate&lt;Integer&gt; predicate) { for(Integer n: list) { if(predicate.test(n)) { System.out.println(n + \" \"); } } }}四、默认方法1、定义Java 8 新增了接口的默认方法。简单说，默认方法就是接口可以有实现方法，而且不需要实现类去实现其方法。为什么要有这个特性？首先，之前的接口是个双刃剑，好处是面向抽象而不是面向具体编程，缺陷是，当需要修改接口时候，需要修改全部实现该接口的类，目前的 java 8 之前的集合框架没有 foreach 方法，通常能想到的解决办法是在JDK里给相关的接口添加新的方法及实现。然而，对于已经发布的版本，是没法在给接口添加新方法的同时不影响已有的实现。所以引进的默认方法。他们的目的是为了解决接口的修改与现有的实现不兼容的问题2、使用我们只需在方法名前面加个 default 关键字即可实现默认方法。示例：public interface Vehicle { default void print(){ System.out.println(\"我是一辆车!\"); }}3、多个默认方法一个接口有默认方法，考虑这样的情况，一个类实现了多个接口，且这些接口有相同的默认方法，以下实例说明了这种情况的解决方法：public interface Vehicle { default void print(){ System.out.println(\"我是一辆车!\"); }} public interface FourWheeler { default void print(){ System.out.println(\"我是一辆四轮车!\"); }}//创建自己的默认方法 覆盖接口的默认方法public class Car implements Vehicle, FourWheeler { default void print(){ System.out.println(\"我是一辆四轮汽车!\"); }}//使用super来调用接口的默认方法public class Car implements Vehicle, FourWheeler { public void print(){ Vehicle.super.print(); }}4、静态默认方法java8的另一特性就是接口可以声明静态方法public interface Vehicle { default void print(){ System.out.println(\"我是一辆车!\"); } // 静态方法 static void blowHorn(){ System.out.println(\"按喇叭!!!\"); }}五、Stream1、定义Java 8 API添加了一个新的抽象称为流Stream，可以让你以一种声明的方式处理数据。Stream 使用一种类似用 SQL 语句从数据库查询数据的直观方式来提供一种对 Java 集合 运算和表达的高阶抽象。Stream API可以极大提高Java程序员的生产力，让程序员写出高效率、干净、简洁的代码。这种风格将要处理的元素集合看作一种流， 流在管道中传输， 并且可以在管道的节点上进行处理， 比如筛选， 排序，聚合等。元素流在管道中经过中间操作（intermediate operation）的处理，最后由最终操作(terminal operation)得到前面处理的结果。2、什么是Stream？Java 8引入了Stream API，是一个新的抽象层，用于对集合数据进行各种操作。它可以让开发人员以声明式方式处理数据，而不是过程式的方式。Stream API 可以让开发人员更容易地编写并行处理代码，并且可以让JVM更好地优化代码。Stream不存储元素，而是按需计算，避免了创建过多的中间对象，从而提高了性能。Stream（流）是一个来自数据源的元素队列并支持聚合操作 元素是特定类型的对象，形成一个队列。 Java中的Stream并不会存储元素，而是按需计算。 数据源 流的来源。 可以是集合，数组，I/O channel， 产生器generator 等。 聚合操作 类似SQL语句一样的操作， 比如filter, map, reduce, find, match, sorted等。和以前的Collection操作不同， Stream操作还有两个基础的特征： Pipelining: 中间操作都会返回流对象本身。 这样多个操作可以串联成一个管道， 如同流式风格（fluent style）。 这样做可以对操作进行优化， 比如延迟执行(laziness)和短路( short-circuiting)。 内部迭代： 以前对集合遍历都是通过Iterator或者For-Each的方式, 显式的在集合外部进行迭代， 这叫做外部迭代。 Stream提供了内部迭代的方式， 通过访问者模式(Visitor)实现。3、生成流(创建流)在java8中，集合接口有两个方法生成流 stream() − 为集合创建串行流。 parallelStream() − 为集合创建并行流。示例代码：import java.util.Arrays;import java.util.List;import java.util.stream.Collectors;public class Stream { public static void main(String[] args) { //创建集合 List&lt;String&gt; strings = Arrays.asList(\"abc\", \"\", \"bc\", \"efg\", \"abcd\",\"\", \"jkl\"); //使用流的方式去除掉空字符 搭配lambda进行使用 List&lt;String&gt; collect = strings.stream().filter(s -&gt; !s.isEmpty()).collect(Collectors.toList()); System.out.println(strings); System.out.println(collect); }}4、Stream提供的方法1.forEachStream 提供了新的方法 ‘forEach’ 来迭代流中的每个数据2.mapmap 方法用于映射每个元素到对应的结果3.filterfilter 方法用于通过设置的条件过滤出元素4.limitlimit 方法用于获取指定数量的流。 以下代码片段使用 limit 方法打印出 10 条数据：5.sortedsorted 方法用于对流进行排序。以下代码片段使用 sorted 方法对输出的 10 个随机数进行排序：6.并行（parallel）程序parallelStream 是流并行处理程序的代替方法。以下实例我们使用 parallelStream 来输出空字符串的数量：示例代码如下：import java.util.Arrays;import java.util.List;import java.util.Random;import java.util.stream.Collectors;public class Stream { public static void main(String[] args) { //创建集合 List&lt;String&gt; strings = Arrays.asList(\"abc\", \"\", \"bc\", \"efg\", \"abcd\",\"\", \"jkl\"); //使用流的方式去除掉空字符 搭配lambda进行使用 List&lt;String&gt; collect = strings.stream().filter(s -&gt; !s.isEmpty()).collect(Collectors.toList()); System.out.println(strings); System.out.println(collect); //forEach使用 以及limit的使用 Random random = new Random(); random.ints().limit(10).forEach(System.out::println); //map使用 List&lt;Integer&gt; numbers = Arrays.asList(3, 2, 2, 3, 7, 3, 5); // 获取对应的平方数 List&lt;Integer&gt; integers = numbers.stream().map(integer -&gt; integer * integer).distinct().collect(Collectors.toList()); System.out.println(integers); //filter的使用 List&lt;String&gt;strings1 = Arrays.asList(\"abc\", \"\", \"bc\", \"efg\", \"abcd\",\"\", \"jkl\"); // 获取空字符串的数量 long count = strings1.stream().filter(s -&gt; s.isEmpty()).count(); System.out.println(count); //sorted的使用 random.ints().limit(10).sorted().forEach(System.out::println); //parallelStream 是流并行处理程序的代替方法 long count1 = strings.parallelStream().filter(s -&gt; s.isEmpty()).count(); // 获取空字符串的数量 System.out.println(count1); }}输出结果如下：[abc, , bc, efg, abcd, , jkl][abc, bc, efg, abcd, jkl]-9593449266031967-19188637582111117054-2017571831768847329-1211126108-1399816495-1026989674-1087976107[9, 4, 49, 25]2-1872100248-1519313149-1057836903-1045386437-901931739-4693799938680892479648360691639564508189928546025、CollectorsCollectors 类实现了很多归约操作，例如将流转换成集合和聚合元素。Collectors 可用于返回列表或字符串将流转化为： 集合 ：.collect(Collectors.toList()) 字符串 ：.collect(Collectors.joining(“,”));示例代码：import java.util.Arrays;import java.util.List;import java.util.stream.Collectors;public class StreamE { public static void main(String[] args) { List&lt;String&gt; strings = Arrays.asList(\"abc\", \"\", \"bc\", \"efg\", \"abcd\",\"\", \"jkl\"); //去除空字符串 List&lt;String&gt; filtered = strings.stream().filter(string -&gt; !string.isEmpty()).collect(Collectors.toList()); //不含有空字符串 System.out.println(\"筛选列表: \" + filtered); //合并字符串 String mergedString = strings.stream().filter(s -&gt; !s.isEmpty()).collect(Collectors.joining(\",\")); System.out.println(\"合并字符串: \" + mergedString); }}输出结果：筛选列表: [abc, bc, efg, abcd, jkl]合并字符串: abc,bc,efg,abcd,jkl6、统计Collectors 类实现了很多归约操作，例如将流转换成集合和聚合元素。Collectors 可用于返回列表或字符串：import java.util.Arrays;import java.util.IntSummaryStatistics;import java.util.List;import java.util.stream.Collectors;public class StreamE { public static void main(String[] args) { //统计 List&lt;Integer&gt; numbers = Arrays.asList(3, 2, 2, 3, 7, 3, 5); IntSummaryStatistics stats = numbers.stream().mapToInt((x) -&gt; x).summaryStatistics(); System.out.println(\"列表中最大的数 : \" + stats.getMax()); System.out.println(\"列表中最小的数 : \" + stats.getMin()); System.out.println(\"所有数之和 : \" + stats.getSum()); System.out.println(\"平均数 : \" + stats.getAverage()); }}输出结果：列表中最大的数 : 7列表中最小的数 : 2所有数之和 : 25平均数 : 3.5714285714285716=7、操作分类Stream API中的操作可以分为两类：中间操作和终端操作。中间操作是一些对数据的处理和转换操作，比如过滤、映射、排序等。终端操作是对数据进行聚合或收集的操作，例如对数据求和、计数、查找最大值或最小值等。1.中间操作Stream中间操作可以链式调用，每个操作都返回一个新的Stream。中间操作不会执行任何操作，只是创建一个新的Stream，它会在结束操作时一次性执行。一些常见的中间操作包括： filter()：过滤出符合条件的元素。 map()：映射出一个新的元素。 sorted()：排序元素。 distinct()：去重。 limit()：限制流的数量。2.结束操作结束操作会遍历Stream并执行操作，最终返回一个非Stream的结果。结束操作包括： forEach()：遍历Stream中的每个元素。 collect()：将Stream转化为集合或其他数据结构。 reduce()：将Stream中的元素按照一定的方式聚合成一个结果。 count()：统计Stream中元素的个数。8、总结Java 8引入了Stream API，它是处理集合和数组的函数式编程方法。Stream API可以让我们更加简单、高效地处理集合和数组中的元素，比如筛选、过滤、映射等操作。Stream是一系列元素的集合，它可以来自集合、数组或I/O通道。Stream的操作可以分为两类，中间操作和结束操作。中间操作会返回一个新的Stream，可以被用于链式操作，而结束操作则会返回一个非Stream的值。Stream API有三个基本步骤：创建Stream、中间操作、结束操作。六、Optional类1、定义Optional 类是一个可以为null的容器对象。如果值存在则isPresent()方法会返回true，调用get()方法会返回该对象。Optional 是个容器：它可以保存类型T的值，或者仅仅保存null。Optional提供很多有用的方法，这样我们就不用显式进行空值检测。2、作用Optional 类的引入很好的解决空指针异常3、类声明模板public final class Optional&lt;T&gt; extends Object4、类方法 序号 方法 &amp; 描述 1 static Optional empty()返回空的 Optional 实例。 2 boolean equals(Object obj)判断其他对象是否等于 Optional。 3 Optional filter(Predicate predicate)如果值存在，并且这个值匹配给定的 predicate，返回一个Optional用以描述这个值，否则返回一个空的Optional。 4 Optional flatMap(Function&gt; mapper)如果值存在，返回基于Optional包含的映射方法的值，否则返回一个空的Optional 5 T get()如果在这个Optional中包含这个值，返回值，否则抛出异常：NoSuchElementException 6 int hashCode()返回存在值的哈希码，如果值不存在 返回 0。 7 void ifPresent(Consumer consumer)如果值存在则使用该值调用 consumer , 否则不做任何事情。 8 boolean isPresent()如果值存在则方法会返回true，否则返回 false。 9 Optional map(Function mapper)如果有值，则对其执行调用映射函数得到返回值。如果返回值不为 null，则创建包含映射返回值的Optional作为map方法返回值，否则返回空Optional。 10 static Optional of(T value)返回一个指定非null值的Optional。 11 static Optional ofNullable(T value)如果为非空，返回 Optional 描述的指定值，否则返回空的 Optional。 12 T orElse(T other)如果存在该值，返回值， 否则返回 other。 13 T orElseGet(Supplier other)如果存在该值，返回值， 否则触发 other，并返回 other 调用的结果。 14 T orElseThrow(Supplier exceptionSupplier)如果存在该值，返回包含的值，否则抛出由 Supplier 继承的异常 15 String toString()返回一个Optional的非空字符串，用来调试 5、示例代码import java.util.Optional;public class OptionalTest { public static void main(String[] args) { OptionalTest OP = new OptionalTest(); Integer value1 = null; Integer value2 = new Integer(10); //允许传递的值为null Optional&lt;Integer&gt; a = Optional.ofNullable(value1); //不允许传递的值为null 会抛出异常 Optional&lt;Integer&gt; b = Optional.of(value2); System.out.println(OP.sum(a,b)); } //自定义方法 public Integer sum(Optional&lt;Integer&gt; a, Optional&lt;Integer&gt; b){ //判断值是否存在 System.out.println(\"第一个值存在：\" + a.isPresent()); System.out.println(\"第二个值存在：\" + b.isPresent()); //值不存在则返回默认值 Integer value1 = a.orElse(new Integer(0)); //获取值 Integer value2 = b.get(); return value1 + value2; }}输出结果：第一个值存在：false 第二个值存在：true 10七、日期时间API1、定义java 8通过发布新的Date-Time API (JSR 310)来进一步加强对日期与时间的处理。在旧版的 Java 中，日期时间 API 存在诸多问题，其中有： 非线程安全 − java.util.Date 是非线程安全的，所有的日期类都是可变的，这是Java日期类最大的问题之一。 设计很差 − Java的日期/时间类的定义并不一致，在java.util和java.sql的包中都有日期类，此外用于格式化和解析的类在java.text包中定义。java.util.Date同时包含日期和时间，而java.sql.Date仅包含日期，将其纳入java.sql包并不合理。另外这两个类都有相同的名字，这本身就是一个非常糟糕的设计。 时区处理麻烦 − 日期类并不提供国际化，没有时区支持，因此Java引入了java.util.Calendar和java.util.TimeZone类，但他们同样存在上述所有的问题。2、新的apijava.time 包下提供了很多新的 API。以下为两个比较重要的 API： Local(本地) − 简化了日期时间的处理，没有时区的问题。 Zoned(时区) − 通过制定的时区处理日期时间。3、本地化日期api LocalDate类 LocalTime类 LocalDateTime 类LocalDateTime now = LocalDateTime.now();System.out.println(now);4、使用时区的日期时间api ZonedDateTime date1 = ZonedDateTime.parse(\"2015-12-03T10:15:30+05:30[Asia/Shanghai]\"); System.out.println(\"date1: \" + date1); ZoneId id = ZoneId.of(\"Europe/Paris\"); System.out.println(\"ZoneId: \" + id); ZoneId currentZone = ZoneId.systemDefault(); System.out.println(\"当期时区: \" + currentZone);八、Base641、定义在Java 8中，Base64编码已经成为Java类库的标准。Java 8 内置了 Base64 编码的编码器和解码器。2、编码器Base64工具类提供了一套静态方法获取下面三种BASE64编解码器： 基本：输出被映射到一组字符A-Za-z0-9+/，编码不添加任何行标，输出的解码仅支持A-Za-z0-9+/。 URL：输出映射到一组字符A-Za-z0-9+_，输出是URL和文件。 MIME：输出隐射到MIME友好格式。输出每行不超过76字符，并且使用’\\r’并跟随’\\n’作为分割。编码输出最后没有行分割。区分： Encoder：编码 Decoder：解码3、内嵌类 序号 内嵌类 &amp; 描述 1 static class Base64.Decoder该类实现一个解码器用于，使用 Base64 编码来解码字节数据。 2 static class Base64.Encoder该类实现一个编码器，使用 Base64 编码来编码字节数据。 4、类方法 序号 方法名 &amp; 描述 1 static Base64.Decoder getDecoder()返回一个 Base64.Decoder ，解码使用基本型 base64 编码方案。 2 static Base64.Encoder getEncoder()返回一个 Base64.Encoder ，编码使用基本型 base64 编码方案。 3 static Base64.Decoder getMimeDecoder()返回一个 Base64.Decoder ，解码使用 MIME 型 base64 编码方案。 4 static Base64.Encoder getMimeEncoder()返回一个 Base64.Encoder ，编码使用 MIME 型 base64 编码方案。 5 static Base64.Encoder getMimeEncoder(int lineLength, byte[] lineSeparator)返回一个 Base64.Encoder ，编码使用 MIME 型 base64 编码方案，可以通过参数指定每行的长度及行的分隔符。 6 static Base64.Decoder getUrlDecoder()返回一个 Base64.Decoder ，解码使用 URL 和文件名安全型 base64 编码方案。 7 static Base64.Encoder getUrlEncoder()返回一个 Base64.Encoder ，编码使用 URL 和文件名安全型 base64 编码方案。 5、示例代码import java.nio.charset.StandardCharsets;import java.util.Base64;public class Base64Test { public static void main(String[] args) { String s = Base64.getEncoder().encodeToString(\"智慧餐厅\".getBytes(StandardCharsets.UTF_8)); System.out.println(s); byte[] decode = Base64.getDecoder().decode(s); System.out.println(new String(decode,StandardCharsets.UTF_8)); }}输出结果：5pm65oWn6aSQ5Y6F智慧餐厅" }, { "title": "过滤器与拦截器", "url": "/posts/Filtersandinterceptors/", "categories": "技术科普", "tags": "学习", "date": "2023-03-16 12:21:00 +0000", "snippet": "过滤器与拦截器一、过滤器1、定义过滤器 Filter 是 Sun 公司在 Servlet 2.3 规范中添加的新功能，其作用是对客户端发送给 Servlet 的请求以及对 Servlet 返回给客户端的响应做一些定制化的处理，例如校验请求的参数、设置请求/响应的 Header、修改请求/响应的内容等。Filter 引入了过滤链（Filter Chain）的概念，一个 Web 应用可以部署多个...", "content": "过滤器与拦截器一、过滤器1、定义过滤器 Filter 是 Sun 公司在 Servlet 2.3 规范中添加的新功能，其作用是对客户端发送给 Servlet 的请求以及对 Servlet 返回给客户端的响应做一些定制化的处理，例如校验请求的参数、设置请求/响应的 Header、修改请求/响应的内容等。Filter 引入了过滤链（Filter Chain）的概念，一个 Web 应用可以部署多个 Filter，这些 Filter 会组成一种链式结构，客户端的请求在到达 Servlet 之前会一直在这个链上传递，不同的 Filter 负责对请求/响应做不同的处理。 Filter 的处理流程如下图所示：Filter 的作用可认为是对 Servlet 功能的增强，因为 Filter 可以对用户的请求做预处理，也可以对返回的响应做后处理，且这些处理逻辑与 Servlet 的处理逻辑是分隔开的，这使得程序中各部分业务逻辑之间的耦合度降低，从而提高了程序的可维护性和可扩展性。2、创建方式创建 Filter 需要实现 javax.servlet.Filter 接口，或者继承实现了 Filter 接口的父类。Filter 接口中定义了三个方法： init：在 Web 程序启动时被调用，用于初始化 Filter。 doFilter：在客户端的请求到达时被调用，doFilter 方法中定义了 Filter 的主要处理逻辑，同时该方法还负责将请求传递给下一个 Filter 或 Servlet。 destroy：在 Web 程序关闭时被调用，用于销毁一些资源。下面我们通过实现 Filter 接口来创建一个自定义的 Filter：public class TestFilter implements Filter { @Override public void init(FilterConfig filterConfig) throws ServletException { System.out.println(filterConfig.getFilterName() + \" 被初始化\"); } @Override public void doFilter(ServletRequest servletRequest, ServletResponse servletResponse, FilterChain filterChain) throws IOException, ServletException { HttpServletRequest request = (HttpServletRequest) servletRequest; System.out.println(\"Filter 拦截到了请求: \" + request.getRequestURL()); System.out.println(\"Filter 对请求做预处理...\"); filterChain.doFilter(servletRequest, servletResponse); System.out.println(\"Filter 修改响应的内容...\"); } @Override public void destroy() { System.out.println(\"Filter 被回收\"); }}1. @WebFilter 注解 + 包扫描除了 FilterRegistrationBean 外，Servlet 3.0 引入的注解 @WebFilter 也可用于配置 Filter。我们只需要在自定义的 Filter 类上添加该注解，就可以设置 Filter 的名称和拦截规则：@WebFilter(urlPatterns = \"/*\", filterName = \"TestFilter\")public class TestFilter implements Filter { // 省略部分代码}复制代码由于@WebFilter 并非 Spring 提供，因此若要使自定义的 Filter 生效，还需在配置类上添加 @ServletComponetScan 注解，并指定扫描的包：@SpringBootApplication@ServletComponentScan(\"com.example.filter\")public class DemoApplication { public static void main(String[] args) { SpringApplication.run(DemoApplication.class, args); }}复制代码需要注意的是，@WebFilter 注解并不允许我们设置 Filter 的执行顺序，且在 Filter 类上添加 @Order 注解也是无效的。如果项目中有多个被 @WebFilter 修饰的 Filter，那么这些 Filter 的执行顺序由其 “类名的字典序” 决定，例如类名为 “Axx” 的 Filter 的执行顺序要先于类名为 “Bxx” 的 Filter。 添加了 @WebFilter 注解后就不要再添加 @Component 注解了，如果都添加，那么系统会创建两个 Filter。2. @Component 注解Spring 项目中，我们可以通过添加 @Component 注解将自定义的 Bean 交给 Spring 容器管理。同样的，对于自定义的 Filter，我们也可以直接添加 @Component 注解使其生效，而且还可以添加 @Order 注解来设置不同 Filter 的执行顺序。@Component@Order(1)public class TestFilter implements Filter { // 省略部分代码}复制代码此种配置方式一般不常使用，因为其无法设置 Filter 的拦截规则，默认的拦截路径为 /*。虽然不能配置拦截规则，但我们可以在 doFilter 方法中定义请求的放行规则，例如当请求的 URL 匹配我们设置的规则时，直接将该请求放行，也就是立即执行 filterChain.doFilter(servletRequest, servletResponse);。3. 继承 OncePerRequestFilterOncePerRequestFilter 是一个由 Spring 提供的抽象类，在项目中，我们可以采用继承 OncePerRequestFilter 的方式创建 Filter，然后重写 doFilterInternal 方法定义 Filter 的处理逻辑，重写 shouldNotFilter 方法设置 Filter 的放行规则。对于多个 Filter 的执行顺序，我们也可以通过添加 @Order 注解进行设置。当然，若要使 Filter 生效，还需添加 @Component 注解将其注册到 Spring 容器。@Component@Order(1)public class CSpringFilter extends OncePerRequestFilter { @Override protected void doFilterInternal(HttpServletRequest request, HttpServletResponse response, FilterChain filterChain) throws ServletException, IOException { // 处理逻辑 } @Override protected boolean shouldNotFilter(HttpServletRequest request) throws ServletException { // 放行规则 }}复制代码实际上，方式 2 和方式 3 本质上并没有什么区别，因为 OncePerRequestFilter 底层也是通过实现 Filter 接口来达到过滤请求/响应的目的，只不过 Spring 在 OncePerRequestFilter 中帮我们封装了许多功能，因此更推荐采用此种方式创建 Filter。3、配置Spring 项目中，我们可以使用 @Configuration + @Bean + FilterRegistrationBean 对 Filter 进行配置：@Configurationpublic class FilterConfig { @Bean public FilterRegistrationBean&lt;TestFilter&gt; registryFilter() { FilterRegistrationBean&lt;TestFilter&gt; registration = new FilterRegistrationBean&lt;&gt;(); registration.setFilter(new TestFilter()); registration.addUrlPatterns(\"/*\"); registration.setName(\"TestFilter\"); registration.setOrder(0); return registration; }}复制代码上述代码中，setFilter 方法用于设置 Filter 的类型；addUrlPatterns 方法用于设置拦截的规则；setName 方法用于设置 Filter 的名称；setOrder 方法用于设置 Filter 的优先级，数字越小优先级越高。4、优先级上文中提到，使用配置类或添加 @Order 注解可以显式的设置 Filter 的执行顺序，修改类名可以隐式的设置 Filter 的执行顺序。如果项目中存在多个 Filter，且这些 Filter 由不同的方式创建，那么它们的执行顺序是怎样的呢？能够确定的是，Spring 根据 Filter 的 order 决定其优先级，如果我们通过配置类或者通过 @Order 注解设置了 Filter 的 order，那么 order 值越小的 Filter 的优先级越高，无论 Filter 由何种方式创建。如果多个 Filter 的优先级相同，那么执行顺序为： 配置类中配置的 Filter 优先执行，如果配置类中存在多个 Filter，那么 Spring 按照其在配置类中配置的顺序依次执行。 @WebFilter 注解修饰的 Filter 之后执行，如果存在多个 Filter，那么 Spring 按照其类名的字典序依次执行。 @Component 注解修饰的 Filter 最后执行，如果存在多个 Filter，那么 Spring 按照其类名的字典序依次执行。注意，以上优先级顺序仅适用于 order 相同的特殊情况。如果我们不配置 Filter 的 order，那么 Spring 默认将其 order 设置为 LOWEST_PRECEDENCE = Integer.MAX_VALUE，也就是最低优先级。由于被 @WebFilter 注解修饰的 Filter 无法显式配置优先级，因此其 order 为 Integer.MAX_VALUE。本文所说的 Filter 的优先级指的是 Filter 对请求做预处理的优先级，对响应做后处理的优先级与之相反。5、应用场景 解决跨域访问：前后端分离的项目往往存在跨域访问的问题，Filter 允许我们在 response 的 Header 中设置 “Access-Control-Allow-Origin”、”Access-Control-Allow-Methods” 等头域，以此解决跨域失败问题。 设置字符编码：字符编码 Filter 可以在 request 提交到 Servlet 之前或者在 response 返回给客户端之前为请求/响应设置特定的编码格式，以解决请求/响应内容乱码的问题。 记录日志：日志记录 Filter 可以在拦截到请求后，记录请求的 IP、访问的 URL，拦截到响应后记录请求的处理时间。当不需要记录日志时，也可以直接将 Filter 的配置注释掉。 校验权限：Web 服务中，客户端在发送请求时会携带 cookie 或者 token 进行身份认证，权限校验 Filter 可以在 request 提交到 Servlet 之前对 cookie 或 token 进行校验，如果用户未登录或者权限不够，那么 Filter 可以对请求做重定向或返回错误信息。 替换内容：内容替换 Filter 可以对网站的内容进行控制，防止输入/输出非法内容和敏感信息。例如在请求到达 Servlet 之前对请求的内容进行转义，防止 XSS 攻击；在 Servlet 将内容输出到 response 时，使用 response 将内容缓存起来，然后在 Filter 中进行替换，最后再输出到客户浏览器（由于默认的 response 并不能严格的缓存输出内容，因此需要自定义一个具备缓存功能的 response）。6、实现原理了解了 Filter 的使用方法之后，我们来分析一下 Filter 的实现原理。上文中提到，客户端的请求会在过滤链上依次传递，链上的每个 Filter 都会调用 doFilter 方法对请求进行处理。这个过程能够有条不紊的进行，底层依靠的是函数的回调机制。1. 回调函数在介绍 Filter 的回调机制之前，我们先了解一下回调函数的概念。如果将函数（C++ 中的函数指针，Java 中的匿名函数、方法引用等）作为参数传递给主方法，那么这个函数就称为回调函数，主方法会在某一时刻调用回调函数。 为了便于区分，我们使用 “主方法” 和 “函数” 来分辨主函数和回调函数。使用回调函数的好处是能够实现函数逻辑的解耦，主方法内可以定义通用的处理逻辑，部分特定的操作则交给回调函数来完成。例如 Java 中 Arrays 类的 sort(T[] a, Comparator&lt;? super T&gt; c) 方法允许我们传入一个比较器来自定义排序规则，这个比较器的 compare 方法就属于回调函数，sort 方法会在排序时调用 compare 方法。2. Filter 的回调机制接下来介绍 Filter 的回调机制，上文中提到，我们自定义的 xxFilter 类需要实现 Filter 接口，且需要重写 doFilter 方法：public class TestFilter implements Filter { @Override public void doFilter(ServletRequest servletRequest, ServletResponse servletResponse, FilterChain filterChain) throws IOException, ServletException { // ... filterChain.doFilter(servletRequest, servletResponse); }}复制代码Filter 接口的 doFilter 方法接收一个 FilterChain 类型的参数，这个 FilterChain 对象可认为是传递给 doFilter 方法的回调函数，严格来说应该是这个 FilterChain 对象的 doFilter 方法，注意这里提到了两个 doFilter 方法。Filter 接口的 doFilter 方法在执行结束或执行完某些步骤后会调用 FilterChain 对象的 doFilter 方法，即调用回调函数。FilterChain 对象的实际类型为 ApplicationFilterChain，其 doFilter() 方法的处理逻辑如下（省略部分代码）：public final class ApplicationFilterChain implements FilterChain { @Override public void doFilter(ServletRequest request, ServletResponse response) throws IOException, ServletException { // ... internalDoFilter(request,response); } private void internalDoFilter(ServletRequest request, ServletResponse response) throws IOException, ServletException { if (pos &lt; n) { // 获取第 pos 个 filter, 即 xxFilter ApplicationFilterConfig filterConfig = filters[pos++]; Filter filter = filterConfig.getFilter(); // ... // 调用 xxFilter 的 doFilter 方法 filter.doFilter(request, response, this); } }}复制代码可见，ApplicationFilterChain 的 doFilter 方法首先根据索引查询到我们定义的 xxFilter，然后调用 xxFilter 的 doFilter 方法，在调用时，ApplicationFilterChain 会将自己作为参数传递进去。xxFilter 的 doFilter 方法执行完某些步骤后，会调用回调函数，即 ApplicationFilterChain 的 doFilter 方法，这样 ApplicationFilterChain 就可以获取到下一个 xxFilter，并调用下一个 xxFilter 的 doFilter 方法，如此循环下去，直到所有的 xxFilter 全部被调用。整个流程如下图所示： xxFilter 执行回调函数的过程就像是给了 ApplicationFilterChain 一个通知，即通知 ApplicationFilterChain 可以执行下一个 xxFilter 的处理逻辑了。综上，如果我们在重写自定义 Filter 的 doFilter 方法时没有添加 filterChain.doFilter(servletRequest, servletResponse);，那么该 Filter 之后的其它 Filter 和 Servlet 就不会被调用。二、拦截器1、定义 本文所说的拦截器指的是 Spring MVC 中的拦截器。拦截器 Interceptor 是 Spring MVC 中的高级组件之一，其作用是拦截用户的请求，并在请求处理前后做一些自定义的处理，如校验权限、记录日志等。这一点和 Filter 非常相似，但不同的是，Filter 在请求到达 Servlet 之前对请求进行拦截，而 Interceptor 则是在请求到达 Controller 之前对请求进行拦截，响应也同理。与 Filter 一样，Interceptor 也具备链式结构，我们在项目中可以配置多个 Interceptor，当请求到达时，每个 Interceptor 根据其声明的顺序依次执行。2、创建方式创建 Interceptor 需要实现 org.springframework.web.servlet.HandlerInterceptor 接口，HandlerInterceptor 接口中定义了三个方法： preHandle：在 Controller 方法执行前被调用，可以对请求做预处理。该方法的返回值是一个 boolean 变量，只有当返回值为 true 时，程序才会继续向下执行。 postHandle：在 Controller 方法执行结束，DispatcherServlet 进行视图渲染之前被调用，该方法内可以操作 Controller 处理后的 ModelAndView 对象。 afterCompletion：在整个请求处理完成（包括视图渲染）后被调用，通常用来清理资源。注意，postHandle 方法和 afterCompletion 方法执行的前提条件是 preHandle 方法的返回值为 true。下面我们创建一个 Interceptor：@Componentpublic class TestInterceptor implements HandlerInterceptor { @Override public boolean preHandle(HttpServletRequest request, HttpServletResponse response, Object handler) throws Exception { System.out.println(\"Interceptor 拦截到了请求: \" + request.getRequestURL()); return true; } @Override public void postHandle(HttpServletRequest request, HttpServletResponse response, Object handler, ModelAndView modelAndView) throws Exception { System.out.println(\"Interceptor 操作 modelAndView...\"); } @Override public void afterCompletion(HttpServletRequest request, HttpServletResponse response, Object handler, Exception ex) throws Exception { System.out.println(\"Interceptor 清理资源...\"); }}3、配置Interceptor 需要注册到 Spring 容器才能够生效，注册的方法是在配置类中实现 WebMvcConfigurer 接口，并重写 addInterceptors 方法:@Configurationpublic class TestInterceptorConfig implements WebMvcConfigurer { @Autowired private TestInterceptor testInterceptor; @Override public void addInterceptors(InterceptorRegistry registry) { registry.addInterceptor(testInterceptor) .addPathPatterns(\"/*\") .excludePathPatterns(\"/**/*.css\", \"/**/*.js\", \"/**/*.png\", \"/**/*.jpg\", \"/**/*.jpeg\") .order(1); }}复制代码上述代码中，addInterceptor 方法用于注册 Interceptor；addPathPatterns 方法用于设置拦截规则；excludePathPatterns 方法用于设置放行规则，order 方法用于设置 Interceptor 的优先级，数字越小优先级越高。4、应用场景Interceptor 的应用场可以参考上文中介绍的 Filter 的应用场景，可以说 Filter 能做到的事 Interceptor 都能做。由于 Filter 在 Servlet 前后起作用，而 Interceptor 可以在 Controller 方法前后起作用，例如操作 Controller 处理后的 ModelAndView，因此 Interceptor 更加灵活，在 Spring 项目中，如果能使用 Interceptor 的话尽量使用 Interceptor。5、实现原理了解了 Interceptor 的使用方法之后，我们也来分析一下 Interceptor 的实现原理。Interceptor 的调用发生在 DispatcherServlet 的 doDispatch 方法中，DispatcherServlet 是 Spring MVC 中请求的统一入口，当客户端的请求到达时，DispatcherServlet 会调用 doDispatch 方法处理当前请求：protected void doDispatch(HttpServletRequest request, HttpServletResponse response) throws Exception { // 省略部分代码 try { ModelAndView mv = null; Exception dispatchException = null; try { processedRequest = checkMultipart(request); multipartRequestParsed = (processedRequest != request); // 根据请求查找处理链 HandlerExecutionChain mappedHandler = getHandler(processedRequest); if (mappedHandler == null) { noHandlerFound(processedRequest, response); return; } // 获取适配器 HandlerAdapter, HandlerAdapter 用于处理请求 HandlerAdapter ha = getHandlerAdapter(mappedHandler.getHandler()); // 省略部分代码 // 依次执行所有拦截器的 preHandle 方法 if (!mappedHandler.applyPreHandle(processedRequest, response)) { return; } // 执行 Controller 方法, 返回 ModelAndView mv = ha.handle(processedRequest, response, mappedHandler.getHandler()); if (asyncManager.isConcurrentHandlingStarted()) { return; } applyDefaultViewName(processedRequest, mv); // 依次执行所有拦截器的 postHandle 方法, 顺序和 preHandle 方法相反 mappedHandler.applyPostHandle(processedRequest, response, mv); } catch (Exception ex) { dispatchException = ex; } catch (Throwable err) { dispatchException = new NestedServletException(\"Handler dispatch failed\", err); } // 进行视图渲染, 结束后执行拦截器的 afterCompletion 方法 processDispatchResult(processedRequest, response, mappedHandler, mv, dispatchException); } // 如果发生异常, 那么执行拦截器的 afterCompletion 方法并抛出异常 catch (Exception ex) { triggerAfterCompletion(processedRequest, response, mappedHandler, ex); } catch (Throwable err) { triggerAfterCompletion(processedRequest, response, mappedHandler, new NestedServletException(\"Handler processing failed\", err)); } // 省略部分代码}复制代码 首先获取当前请求对应的处理链 HandlerExecutionChain，HandlerExecutionChain 中包含了当前请求匹配的 Interceptor 列表。 然后根据 HandlerExecutionChain 获取适配器 HandlerAdapter，HandlerAdapter 用于处理当前请求。 依次执行所有 Interceptor 的 preHandle 方法，如果某个 preHandle 方法返回 false，那么程序将不再向下执行。 调用 HandlerAdapter 处理请求，返回 ModelAndView。 请求处理完成后，依次执行所有 Interceptor 的 postHandle 方法，执行的顺序与 preHandle 方法相反。 进行视图渲染，渲染结束后执行所有 Interceptor 的 afterCompletion 方法，执行顺序与 postHandle 方法相同。 如果以上操作发生异常，那么执行所有 Interceptor 的 afterCompletion 方法并抛出异常。 由上述过程我们得知，如果 Controller 抛出异常，那么 postHandle 方法将不会执行，afterCompletion 方法则一定执行。三、两者的区别1. 规范不同Filter 在 Servlet 规范中定义，依赖于 Servlet 容器（如 Tomcat）；Interceptor 由 Spring 定义，依赖于 Spring 容器（IoC 容器）。2. 适用范围不同Filter 仅可用于 Web 程序，因为其依赖于 Servlet 容器；Interceptor 不仅可以用于 Web 程序，还可以用于 Application、Swing 等程序。3. 触发时机不同Filter 在请求进入 Servlet 容器，且到达 Servlet 之前对请求做预处理；在 Servlet 处理完请求后对响应做后处理。Interceptor 在请求进入 Servlet，且到达 Controller 之前对请求做预处理；在 Controller 处理完请求后对 ModelAndView 做后处理，在视图渲染完成后再做一些收尾工作。下图展示了二者的触发时机：当 Filter 和 Interceptor 同时存在时，Filter 对请求的预处理要先于 Interceptor 的 preHandle 方法；Filter 对响应的后处理要后于 Interceptor 的 postHandle 方法和 afterCompletion 方法。四、补充说明1. 在 Filter 和 Interceptor 注入 Bean 的注意事项有些文章在介绍 Filter 和 Interceptor 的区别时强调 Filter 不能通过 IoC 注入 Bean，如果我们采用本文中的第一种创建 Filter，那么确实不能注入成功：// 自定义的 Filter, 未添加 @Component 注解public class TestFilter implements Filter { @Autowired private UserService userService; @Override public void doFilter(ServletRequest servletRequest, ServletResponse servletResponse, FilterChain filterChain) throws IOException, ServletException { HttpServletRequest request = (HttpServletRequest) servletRequest; System.out.println(userService); filterChain.doFilter(servletRequest, servletResponse); } // ...}// 配置类@Configurationpublic class FilterConfig { @Bean public FilterRegistrationBean&lt;TestFilter&gt; registryFilter() { FilterRegistrationBean&lt;TestFilter&gt; registration = new FilterRegistrationBean&lt;&gt;(); registration.setFilter(new TestFilter()); registration.addUrlPatterns(\"/*\"); registration.setName(\"TestFilter\"); registration.setOrder(0); return registration; }}复制代码上述代码执行后，userService 输出为 null，因为注册到 IoC 容器中的是 new 出来的一个 TestFilter 对象（registration.setFilter(new TestFilter());），并不是 Spring 自动装配的。若要使 userService 注入成功，可改为如下写法：// 自定义的 Filter, 未添加 @Component 注解@Componentpublic class TestFilter implements Filter { @Autowired private UserService userService; @Override public void doFilter(ServletRequest servletRequest, ServletResponse servletResponse, FilterChain filterChain) throws IOException, ServletException { HttpServletRequest request = (HttpServletRequest) servletRequest; System.out.println(userService); filterChain.doFilter(servletRequest, servletResponse); } // ...}// 配置类@Configurationpublic class FilterConfig { @Autowired private TestFilter testFilter; @Bean public FilterRegistrationBean&lt;TestFilter&gt; registryFilter() { FilterRegistrationBean&lt;TestFilter&gt; registration = new FilterRegistrationBean&lt;&gt;(); registration.setFilter(testFilter); registration.addUrlPatterns(\"/*\"); registration.setName(\"TestFilter\"); registration.setOrder(0); return registration; }}复制代码与第一种写法的区别在于，TestFilter 类上添加了 @Component 注解，且配置类中通过 @Autowired 注入 TestFilter 对象。除了使用配置类外，本文介绍的其它几种方式（添加 @Component 注解或 @WebFilter 注解）都可以直接注入 Bean。 所以还是采用继承 OncePerRequestFilter 的方式创建 Filter 比较方便。另外，使用本文介绍的创建 Interceptor 的写法是可以直接注入 Bean 的，该写法也是先在自定义的 Interceptor 上添加 @Component 注解，然后在配置类中使用 @Autowired 注入自定义的 Interceptor。2. Interceptor 可以拦截静态请求有文章提到 Interceptor 不能拦截静态请求，其实在 Spring 1.x 的版本中确实是这样的，但 Spring 2.x 对静态资源也进行了拦截，例如上文中我们在测试 TestInterceptor 是否生效时，发现其拦截到了 /favicon.ico 请求，该请求是一个由浏览器自动发送的静态请求。" }, { "title": "字典树(Trie)的应用", "url": "/posts/Trie/", "categories": "数据结构", "tags": "学习", "date": "2023-03-05 07:35:00 +0000", "snippet": "字典树(Trie)的应用当我们需要处理大量字符串数据的时候，如何高效地进行字符串匹配，是一个非常重要的问题。在这个问题中，前缀树是一种非常高效的数据结构，它能够支持快速地查找字符串，以及对字符串进行前缀匹配。一、什么是前缀树？​\t前缀树（Trie树）是一种用于快速检索字符串的树型数据结构，也称为字典树或关键词树。它的基本思想是，将字符串分解为多个字符，在树上将这些字符逐个存储，从而达到高效的...", "content": "字典树(Trie)的应用当我们需要处理大量字符串数据的时候，如何高效地进行字符串匹配，是一个非常重要的问题。在这个问题中，前缀树是一种非常高效的数据结构，它能够支持快速地查找字符串，以及对字符串进行前缀匹配。一、什么是前缀树？​\t前缀树（Trie树）是一种用于快速检索字符串的树型数据结构，也称为字典树或关键词树。它的基本思想是，将字符串分解为多个字符，在树上将这些字符逐个存储，从而达到高效的检索效果。通过利用字符串的公共前缀来减少字符串的比较次数，Trie树可以在非常短的时间内找到需要查找的字符串，因此在实际应用中非常有用。二、如何实现前缀树?​\tTrie树的根节点代表一个空字符串，每个节点包含一个字符和一个布尔值，表示到达当前节点的字符串是否为某个单词的末尾。从根节点开始遍历Trie树，依次经过每个字符，直到到达一个布尔值为真的节点。这个过程可以完成对一个字符串的检索。​\tTrie树的构建过程是将字符串一个字符一个字符地插入到树中，每个字符串都对应着一条从根节点到某个布尔值为真的节点的路径。在构建过程中，如果两个字符串有公共前缀，那么它们在Trie树中的路径也会有公共的前缀。这样，Trie树可以利用公共前缀来减少查找的时间，使得Trie树在大量字符串比较的场景下效率很高。具体实现：​\t实现前缀树的方法非常简单。我们可以使用一个哈希表来保存每个节点的子节点，其中键是字符，值是子节点。我们还需要在每个节点上设置一个标志，表示该节点是否为字符串的结尾字符。(简易点也可以使用字符数组来保存每个节点的子节点)示例代码如下：//java语言实现class Trie { class TrieNode { Map&lt;Character, TrieNode&gt; children; boolean isEndOfWord; public TrieNode() { children = new HashMap&lt;&gt;(); isEndOfWord = false; } } private TrieNode root; public Trie() { root = new TrieNode(); } public void insert(String word) { TrieNode curr = root; for (char c : word.toCharArray()) { TrieNode node = curr.children.get(c); if (node == null) { node = new TrieNode(); curr.children.put(c, node); } curr = node; } curr.isEndOfWord = true; } public boolean search(String word) { TrieNode curr = root; for (char c : word.toCharArray()) { TrieNode node = curr.children.get(c); if (node == null) { return false; } curr = node; } return curr.isEndOfWord; } public boolean startsWith(String prefix) { TrieNode curr = root; for (char c : prefix.toCharArray()) { TrieNode node = curr.children.get(c); if (node == null) { return false; } curr = node; } return true; }}三、前缀树的应用范围前缀树的应用场景非常广泛，常见的有以下几个方面：1.字符串匹配前缀树可以用于高效地实现字符串的匹配操作，特别是在需要匹配多个模式串的情况下。传统的字符串匹配算法如KMP、BM、Sunday等需要对模式串进行预处理，时间复杂度较高。而使用前缀树，只需要将模式串依次插入到前缀树中，然后对文本串在前缀树上进行匹配即可，时间复杂度为O(m+n)，其中m和n分别是模式串和文本串的长度。2.单词查找前缀树可以用于实现单词查找功能。例如，输入一个前缀，返回所有以该前缀开头的单词。这个功能在搜索引擎中广泛应用，可以提高搜索效率，缩小搜索范围。3.自动补全基于前缀树的自动补全功能可以提高用户输入体验，为用户提供更快捷的输入方式。例如，在搜索引擎中，用户输入关键词时，可以根据前缀匹配，自动提示可能的关键词。4.IP地址查找前缀树也可以用于IP地址查找。IPv4地址通常用32位二进制数表示，IPv6地址用128位二进制数表示，因此在存储和查找时需要高效的算法。前缀树可以将IP地址看做一个32位或128位的二进制数，将地址插入到前缀树中，然后进行查找操作。5.路径查找前缀树还可以用于路径查找。例如，对于一个文件系统，我们需要根据路径名查找文件或目录，可以将文件系统中的所有路径插入到前缀树中，然后在前缀树上进行查找操作。综上所述，前缀树在各种领域都有着广泛的应用。在实际工程中，需要根据具体场景进行选择和优化，以满足不同的需求。四、基于搜索引擎的下拉框联想词推荐 即为自动补全应用的具体实现1.定义前缀树的数据结构package com.searchengine.utils;import java.util.*;/** * @author: YKFire * @description: 实现Trie树用于，在搜索框输入文字时，提供提示内容 * @date: 2022-06-03 19:34 */public class Trie { final int COUNT = 10; // 提示内容的最大数目 int cnt = 0; class Node { Map&lt;Character, Node&gt; child; // 结点的所有孩子 boolean isEnd; // 用于判断该结点是否为结束 Node() { this.child = new HashMap&lt;&gt;(1000); isEnd = false; } void addChild(char c) { this.child.put(c, new Node()); } } Node root = new Node(); // 初始化一个根结点 /** * @author: YKFire * @description: 向Trie树中添加文字 * @date: 2022-06-03 19:38 */ public void add(String word) { Node node = root; int len = word.length(); for (int i = 0; i &lt; len; i++) { char c = word.charAt(i); if (!node.child.containsKey(c)) { node.addChild(c); if (i == len - 1) { node.child.get(c).isEnd = true; } } node = node.child.get(c); } } /** * @author: YKFire * @description: 在Trie树中查找word是否存在 * @date: 2022-06-03 19:38 */ public boolean search(String word) { Node node = root; int len = word.length(); for (int i = 0; i &lt; len; i++) { char c = word.charAt(i); if (!node.child.containsKey(c)) return false; node = node.child.get(c); } return node.isEnd; } }}2.初始化静态前缀树结构 在服务启动时，存储数据数据库中分词信息(数据量过多时选择存储最近的热点数据分词)package com.searchengine.common;import com.searchengine.dao.SegmentationDao;import com.searchengine.entity.Segmentation;import com.searchengine.utils.Trie;import org.springframework.beans.factory.annotation.Autowired;import org.springframework.stereotype.Component;import javax.annotation.PostConstruct;import java.util.List;@Component/** * @author: YKFire * @description: 将某些数据缓存到全局变量中 * @date: 2022-06-03 18:24 */public class CodeCache { public static Trie trie = new Trie(); @Autowired private SegmentationDao segmentationDao; //使用该注解来完成初始化，@PostConstruct注解的方法将会在依赖注入对象注入完成后被自动调用 //执行顺序：构造方法 &gt; @Autowired &gt; @PostConstruct @PostConstruct public void init() { List&lt;Segmentation&gt; segmentations = segmentationDao.selectAllSeg(); for (int i = 0; i &lt; segmentations.size(); i++) { String word = segmentations.get(i).getWord(); trie.add(word); } }}3.编写对应算法，实现联想词推荐 采取深度优先遍历算法，遍历前缀树获取搜索关键词对应的后缀，最终装进List集合中返回/** * @author: YKFire * @description: 找到10个相关的词 * @date: 2022-06-03 19:39 */ public List&lt;String&gt; getRelatedWords(String word) { cnt = 0; List&lt;String&gt; res = new ArrayList&lt;&gt;(); Node node = root; int len = word.length(); for (int i = 0; i &lt; len; i++) { char c = word.charAt(i); if (!node.child.containsKey(c)) return null; node = node.child.get(c); } dfs(word, node, res, \"\"); return res; } //本质上为一个深度优先搜索的方法 // 函数的参数包括当前的单词 word，当前节点 node，结果列表 res，以及当前路径 path public void dfs(String word, Node node, List&lt;String&gt; res, String path) { // 没必要使用StringBuilder了 //首先判断是否已经找到了足够数量的单词，如果是则直接返回 if (cnt &gt;= COUNT) return; //然后判断当前节点是否是一个单词的结尾，如果是且这个单词不是初始单词本身，则将它加入到结果列表中，并将计数器 cnt 加 1 if (node.isEnd &amp;&amp; !word.equals(word + path)) { res.add(word + path); cnt++; } for (Map.Entry&lt;Character, Node&gt; entry : node.child.entrySet()) { //更新节点与路径 进行深度优先搜索 node = entry.getValue(); path = path + entry.getKey(); dfs(word, node, res, path); //将路径 path 恢复到遍历当前节点之前的状态，以便遍历下一个子节点时继续使用 path = path.substring(0, path.length() - 1); } }4.效果图" }, { "title": "Socket实战", "url": "/posts/Socket/", "categories": "技术科普", "tags": "学习", "date": "2023-02-25 06:23:00 +0000", "snippet": "Socket实战一、定义Socket（套接字）是一种通信协议，用于在计算机之间进行数据交换。它是一个编程接口（API），提供了网络编程中的一些基本操作，如创建、连接、发送和接收数据等。通过Socket，应用程序可以使用TCP或UDP协议在不同计算机之间进行数据通信。在网络编程中，Socket是一个端点，用于标识一个连接的两端。每个Socket都有一个IP地址和一个端口号，用于唯一标识一个连接...", "content": "Socket实战一、定义Socket（套接字）是一种通信协议，用于在计算机之间进行数据交换。它是一个编程接口（API），提供了网络编程中的一些基本操作，如创建、连接、发送和接收数据等。通过Socket，应用程序可以使用TCP或UDP协议在不同计算机之间进行数据通信。在网络编程中，Socket是一个端点，用于标识一个连接的两端。每个Socket都有一个IP地址和一个端口号，用于唯一标识一个连接。应用程序可以使用Socket来建立连接、发送数据、接收数据、关闭连接等操作。在客户端/服务器应用程序中，Socket通常由服务器端创建并等待客户端连接。一旦客户端连接到服务器，服务器就可以使用Socket与客户端进行数据通信。客户端也可以创建一个Socket，然后连接到服务器，进行数据交换。二、实战使用Java编程语言编写Socket程序是相对简单的，以下是一个简单的例子，说明如何使用Socket在Java中进行客户端和服务器之间的通信： 服务器端的代码如下：import java.io.*;import java.net.*;public class Server { public static void main(String[] args) throws IOException { ServerSocket serverSocket = new ServerSocket(5555); //创建客户端socket对象 用于接收客户端的信息 Socket clientSocket = serverSocket.accept(); //创建输出流以及输入流 便于操作 //创建输出流 用于向客户端返回信息 PrintWriter out = new PrintWriter(clientSocket.getOutputStream(), true); //BufferedReader 是 Java 中用于读取字符流的缓冲输入流类 BufferedReader in = new BufferedReader(new InputStreamReader(clientSocket.getInputStream())); String inputLine, outputLine; //当接收的信息不为空时，以以下该格式输出返回给客户端 while ((inputLine = in.readLine()) != null) { outputLine = \"Server received: \" + inputLine; out.println(outputLine); //当客户端发送：Bye. 则跳出循环并且关闭相关资源 if (inputLine.equals(\"Bye.\")) { break; } } //输出流关闭 out.close(); //输入流关闭 in.close(); //客户端与服务端连接关闭 clientSocket.close(); serverSocket.close(); }} 客户端的代码如下：import java.io.*;import java.net.*;public class Client { public static void main(String[] args) throws IOException { //服务端的ip地址以及端口号 String hostName = \"localhost\"; int portNumber = 5555; try ( //创建socket对象 Socket socket = new Socket(hostName, portNumber); //创建输出流对象 用于向服务端发送信息 PrintWriter out = new PrintWriter(socket.getOutputStream(), true); //创建输入流对象 用于读取服务器发过来的数据 BufferedReader in = new BufferedReader(new InputStreamReader(socket.getInputStream())); //创建输入流对象 用于从 标准输入 读取数据，即客户端输入的数据 BufferedReader stdIn = new BufferedReader(new InputStreamReader(System.in)) ) { //用户输入 String userInput; while ((userInput = stdIn.readLine()) != null) { out.println(userInput); System.out.println(\"Server says: \" + in.readLine()); //输出服务端传递过来的数据 if (userInput.equals(\"Bye.\")) { break; } } } catch (UnknownHostException e) { //异常处理 System.err.println(\"Don't know about host \" + hostName); System.exit(1); } catch (IOException e) { System.err.println(\"Couldn't get I/O for the connection to \" + hostName); System.exit(1); } }}}这个例子是一个简单的echo服务器，客户端向服务器发送消息，服务器将其原样返回。在这个例子中，服务器在5555端口监听客户端连接请求，客户端连接到服务器，发送消息，并接收来自服务器的响应。当客户端发送”Bye.”时，连接被关闭。总之，使用Java编程语言编写Socket程序相对简单，需要使用Java提供的Socket API，通过创建ServerSocket和Socket实例，来实现客户端和服务器之间的通信。通过上述代码，你可以简单了解如何使用Socket进行数据交换。三、总结Socket是一种通信协议和API，用于在计算机之间进行数据交换。它是网络编程中非常重要的一部分，使应用程序可以通过TCP或UDP协议进行数据通信。" }, { "title": "什么是RPC及其调用原理", "url": "/posts/RPC/", "categories": "技术科普", "tags": "学习", "date": "2023-02-19 14:15:00 +0000", "snippet": "什么是RPC及其调用原理一、RPC的定义RPC(Remote Procedure Call Protocol)–远程过程调用协议，它是一种通过网络从远程计算机程序上请求服务，而不需要了解底层网络技术的协议。RPC协议假定某些传输协议的存在，如TCP或UDP，为通信程序之间携带信息数据。在OSI网络通信模型中，RPC跨越了传输层和应用层。RPC使得开发包括网络分布式多程序在内的应用程序更加容易...", "content": "什么是RPC及其调用原理一、RPC的定义RPC(Remote Procedure Call Protocol)–远程过程调用协议，它是一种通过网络从远程计算机程序上请求服务，而不需要了解底层网络技术的协议。RPC协议假定某些传输协议的存在，如TCP或UDP，为通信程序之间携带信息数据。在OSI网络通信模型中，RPC跨越了传输层和应用层。RPC使得开发包括网络分布式多程序在内的应用程序更加容易。RPC采用客户机/服务器模式。请求程序就是一个客户机，而服务提供程序就是一个服务器。二、RPC调用所需要具备的能力RPC调用的是一个远端对象，调用者和被调用者是处于不同的地址空间，要想完成调用，必须实现以下几个能力：1.服务发现​\t由于调用的是远端对象，因此需要可以定位到调用的服务器地址以及调用的具体方法的进程，一般可以通过注册中心或者哈希查找来实现，将被调的接口注册到某个地方，比如使用CallerId和函数名作为映射，调用者通过CallerId进行调用，服务端收到调用请求后，通过CallerId查找到对应函数，再执行后续处理流程。2.序列化和反序列化​\t调用者和被调用者处于不同的机器上，因此函数参数及其他一些Context信息需要经过网络传输才能到达服务端，网络传输的是二进制数据，因此RPC需要支持将函数参数（如基本类型int、long、map以及自定义类型等）可以序列化为网络可以传输的二进制数据，同时，也需要支持将二进制数据反序列化为程序可以处理的数据类型，是序列化的逆过程。3.网络传输​\t网络传输协议和RPC本身没有关系，一般使用TCP进行网络传输，通过HTTP或者UDP也是可以完成RPC所需要的功能的。如上，如果某个服务框架实现了上述这些功能，就是一个RPC框架，通过此框架便可以像调用本地方法一样来调用远端方法。三、为什么要使用RPC？​\t如果是一个功能比较简单的应用，可以将所有功能实现在一个服务中或者实现在多个服务中，但是部署在一个机器上，如此，接口间调用要么在同一进程内或者是本地进程间通信。随着业务的发展，服务功能持续迭代，单体应用出现性能瓶颈，因此需要考虑对服务进行拆分，根据业务功能划分为不同的模块，不同的模块部署到不同集群上，模块间进行通信完成功能。如将服务的管理域和执行域进行拆分，不仅是服务架构更合理清晰，也方便根据不同的模块划分不同的资源，保证服务的整体性能。​\t一个服务拆分为不同的模块，或者单体应用拆分为多个微服务时，此时便需要RPC出场了，不同模块及不同服务间都需要RPC才能完成通信。可以说RPC是分布式系统架构或者微服务架构必不可少的实现手段。四、RPC的调用原理RPC框架的实现原理如下：当客户端发起一次远程调用时： client以本地方式方式调用远程方法； client stub收到调用后负责将方法以及参数等组装成网络可以传输的消息体； client stub找到服务端地址，通过系统调用，将数据传输服务端 server stub收到消息后进行反序列化； server stub通过得到的参数调用本地方法； 执行结果返回给server stub； server stub将执行结果进行序列化； server stub进行系统调用将数据发送给客户端； client stub收到消息并进行解码； cleint stub将结果返回给调用者。​\tRPC框架所要实现的便是将序列化、反序列化以及网络传输等流程进行封装，这些流程对于调用者来说是透明的，调用者并不需要了解这些细节以及调用流程。五、常见的RPC框架有哪些？1.Thrift​\tThrift是一个轻量级、跨语言的RPC框架，主要用于各个服务之间的RPC通信，最初由Facebook于2007年开发，2008年进入Apache开源项目。它通过自身的IDL中间语言，并借助代码生成引擎生成各种主流语言的RPC服务端和客户端模板代码。Thrift支持多种不同的编程语言，包括C++、Java、Python、PHP、Ruby、Erlag、Haskell、C#、Cocoa、JavaScript、Nodejs、Smalltalk、OCaml和Golang等。2.gRPC​\tgRPC是一个高性能、通用的开源RPC框架，其由Google2015年主要面向移动应用开发并基于HTTP/2标准协议而设计，基于ProtoBuf序列化协议开发，且支持众多语言开发。gRPC提供了一种简单的方法来精确地定义服务和为iOS、Android和后台支持服务自动生成可靠性很强的客户端功能库。客户端充分利用高级流和链式功能，从而有助于节省带宽、降低TCP连接次数、节省CPU使用、电池寿命。支持C、C++、Node.js、Python、Ruby、Objective-C、PHP和C#等语言3.Dubbo​\tDubbo是一个分布式服务框架，以及SOA治理方案。其功能主要包括：高性能NIO通讯及多协议集成，服务动态寻址与路由，软负载均衡与容错，依赖分析与降级等。 Dubbo是阿里巴巴内部的SOA服务化治理方案的核心框架，Dubbo自2011年开源后，已被许多非阿里系公司使用，目前Dubbo已经交给Apache基金会维护。" }, { "title": "本地锁到分布式锁的演变", "url": "/posts/Block/", "categories": "技术科普", "tags": "学习", "date": "2023-02-04 09:44:00 +0000", "snippet": "本地锁到分布式锁的演变一、本地锁1.1、本地锁的使用本地锁主要是针对单体服务而言的，锁的都是单体应用内的进程。像之前在单机情况下出现的读写并发情况。因为并发情况下网络出现问题或是出现其他卡顿问题，导致执行顺序发生变化，从而产生了数据不一致性。如下图：解决并发最快的方式就是加锁吗，我们也就给它来把锁吧，Java中的锁是有蛮多的，我这里不过多讨论啦（synchronized、JUC）等等。我案例...", "content": "本地锁到分布式锁的演变一、本地锁1.1、本地锁的使用本地锁主要是针对单体服务而言的，锁的都是单体应用内的进程。像之前在单机情况下出现的读写并发情况。因为并发情况下网络出现问题或是出现其他卡顿问题，导致执行顺序发生变化，从而产生了数据不一致性。如下图：解决并发最快的方式就是加锁吗，我们也就给它来把锁吧，Java中的锁是有蛮多的，我这里不过多讨论啦（synchronized、JUC）等等。我案例中所使用的是 JUC 包下的读写锁ReentrantReadWriteLock ，毕竟不能让锁直接限制了Redis 的发挥~，读写锁是读并发，写独占的模式。增加读写锁之后的流程图如下：（图片说明：加上锁之后的流程）案例代码如下：@Slf4j@Servicepublic class RedisCacheServiceImpl implements IRedisCacheService { @Autowired private MenuMapper menuMapper; @Autowired StringRedisTemplate redisTemplate; private static final String REDIS_MENU_CACHE_KEY = \"menu:list\"; private ReentrantReadWriteLock readWriteLock = new ReentrantReadWriteLock(); @Override public List&lt;MenuEntity&gt; getList() { //1、从缓存中获取 String menuJson = redisTemplate.opsForValue().get(REDIS_MENU_CACHE_KEY); if (!StringUtils.isEmpty(menuJson)) { System.out.println(\"缓存中有，直接从缓存中获取\"); //2、缓存不为空直接返回 List&lt;MenuEntity&gt; menuEntityList = JSON.parseObject(menuJson, new TypeReference&lt;List&lt;MenuEntity&gt;&gt;() { }); return menuEntityList; } //3、查询数据库 //不加锁情况下 // List&lt;MenuEntity&gt; noLockList = getMenuJsonFormDb(); // 加锁情况下 List&lt;MenuEntity&gt; menuEntityList = getMenuJsonFormDbWithReentrantReadWriteLock(); return menuEntityList; } public List&lt;MenuEntity&gt; getMenuJsonFormDb() { System.out.println(\"缓存中没有，重新从数据中查询~==&gt;\"); //缓存为空，查询数据库，重新构建缓存 List&lt;MenuEntity&gt; result = menuMapper.selectList(new QueryWrapper&lt;MenuEntity&gt;()); //4、将查询的结果，重新放入缓存中 redisTemplate.opsForValue().set(REDIS_MENU_CACHE_KEY, JSON.toJSONString(result)); return result; } public List&lt;MenuEntity&gt; getMenuJsonFormDbWithReentrantReadWriteLock() { List&lt;MenuEntity&gt; result = null; System.out.println(\"缓存中没有，加锁，重新从数据中查询~==&gt;\"); // synchronized 是同步锁，所以当多个线程同时执行到这里时，会阻塞式等待 ReentrantReadWriteLock.ReadLock readLock = readWriteLock.readLock(); readLock.lock(); try { String menuJson = redisTemplate.opsForValue().get(REDIS_MENU_CACHE_KEY); //加锁成功... 再次判断缓存是否为空 if (!StringUtils.isEmpty(menuJson)) { System.out.println(\"缓存中，直接从缓存中获取\"); //2、缓存不为空直接返回 List&lt;MenuEntity&gt; menuEntityList = JSON.parseObject(menuJson, new TypeReference&lt;List&lt;MenuEntity&gt;&gt;() { }); return menuEntityList; } //缓存为空，查询数据库，重新构建缓存 result = menuMapper.selectList(new QueryWrapper&lt;MenuEntity&gt;()); //4、将查询的结果，重新放入缓存中 redisTemplate.opsForValue().set(REDIS_MENU_CACHE_KEY, JSON.toJSONString(result)); return result; } finally { readLock.unlock(); } } @Override public Boolean updateMenuById(MenuEntity menu) { //return updateMenuByIdNoWithLock(menu); return updateMenuByIdWithReentrantReadWriteLock(menu); } public Boolean updateMenuByIdNoWithLock(MenuEntity menu) { // 1、删除缓存 redisTemplate.delete(REDIS_MENU_CACHE_KEY); System.out.println(\"清空单机Redis缓存\"); // 2、更新数据库 return menuMapper.updateById(menu) &gt; 0; } public Boolean updateMenuByIdWithReentrantReadWriteLock(MenuEntity menu) { ReentrantReadWriteLock.WriteLock writeLock = readWriteLock.writeLock(); writeLock.lock(); try { // 1、删除缓存 System.out.println(\"清除缓存\"); redisTemplate.delete(REDIS_MENU_CACHE_KEY); // 2、更新数据库 return menuMapper.updateById(menu) &gt; 0; }finally { writeLock.unlock(); } }}复制代码具体还需大家去了解~我这里加的 JUC 下的读写锁，原本我想的是弄的JUC的锁1.2、本地锁存在的问题看起来本地锁没有并发问题，不管有多少请求一起进来，都要去争取那唯一的一把锁，抢到了才能继续往下执行业务。单体项目中，每把锁锁的就是当前服务中的当前线程的请求。（图片说明：单体服务时）但是当服务需要进一步扩展时，就会随之产生出一些问题。多服务并发时，如果还是只给当前线程加锁，多个用户一起尝试获取锁时，可能会有多个用户同时获取到锁，导致出现问题。如下图： 每个服务都是单独的，加锁操作也只是给自己的，大家不能共享，那么实际上在高并发的时候，是根本没效果的。我1号服务抢到了锁，还没等到释放，2号服务又获取到了锁，接着3号、4号等等，大家都可以操作数据库，这把锁也就失去了它该有的作用。因此就进一步出现了分布式锁，接下来继续看吧。二、分布式锁的介绍本地锁失效是因为无法锁住各个应用的读写请求，失效的根本原因就是其他的服务无法感知到是否已经有请求上锁了，即无法共享锁信息。分布式锁，其实也就是将加锁的这一个操作，单独的抽取出来了，让每个服务都能感知到。之前就说了，软件架构设计中，”没有什么是加一层解决不了的，如果加一层不行就再加一层“。这里其实也是一样，只不过碰巧这一层可以在Redis中实现罢了，看起来倒是没有多加一层，但如果是用Zookeeper 或者其他方式来实现，你会发现架构中会多一层滴。其实理解思想实现的方式有很多种的， Redis 实现分布式锁 Zookeeper 实现分布式锁 MySQL 专门用一张表来记录信息，实现分布式锁，也是常说的基于数据库实现分布式锁。所谓的加锁，其本质也就是判断一个信号量是否存在罢了，分布式也就是把这个信号量从本地线程中，移植到了Redis中存储，让所有服务中的请求都能共享一把锁。知道思想后，实现方式并不局限，大家也不要局限了自己，都已经站在巨人肩膀上，就要想的更多一些~我采用 Redis 实现分布式锁，主要原因： Redis 是基于内存操作的数据库，速度快； 市场主流的数据库，拥有较多的参考资料； Redis 社区开发者活跃，并且 Redis 对分布式锁有较好的支持；今天所讨论的，主要就是针对于使用 Redis 实现分布式锁，流程图大致如下：（图片说明：此图为获取锁的大致流程，其之后的构建缓存、释放锁等未在图上所标明）虽然两个服务都是独立的，但是在执行数据库代码前，都需要先获取到读锁或者写锁，以确保并发时执行的正确性~接下来就是说分布式锁的实现啦~三、分布式锁的实现在上一小节就已说分布式锁的实现有多种方式，大的范围中有 Redis、Zookeeper、MySQL等实现方式，我具体讲的是以 Redis 的实现。讲解过程也是逐步深入，逐步演进，并非是直接丢出实现代码，针对为什么要这么做，为什么最终是这样，让大家有一个了解过程。锁的第一个要求就是要能做到互斥，而在Redis中最容易想到，也是最简单的，无疑就是 setnx 命令。我们就以 setnx抛砖引玉，来对分布式锁的实现，做一个逐步演进的讨论。3.1、setnxRedis Setnx（ SET IF Not EXists ）命令在指定的 key 不存在时，为 key 设置指定的值，这种情况下等同 SET 命令。当 key存在时，什么也不做。返回值 如果key设置成功了，则返回1 如果key设置失败了，则返回0redis&gt; SETNX mykey \"Hello\"(integer) 1redis&gt; SETNX mykey \"World\"(integer) 0redis&gt; GET mykey\"Hello\"复制代码转换到加锁的流程中就是，当读请求过来的时候，先用setnx命令往 Redis 中设置一个值，返回1则是加锁成功了，返回0则是加锁失败了。如何释放锁呢？setnx其本质和set命令其实差不多，都是往 Redis 中设置一个值，释放锁，也就是删除这个key值，使用 del key 命令删除即等于释放锁~思路👨‍🏫： 读请求过来，判断缓存中是否存在数据，存在立马返回，不存在则往下走。 读请求尝试获取读锁，使用 setnx 命令 向Redis中尝试set一个值，当且仅当值不存在时设置成功。 设置成功返回 1，否则返回 0 获取成功后，查询数据库，重新构建缓存。代码案例实现：@Servicepublic class SetNxExLockServiceImpl implements ISetNxExLockService { @Autowired StringRedisTemplate stringRedisTemplate; @Autowired private MenuMapper menuMapper; private static final String SET_NX_EX_MENU_LIST = \"set:nx:ex:menu:list\"; private static final String SET_NX_EX_MENU_LIST_LOCK = \"set:nx:ex:menu:list:lock\"; private static final String SET_NX_EX_MENU_LIST_LOCK_VALUE = \"lock\"; @Override public List&lt;MenuEntity&gt; getList() { // 判断缓存是否有数据 String menuJson = stringRedisTemplate.opsForValue().get(SET_NX_EX_MENU_LIST); if (menuJson != null) { System.out.println(\"缓存中有，直接返回缓存中的数据\"); List&lt;MenuEntity&gt; menuEntityList = JSON.parseObject(menuJson, new TypeReference&lt;List&lt;MenuEntity&gt;&gt;() { }); return menuEntityList; } // 从数据库中查询 List&lt;MenuEntity&gt; result = getMenuJsonFormDbWithLock(); return result; } /** * 问题：死锁问题， 如果在执行解锁操作之前，服务突然宕机，那么锁就永远无法被释放，从而造成死锁问题 * 解决方案: 因为 Redis 的更新，现在 setIfAbsent 支持同时设置过期时间，而无需分成两步操作。 * * @return */ public List&lt;MenuEntity&gt; getMenuJsonFormDbWithLock() { List&lt;MenuEntity&gt; result = null; System.out.println(\"缓存中没有，加锁，重新从数据中查询~==&gt;\"); Boolean lock = stringRedisTemplate.opsForValue().setIfAbsent(SET_NX_EX_MENU_LIST_LOCK, SET_NX_EX_MENU_LIST_LOCK_VALUE); if (lock) { System.out.println(\"获取分布式锁成功...\"); try { //加锁成功...执行业务 result = menuMapper.selectList(new QueryWrapper&lt;MenuEntity&gt;()); stringRedisTemplate.opsForValue().set(SET_NX_EX_MENU_LIST, JSON.toJSONString(result)); } finally { // 释放锁 stringRedisTemplate.delete(SET_NX_EX_MENU_LIST_LOCK); } return result; } else { System.out.println(\"获取分布式锁失败...等待重试...\"); //加锁失败...重试机制 //休眠一百毫秒 try { TimeUnit.MILLISECONDS.sleep(100); } catch (InterruptedException e) { e.printStackTrace(); } // 手动实现自旋 return getMenuJsonFormDbWithLock(); } } // 更新操作 @Override public Boolean updateMenuById(MenuEntity menu) { return updateMenuByIdWithLock(menu); } public Boolean updateMenuByIdWithLock(MenuEntity menu) { Boolean lock = stringRedisTemplate.opsForValue().setIfAbsent(SET_NX_EX_MENU_LIST_LOCK, SET_NX_EX_MENU_LIST_LOCK_VALUE); Boolean update = false; if (lock) { System.out.println(\"更新操作：获取分布式锁成功===&gt;\"); // 删除缓存 try { stringRedisTemplate.delete(SET_NX_EX_MENU_LIST); // 更新数据库 update = menuMapper.updateById(menu) &gt; 0; } finally { // 一定要释放锁，以免造成死锁问题 stringRedisTemplate.delete(SET_NX_EX_MENU_LIST_LOCK); } return update; }else{ try { Thread.sleep(100); } catch (InterruptedException e) { throw new RuntimeException(e); } return updateMenuByIdWithLock(menu); } }}复制代码注意：代码中有加锁操作，就一定要记得将解锁操作放到了finally执行，以保证在代码层面不会出现死锁问题。你觉得上面的案例存在什么样的问题吗？咋一看，上面的好像确实实现了分布式锁，setIfAbsent()方法实现了锁的互斥性，Redis 也让锁得到共享，但是真的没问题吗？思考🧐：如果某个时刻代码执行到释放锁的那一步，服务突然挂了，是不是也就意味着锁释放失败了勒，那么也就造成了最大的问题死锁，因为没有解锁操作，锁也就将无法被释放。那该如何解决呢？最佳的方式就是给锁设置一个过期时间~3.2、解决死锁问题最容易想到也是最简单的方式就是给这个分布式锁加个过期时间，比如3s、5s之类，使用EXPIRE设置一个过期时间但如果想到的是这种：redis&gt; SETNX mykey \"Hello\"(integer) 1redis&gt; EXPIRE mykey 10(integer) 1redis&gt; SETNX mykey \"World\"(integer) 0redis&gt; TTL mykey(integer) 8redis&gt; GET mykey\"Hello\"复制代码如果是这么实现的话，咱们就还是对于 Redis 了解太少了。首先说说这样操作会出现的问题： 我成功设置了key，但是还没执行到设置时间那一步，应用程序突然挂了，导致死锁问题产生。 或者是成功设置了 key 后，Redis 服务突然崩溃了，不干活了，这也导致了后面的EXPIRE无法执行成功，同样会产生死锁问题。【重点】：因为加锁和设置时间不是一个原子性操作怎么才能将加锁和设置锁时间变成一个原子操作呢？其实这一步Redis已经帮我们做好啦~Redis 中的set命令是可以添加参数的，一条set命令即可实现SETNX+EXPIRE效果，将加锁和设置时间变成一个原子性操作，要么一起成功，要么一起失败。完整命令参数：文档链接 SET key value [EX seconds PX milliseconds KEEPTTL] [NX XX] [GET] EX seconds – 设置键key的过期时间，单位时秒 PX milliseconds – 设置键key的过期时间，单位时毫秒 NX – 只有键key不存在的时候才会设置key的值 XX – 只有键key存在的时候才会设置key的值 KEEPTTL – 获取 key 的过期时间 GET – 返回 key 存储的值，如果 key 不存在返回空注意: 由于SET命令加上选项已经可以完全取代SETNX, SETEX, PSETEX, GETSET,的功能，所以在将来的版本中，redis可能会不推荐使用并且最终抛弃这几个命令。返回值字符串: 如果SET命令正常执行那么回返回OK 多行字符串: 使用 GET 选项，返回 key 存储的值，如果 key 不存在返回空: 否则如果加了NX 或者 XX选项，SET 没执行，那么会返回nil。例子：从这个小案例中可以看出，这是符合我们要的命令的~Java 代码实现：这一步的代码实现和上一小节相比，仅改动了一行代码：上一小节：Boolean lock = stringRedisTemplate.opsForValue().setIfAbsent(SET_NX_EX_MENU_LIST_LOCK, SET_NX_EX_MENU_LIST_LOCK_VALUE);复制代码加上过期时间：Boolean lock = stringRedisTemplate.opsForValue().setIfAbsent(SET_NX_EX_MENU_LIST_LOCK, SET_NX_EX_MENU_LIST_LOCK_VALUE, 5L, TimeUnit.SECONDS);复制代码就这样，再那样，再这样，你看死锁问题就被解决啦😜死锁问题确实被解决了，现在你觉得还有问题吗？我们把它的每一个步骤都拆分来看，就会出现下面的这样一个场景：假设我们约定锁过期时间为5s，但是执行这个业务时突然卡起来，此业务执行时间超过了我们预估的 5s，那么就可能出现以下情况：第一条线程抢到锁，业务执行超时，第一条线程所持有的锁被自动释放；此时第二条线程拿到锁，准备执行业务，刚好第一条线程业务执行完成，照常执行了释放锁的过程，导致第二条线程持有的锁被第一条线程所释放，锁被其他人释放。这个情况中存在两个问题： 业务执行超时，锁被自动释放 锁被其他人释放，导致业务出现问题关于第一个问题，就是常说的锁续期问题，这点之后在使用 Redission 时再细谈。第二个问题，就比较好解决了，我们每次加锁的时候，带上自己的身份标识，在解锁的时候，进行一次判断即可。接着往下看吧 👇3.3、锁被其他人释放，该怎么办？我们每次加锁的时候，带上自己的身份标识，在解锁的时候，进行一次判断即可。比如：加锁的时候，生成一个UUID作为 KEY，释放锁时，获取一下锁，判断一下UUID是否相等，相等则执行删除，否则不执行删除。在 Redis 中命令演示如下：设置锁：set uuid \"lock\" NX EX 60释放锁：1、get uuid，身份标识相等，则执行2；2、 del uuidJava 代码如下：@Servicepublic class SetNxExLockServiceImpl implements ISetNxExLockService { @Autowired StringRedisTemplate stringRedisTemplate; @Autowired private MenuMapper menuMapper; private static final String SET_NX_EX_MENU_LIST = \"set:nx:ex:menu:list\"; private static final String SET_NX_EX_MENU_LIST_LOCK = \"set:nx:ex:menu:list:lock\"; private static final String SET_NX_EX_MENU_LIST_LOCK_VALUE = \"lock\"; @Override public List&lt;MenuEntity&gt; getList() { // 判断缓存是否有数据 String menuJson = stringRedisTemplate.opsForValue().get(SET_NX_EX_MENU_LIST); if (menuJson != null) { System.out.println(\"缓存中有，直接返回缓存中的数据\"); List&lt;MenuEntity&gt; menuEntityList = JSON.parseObject(menuJson, new TypeReference&lt;List&lt;MenuEntity&gt;&gt;() { }); return menuEntityList; } // 从数据库中查询 List&lt;MenuEntity&gt; result = getMenuJsonFromDbWithRedisLock(); return result; } /** * 问题：锁被其他人释放，这该如何处理。 * 加锁的时候，将值给定位一个唯一的标识符（我这里使用的是 UUID ） * 1、解锁之前，先判断是不是自己获取的那把锁， * 2、确定是一把锁就执行 解锁锁操作 * * @return */ public List&lt;MenuEntity&gt; getMenuJsonFromDbWithRedisLock() { List&lt;MenuEntity&gt; result = new ArrayList&lt;&gt;(); System.out.println(\"缓存中没有，加锁，重新从数据中查询~==&gt;\"); // 给锁设定一个时间 String uuid = UUID.randomUUID().toString(); Boolean lock = stringRedisTemplate.opsForValue().setIfAbsent(SET_NX_EX_MENU_LIST_LOCK, uuid, 5L, TimeUnit.SECONDS); if (lock) { System.out.println(\"获取分布式锁成功...\"); try { //加锁成功...执行业务 result = menuMapper.selectList(new QueryWrapper&lt;MenuEntity&gt;()); stringRedisTemplate.opsForValue().set(SET_NX_EX_MENU_LIST, JSON.toJSONString(result)); } finally { // 获取锁，判断是不是当前线程的锁 String token = stringRedisTemplate.opsForValue().get(SET_NX_EX_MENU_LIST_LOCK); if (uuid.equals(token)) { // 确定是同一把锁， 才释放锁 stringRedisTemplate.delete(SET_NX_EX_MENU_LIST_LOCK); } } return result; /** * 那这样就没有问题了吗？ * 并不是。 * 这里存在的问题也很明显，删除操作已经不在是一个原子性操作了。 * 1、一个是查询判断 * 2、第二个才是解锁操作 * 那么又会拆分成，如果我第一步执行成功，第二步执行失败的场景，所以我们要把它变成原子操作才行。 */ } else { System.out.println(\"获取分布式锁失败...等待重试...\"); //加锁失败...重试机制 //休眠一百毫秒 try { TimeUnit.MILLISECONDS.sleep(100); } catch (InterruptedException e) { e.printStackTrace(); } return getMenuJsonFromDbWithRedisLock(); } } @Override public Boolean updateMenuById(MenuEntity menu) { // return updateMenuByIdWithLock(menu); // return updateMenuWithExpireLock(menu); return updateMenuWithRedisLock(menu); } public Boolean updateMenuWithExpireLock(MenuEntity menu) { // 给锁设定一个时间 Boolean lock = stringRedisTemplate.opsForValue().setIfAbsent(SET_NX_EX_MENU_LIST_LOCK, SET_NX_EX_MENU_LIST_LOCK_VALUE, 5L, TimeUnit.SECONDS); Boolean update = false; if (lock) { System.out.println(\"更新操作：获取分布式锁成功===&gt; 清除缓存\"); try { // 删除缓存 stringRedisTemplate.delete(SET_NX_EX_MENU_LIST); // 更新数据库 update = menuMapper.updateById(menu) &gt; 0; } finally { // 一定要释放锁，以免造成死锁问题 stringRedisTemplate.delete(SET_NX_EX_MENU_LIST_LOCK); } return update; } else{ try { Thread.sleep(100); } catch (InterruptedException e) { throw new RuntimeException(e); } return updateMenuWithExpireLock(menu); } }}复制代码那么这就没有问题了吗？并不是。这里存在的问题也很明显，删除操作又已经不再是一个原子性操作了。 第一步是查询判断 第二步才是解锁操作那么继而又会出现，我第一步执行成功，第二步执行失败的场景，所以我们必须要把它变成原子性操作才行。这个时就要用到了 Redis 中的 lua 脚本啦~让它去帮助我们实现将判断和解锁变成一步原子性操作~ 接着看吧3.4、lua 脚本实现分布式锁其实有阅读过 Redis 官方文档的朋友，在看上面的那个 set命令的文档时，就会发现，其实滑到下半部分，Redis 就有提到不推荐使用SET resource-name anystring NX EX max-lock-time 这个简单方法来实现分布式锁~并且也给出了相应的建议：如下图if redis.call(\"get\",KEYS[1]) == ARGV[1]then return redis.call(\"del\",KEYS[1])else return 0end复制代码我们使用的其实就是上面的脚本，哈哈@Servicepublic class SetNxExLockServiceImpl implements ISetNxExLockService { @Autowired StringRedisTemplate stringRedisTemplate; @Autowired private MenuMapper menuMapper; private static final String SET_NX_EX_MENU_LIST = \"set:nx:ex:menu:list\"; private static final String SET_NX_EX_MENU_LIST_LOCK = \"set:nx:ex:menu:list:lock\"; private static final String SET_NX_EX_MENU_LIST_LOCK_VALUE = \"lock\"; @Override public List&lt;MenuEntity&gt; getList() { // 判断缓存是否有数据 String menuJson = stringRedisTemplate.opsForValue().get(SET_NX_EX_MENU_LIST); if (menuJson != null) { System.out.println(\"缓存中有，直接返回缓存中的数据\"); List&lt;MenuEntity&gt; menuEntityList = JSON.parseObject(menuJson, new TypeReference&lt;List&lt;MenuEntity&gt;&gt;() { }); return menuEntityList; } // 从数据库中查询 List&lt;MenuEntity&gt; result = getMenuJsonFormDbWithLock(); return result; } /** * 问题：释放锁操作 丢失 原子性 * 解决方案: lua 脚本 * 既然删除操作变成了两步，失去了原子性，那么我们就把它改成一步就行啦呀， * 此时就得用上我们的 lua 脚本了 * * @return */ public List&lt;MenuEntity&gt; getMenuJsonFormDbWithLuaLock() { List&lt;MenuEntity&gt; result = new ArrayList&lt;&gt;(); System.out.println(\"缓存中没有，加锁，重新从数据中查询~==&gt;\"); // 给锁设定一个时间 String uuid = UUID.randomUUID().toString(); Boolean lock = stringRedisTemplate.opsForValue().setIfAbsent(SET_NX_EX_MENU_LIST_LOCK, uuid, 5L, TimeUnit.SECONDS); if (lock) { System.out.println(\"获取分布式锁成功...\"); try { //加锁成功...执行业务 result = menuMapper.selectList(new QueryWrapper&lt;MenuEntity&gt;()); stringRedisTemplate.opsForValue().set(SET_NX_EX_MENU_LIST, JSON.toJSONString(result)); } finally { // 编写 lua 脚本 String script = \"if redis.call('get', KEYS[1]) == ARGV[1] then return redis.call('del', KEYS[1]) else return 0 end\"; //删除锁 execute 这个放番中的参数可能还需要提一提 stringRedisTemplate.execute(new DefaultRedisScript&lt;Long&gt;(script, Long.class), Arrays.asList(SET_NX_EX_MENU_LIST_LOCK), uuid); } return result; } else { System.out.println(\"获取分布式锁失败...等待重试...\"); //加锁失败...重试机制 //休眠一百毫秒 try { TimeUnit.MILLISECONDS.sleep(100); } catch (InterruptedException e) { e.printStackTrace(); } return getMenuJsonFormDbWithLuaLock(); } } @Override public Boolean updateMenuById(MenuEntity menu) {// return updateMenuByIdWithLock(menu);// return updateMenuWithExpireLock(menu); return updateMenuWithLuaLock(menu); } /** * 问题：释放锁操作 丢失 原子性 * 解决方案: lua 脚本 * 既然删除操作变成了两步，失去了原子性，那么我们就把它改成一步就行啦呀， * 此时就得用上我们的 lua 脚本了 * * @return */ public Boolean updateMenuWithRedisLock(MenuEntity menu) { String uuid = UUID.randomUUID().toString(); Boolean lock = stringRedisTemplate.opsForValue().setIfAbsent(SET_NX_EX_MENU_LIST_LOCK, uuid, 5L, TimeUnit.SECONDS); Boolean update = false; if (lock) { System.out.println(\"获取分布式锁成功...\"); try { //加锁成功...执行业务 stringRedisTemplate.delete(SET_NX_EX_MENU_LIST); // 更新数据库 update = menuMapper.updateById(menu) &gt; 0; } finally { // 获取锁，判断是不是当前线程的锁 String token = stringRedisTemplate.opsForValue().get(SET_NX_EX_MENU_LIST_LOCK); if (uuid.equals(token)) { // 确定是同一把锁， 才释放锁 stringRedisTemplate.delete(SET_NX_EX_MENU_LIST_LOCK); } } return update; } else{ try { Thread.sleep(100); } catch (InterruptedException e) { throw new RuntimeException(e); } return updateMenuWithRedisLock(menu); } }}复制代码借助 lua 脚本我们成功将释放锁的操作变成原子性操作，确保了其正确性。写到这里，我们对于分布式锁的来龙去脉，应该产生了一些属于自己的理解，现在就只剩下一个锁自动续期的问题没有解决了~3.5、Redisson 实现分布式锁锁需要续期，主要就是为了解决在一些业务场景中，业务执行超时，锁已经过期，但业务仍没执行完成的场景。究其根本就是到底给锁多长时间算合适呢？这点其实是没法准确评估的。 如果不打算给锁自动续期的话，那么我觉得应当对锁的过期时间，适当的延长一些，以确保业务正确执行。如果你和我一样是一名Java开发者，想要去实现锁自动续期，这个方案在市面上已经有成熟的轮子Redisson 啦~在Redisson中，有一个著名的看门狗机制，当我们使用 Redisson 来实现分布式锁时，加锁时，每次都会默认设置过期时间30s，然后当业务执行超过10s，也就是锁时间还剩下 20s 时，它就会自动续期。光说不练假把式，我们直接来用代码实现一下看看吧引入依赖&lt;dependency&gt; &lt;groupId&gt;org.redisson&lt;/groupId&gt; &lt;artifactId&gt;redisson-spring-boot-starter&lt;/artifactId&gt; &lt;version&gt;3.17.6&lt;/version&gt;&lt;/dependency&gt;复制代码首先就是SpringBoot的起手式，编写MyRedissionConfig 类@Configurationpublic class MyRedissonConfig { /** * 所有对Redisson的使用都是通过RedissonClient * @return * @throws IOException */ @Bean(destroyMethod=\"shutdown\") public RedissonClient redissonClient() throws IOException { //1、创建配置 Config config = new Config(); // 连接必须要以 redis 开头~ 有密码填密码 config.useSingleServer().setAddress(\"redis://IP地址:6379\").setPassword(\"000415\"); //2、根据Config创建出RedissonClient实例 //Redis url should start with redis:// or rediss:// RedissonClient redissonClient = Redisson.create(config); return redissonClient; }}复制代码我们所有的操作都是基于RedissonClient 来实现的，将它注入到Spring 容器中之后，在需要的时候直接引入即可。另外 Redisson 它实现了 JUC 包下的大部分锁相关的实现，如果熟悉 JUC 的开发，使用这方面算是没什么学习成本的。如JUC 下的读写锁、信号量等。我们就来使用一下Redisson中的读写锁吧~/** * @description: * @author: Ning Zaichun * @date: 2022年09月20日 20:59 */@Servicepublic class RedissonServiceImpl implements IRedissonService { @Autowired StringRedisTemplate stringRedisTemplate; @Autowired private MenuMapper menuMapper; private static final String REDISSON_MENU_LIST = \"redisson:menu:list\"; private static final String REDISSON_MENU_LIST_LOCK_VALUE = \"redisson:lock\"; @Autowired private RedissonClient redissonClient; @Override public List&lt;MenuEntity&gt; getList() { // 判断缓存是否有数据 String menuJson = stringRedisTemplate.opsForValue().get(REDISSON_MENU_LIST); if (menuJson != null) { System.out.println(\"缓存中有，直接返回缓存中的数据\"); List&lt;MenuEntity&gt; menuEntityList = JSON.parseObject(menuJson, new TypeReference&lt;List&lt;MenuEntity&gt;&gt;() { }); return menuEntityList; } // 从数据库中查询 List&lt;MenuEntity&gt; result = getMenuJsonFromDbWithRedissonLock(); return result; } /** * 问题：其实写成上面那种模样，相对来说，也能解决很多时候的问题了，从头到尾看过来的话，其实也能发现就一个锁自动过期问题没有解决了。 * 但在实现这个之前，我还是说明一下，为什么说在没有解决锁自动过期问题时，就已经能应付大多数场景了。 * 重点在于如何评估 锁自动过期时间，锁自动过期时间到底设置多少合适呢？ * 其实如果对于业务理解较为透彻，对于这一部分的业务代码执行时间能有一个较清晰的估算，给定一个合适的时间，在不出现极端情况，基本都能应付过来了。 * &lt;p&gt; * 但是呢，很多时候，还是会怕这个万一的，万一真出现了，可能造成的损失就不止 一万了，哈哈。 * 解决方案 * 1、在市场主流的 Redission 中，针对这样的问题，已经有了解决方案。这也是Redission中常说的看门狗机制。 * &lt;p&gt; * 如果需要自己实现的思路： * 1、这方面的问题，也做了十分浅显的思考，我觉得应该还是依赖于定时任务去实现，但到底该如何实现这个定时任务，我还没法给出一个合适的解决方案。 或许我应该会尝试一下。 * * @return */ public List&lt;MenuEntity&gt; getMenuJsonFromDbWithRedissonLock() { System.out.println(\"从数据库中查询\"); //1、占分布式锁。去redis占坑 //（锁的粒度，越细越快:具体缓存的是某个数据，11号商品） product-11-lock //RLock catalogJsonLock = redissonClient.getLock(\"catalogJson-lock\"); //创建读锁 RReadWriteLock readWriteLock = redissonClient.getReadWriteLock(REDISSON_MENU_LIST_LOCK_VALUE); RLock rLock = readWriteLock.readLock(); List&lt;MenuEntity&gt; result = null; try { rLock.lock(); String menuJson = stringRedisTemplate.opsForValue().get(REDISSON_MENU_LIST); if (menuJson != null) { System.out.println(\"缓存中有，直接返回缓存中的数据\"); List&lt;MenuEntity&gt; menuEntityList = JSON.parseObject(menuJson, new TypeReference&lt;List&lt;MenuEntity&gt;&gt;() { }); return menuEntityList; } try { Thread.sleep(50000); } catch (InterruptedException e) { throw new RuntimeException(e); } //加锁成功...执行业务 //加锁成功...执行业务 result = menuMapper.selectList(new QueryWrapper&lt;MenuEntity&gt;()); // 构建缓存 stringRedisTemplate.opsForValue().set(REDISSON_MENU_LIST, JSON.toJSONString(result)); } finally { rLock.unlock(); } return result; } @Override public Boolean updateMenuById(MenuEntity menu) { return updateMenuWithRedissonLock(menu); } public Boolean updateMenuWithRedissonLock(MenuEntity menu) { RReadWriteLock readWriteLock = redissonClient.getReadWriteLock(REDISSON_MENU_LIST_LOCK_VALUE); RLock writeLock = readWriteLock.writeLock(); Boolean update = false; try { writeLock.lock(); //加锁成功...执行业务 //加锁成功...执行业务 update = menuMapper.updateById(menu) &gt; 0; } finally { writeLock.unlock(); } return update; }}复制代码为了给大家测试一下它的自动续期，我在它第一次查询数据库获取锁时，让线程睡了一会，来进行测试。测试：Redission的一些其他用法以及内部是如何实现锁续期的等等，这些都留在下一篇文章中啦。3.6、关于Redission的一些补充Redisson 默认锁过期时间 30s，一旦进行修改了的话，Redisson会取消此锁的自动续期机制。 这一步的源码在 RedissonLock的tryAcquireAsync中 另外 Redisson 自动续期机制，也被大家称为看门狗机制，每次在锁还剩下20秒的时候，又会自动续到30s。 默认时间在：关于Redisson底层的一些流程分析，在明天的关于 Redisson 源码浅析的文章当中。3.7、正确使用Redis的分布式事务锁其实就下面两点： 上锁和设置过期时间需要保证原子性；（加锁） 判断锁ID是否为自己所有和解锁需要保证原子性 （解锁）保证这两步才能保证正确使用分布式锁，另外则是对于锁的过期时间需要进行一个合理评估，适当延长锁过期时间，如果需要实现锁自动续期可采用现有的轮子 Redisson 来实现。四、小结4.1、回顾我们从本地锁一直讲到分布式锁，将Redis实现分布式锁中的一些问题，逐步进行了讲述。 从使用简单的 Redis 中的 SET KEY NX命令实现分布式锁 到使用SET KEY NX EX TIME 命令解决死锁问题 到增加身份标识（UUID） 解决锁被其他人释放问题 再到使用 Lua 脚本，将解锁操作变成原子性操作 最后讲述了Redisson实现分布式锁，解决了锁自动续期问题说它非常难的话，其实也没有，但是这是终点吗？4.2、扩展其实并不是，不知道你们有没有发现我上面所讲述的分布式锁，从始至终都是将Redis当作了一个实例来看待。但在真正的环境中，Redis 远不止一个实例，部署方式也是多样的，主从复制、哨兵模式、集群模式，主从集群等等，那么在这些情况下，按照上面的方式去编写分布式锁会不会有问题呢？ 你觉得呢？答案是只要牵扯到网络通信，那么必然就会产生问题。 （可以说在分布式中，网络永远都是处于个不可信的状态）举个最简单的例子：假设现在的部署方式是主从集群+哨兵模式，这样的好处是，当主库异常宕机时，哨兵可以实现「故障自动切换」，把从库提升为主库，继续提供服务，以此保证可用性。那假设现在我一个请求进来，刚获取到锁，然后主节点就挂了，此时锁还没有同步到从节点上去，即使之后完成了主从切换，但是此次所加的锁也已经丢失。因此在这样的基础上，Redis 官方继而又推出了 Redlock(红锁算法)。五、关于红锁 Redlock关于这些问题，Redis 的作者也提供了一些解决方案【Redlock】 也就是我们常说的红锁。官方文档：Redlock红锁分析 Martin Kleppmann（英国剑桥大学的一名分布式系统研究员）关于 Redlock的分析 Redis 作者 Antirez 对于Martin Kleppmann的分析回复两人的辩论都十分精彩，非常值得拜读，从中可以领略到诸多关于分布式的思考。Redlock 简单使用来自 Redis 官网5.1、红锁算法在算法的分布式版本中，我们假设我们有 N 个 Redis master。这些节点是完全独立的，所以我们不使用复制或任何其他隐式协调系统。我们已经描述了如何在单个实例中安全地获取和释放锁。我们理所当然地认为算法会使用这种方法在单个实例中获取和释放锁。在我们的示例中，我们设置了 N=5，这是一个合理的值，因此我们需要在不同的计算机或虚拟机上运行 5 个 Redis 主服务器，以确保它们以几乎独立的方式发生故障。为了获取锁，客户端执行以下操作： 它以毫秒为单位获取当前时间。 它尝试顺序获取所有 N 个实例中的锁，在所有实例中使用相同的键名和随机值。在步骤 2 中，当在每个实例中设置锁时，客户端使用一个与锁自动释放总时间相比较小的超时来获取它。例如，如果自动释放时间为 10 秒，则超时可能在 ~ 5-50 毫秒范围内。这可以防止客户端在尝试与已关闭的 Redis 节点通信时长时间保持阻塞：如果一个实例不可用，我们应该尽快尝试与下一个实例通信。 客户端通过从当前时间中减去步骤 1 中获得的时间戳来计算获取锁所用的时间。当且仅当客户端能够在大多数实例（至少 3 个）中获取锁时，且获取锁的总时间小于锁的有效时间，则认为锁已被获取。 如果获得了锁，则其有效时间被认为是初始有效时间减去经过的时间，如步骤 3 中计算的那样。 如果客户端由于某种原因未能获得锁（它无法锁定 N/2+1 个实例或有效时间为负数），它将尝试解锁所有实例（即使是它认为没有的实例）能够锁定）。5.2、算法是异步的吗？该算法依赖于这样一个假设，即虽然进程之间没有同步时钟，但每个进程中的本地时间以大致相同的速率更新，与锁的自动释放时间相比误差很小。这个假设非常类似于现实世界的计算机：每台计算机都有一个本地时钟，我们通常可以依靠不同的计算机来获得很小的时钟漂移。在这一点上，我们需要更好地指定我们的互斥规则：只有持有锁的客户端在锁有效时间内（如步骤 3 中获得）内终止其工作，减去一些时间（仅几毫秒为了补偿进程之间的时钟漂移）。本文包含有关需要绑定时钟漂移的类似系统的更多信息：租赁：分布式文件缓存一致性的有效容错机制。5.3、失败重试当客户端无法获取锁时，它应该在随机延迟后重试，以尝试使多个客户端同时尝试获取同一资源的锁（这可能导致没有人的脑裂情况）胜）。此外，客户端在大多数 Redis 实例中尝试获取锁的速度越快，裂脑条件的窗口就越小（并且需要重试），因此理想情况下，客户端应该尝试将SET命令发送到 N 个实例同时使用多路复用。值得强调的是，对于未能获得大部分锁的客户端来说，尽快释放（部分）获得的锁是多么重要，这样就无需等待密钥到期才能再次获得锁（但是，如果发生网络分区并且客户端不再能够与 Redis 实例通信，则会在等待密钥到期时造成可用性损失）。5.4、释放锁释放锁很简单，无论客户端是否相信它能够成功锁定给定实例，都可以执行。就是同时给所有的 Redis 实例发消息说要释放这把锁。我此处只是一个简单的思路，如果对Redlock展开说，这篇文章的字数可能还需要翻上一倍，而且我感觉如果是没有实践的去分析，文字会稍显稚嫩，同时也会因为无案例支撑，让其真实性也会大打折扣。想仔细了解的，大家可以多找找，网上也有很多针对两位大佬的辩论分析的文章。5.5、补充其实，如果你的应用只需要高性能的分布式锁并且可以接受一定程度上的数据不一致性【像之前说的刚设置完锁，Redis中的主机就宕机，导致没有成功同步到从机，所产生的数据不一致性】，那么实际上之前所讨论的 Redis 分布式锁也是足够了的。但是如果业务要求一定要保证应用中数据的强一致性，那么我觉得你可以试着找找其他的方式，换成zookeeper 加上一定的补偿机制去试一试。毕竟Redlock(红锁)一方面太重了，不是特别大的项目，我个人觉得也不会用至少五台起步的Redis实例吧，另外一方面，看了两位大佬的讨论，特别极端的情况下，也是有可能出现问题的。刚刚说到的Zookeeper 实现分布式锁,虽然它也有问题，但总归它是保证CAP机制中的CP的，可以保证任何时刻对Zookeeper的访问请求能得到一致性的数据，但不绝对保证服务一定可用~ 属于是用性能换安全啦~总之，如果项目中一定要有非常强的数据一致性，在那么对于分布式锁，你也保留一丝怀疑，毕竟它也不是真的100%安全的。既然看到这里啦，我觉得再看一遍大纲，判断一下自己理解了多少是非常重要的：关于代码不知道阅读的小伙伴，有木有发现代码中有一点点小问题~后期在检查时，案例中的代码是有点不太合适的，应当将所有案例中的递归调用方法改为循环重试，并限制重试次数，而非一直递归调用。 原因：如果一直没有抢到锁，重复的递归调用是有很大可能会导致程序崩溃的,这是不合适的。另外如果是阅读过一些框架源码的话，它们的底层调用大都是写个while(true)来达到某个方法的重复调用，而并非是递归调用。 此处是我的疏忽，各位见谅见谅。另外重试机制下可能会出现的问题：幂等性问题： （查询操作具有天然幂等性）在分布式架构下，服务之间调用会因为网络原因出现超时失败情况，而重试机制会重复多次调用服务，但是对于被调用放，就可能收到了多次调用。如果被调用方不具有天生的幂等性，那就需要增加服务调用的判重模块，并对每次调用都添加一个唯一的id。大量请求超时堆积：超高并发下，大量的请求如果都进行超时重试的话，如果你的重试时间设置不安全的话，会导致大量的请求占用服务器线程进行重试，这时候服务器线程负载就会暴增，导致服务器宕机。对于这种超高并发下的重试设计，我们不能让重试放在业务线程，而是统一由异步任务来执行。" }, { "title": "高效Debug，排查问题效率大大提升", "url": "/posts/Debug/", "categories": "技术科普", "tags": "学习", "date": "2023-01-28 09:00:00 +0000", "snippet": "高效Debug，排查问题效率大大提升Debug是开发人员必备的基础技能，伴随着开发生涯，只要需要写代码，就一定有debug的诉求… 因为大部分开发同学都是用Debug来确认程序是不是预期进行(单元测试也可以)。Debug一个非常常见的我们以为自己已经熟练掌握的技能，有点像说话一样，每个人都可以把话说出来，但不是每个人都能表达出影响力…大家都会Debug，不过有些Debug的方式可能确实会效率...", "content": "高效Debug，排查问题效率大大提升Debug是开发人员必备的基础技能，伴随着开发生涯，只要需要写代码，就一定有debug的诉求… 因为大部分开发同学都是用Debug来确认程序是不是预期进行(单元测试也可以)。Debug一个非常常见的我们以为自己已经熟练掌握的技能，有点像说话一样，每个人都可以把话说出来，但不是每个人都能表达出影响力…大家都会Debug，不过有些Debug的方式可能确实会效率更高一些，还是直接进入主题吧；一、基本的Debug相信大家都知道如何开始Debug：1、在Idea的某个程序文件的目标行旁边，点击一下，设置个小红点2、使用Debug按钮运行程序，如果程序可以走到断点这里，就开始进入Debug模式3、基本操作就是： step in 进入方法内部 step over 直接执行到下一行 step out 跳出当前的方法重复1,2,3步骤，刚开始调试的时候主要就是这几个步骤；二、断点相关经验1、只有满足某些条件才会进入断点如果说Debug的位置是网关入口，那么流量会很大，各种类型的请求都会走到这个断点里面，如果不能按照条件进入断点，会非常影响我们的效率。因为进入断点的请求，都不是我们想要的；这个时候可以对断点设置条件，当前请求中必须有满足什么条件才会进入Debug模式。1、点击程序的目标行旁边，生成一个小红点； 2、右键小红点，可以在condition那里设置程序中的条件；举个例子，如下当用Debug运行的时候，是不会走到断点的。而且在设置完成断点条件后，断点旁边会多出一个？和普通的断点不同。2、Debug断点只生效一次，同时不阻塞系统如果说Debug某个正在运行的系统，默认情况下会挂起所有的后续请求，很多人都以为系统死机了… 其实最后发现是你在调试。有什么办法，可以在调试的时候不阻塞剩余的请求吗？ 1、默认只断点一次； 2、断点的时候不挂起整个系统；如下，通过断点管理器器，进入进来，或者右键断点，然后点击more可以进入进来 设置挂起选项，只挂起当前正在调试的线程，然后再下面勾选一旦命中移除断点。3、静态断点，只是想看程序会不会运行到这里来(类似于动态日志)想确定请求能不能走到某个位置，但是又不想进入debug模式，感觉太重了，能不能如果经过这一行就直接打个日志呢？这对于有时候程序的一些方法没有打日志，但是又想确认是不是能执行到这里有帮助。在断点配置里面勾选，命中后打日志，也可以自己加一些其他的输出： 不要挂起程序 命中处打日志4、分组管理断点(系统不同链路的断点)系统常用的链路主要就几条，而常调试问题的地方也只有几处，可以把这几处位置的断点管理起来，在遇到问题的时候直接把断点分组管理拿出来就可以了。5、远程服务器Debug这里主要是一个配置问题，和本地Debug区别不大，学会配置就好了。 启动程序的时候： java -Xdebug -Xrunjdwp:transport=dt_socket,server=y,suspend=n,address=0.0.0.0:5005 -jar 待发布的程序jar包名称.jar在Idea里面：三、调试相关经验1、快速执行到某个位置有时候我们的断点没有设置在某个位置，但是也不想设置在哪个位置； 在Debug的时候想让程序直接运行到那个位置，怎么处理？第一种方式： 鼠标移动到对应的行数，然后按下run to cursor按钮第二种方式：直接点击文件旁边的数字即可，运行到对应的行哪里2、回退重新开始执行在一些复杂的链路中，方法调用很长，手一抖结果代码又运行了几行，这个时候想去重新开始执行这个断点怎么办？常规操作是再模拟发出个请求，重新进入断点； 但其实Idea已经提供了对应的方式，直接撤回当前的帧即可，断点会重新进入方法开始执行。在Frame的位置点击撤回按钮，就会重新进入这个方法开始运行3、中断后续执行链路如果说debug到一半发现可能会往数据库写入脏数据，想直接停止当前的调试，怎么做？同样在帧的位置，右键，可以提前返回不继续运行，这个提前返回是针对当前的方法的，也可以直接抛出异常；4、调试Strem流Java8之后的labmda表达式里面一般流程会多一点，也不是很好调试，Idea也有对应的工具，可以直接查看Strem流中的数据，在Debug Window下发，如果识别到labmda表达式后会展示出来。5、断点的时候运行一些额外代码在Debug模式下，Idea提供了一个类似于解释器的工具，可以输入一些额外的程序在运行，哪怕和本次debug无关；当然有个店是这个表达式执行只会返回最后y一行语句的结果。四、总结debug代码是一个常用而且很常见的技能，但是不是每个人都能很有效率的debug代码…有一些idea隐藏的debug方式，虽然一些人不关注，但是有用并且能极大提升效率文章主要介绍一些一些在实际项目中相对有用的可以提升debug能力的一些经验。最后如果说实在是有问题，但是又没办法进入调试模式，可以考虑arthas的trace和watch。" }, { "title": "ArrayList 可以完全替代数组吗？", "url": "/posts/ArrayList/", "categories": "数据结构", "tags": "学习", "date": "2023-01-21 07:50:00 +0000", "snippet": "ArrayList 可以完全替代数组吗？一、ArrayList 和 LinkedList 的区别 1、数据结构： 在数据结构上，ArrayList 和 LinkedList 都是 “线性表”，都继承于 Java 的 List 接口。另外 LinkedList 还实现了 Java 的 Deque 接口，是基于链表的栈或队列，与之对应的是 ArrayDeque 基于数组的栈或队列； 2、线程...", "content": "ArrayList 可以完全替代数组吗？一、ArrayList 和 LinkedList 的区别 1、数据结构： 在数据结构上，ArrayList 和 LinkedList 都是 “线性表”，都继承于 Java 的 List 接口。另外 LinkedList 还实现了 Java 的 Deque 接口，是基于链表的栈或队列，与之对应的是 ArrayDeque 基于数组的栈或队列； 2、线程安全： ArrayList 和 LinkedList 都不考虑线程同步，不保证线程安全； 3、底层实现： 在底层实现上，ArrayList 是基于动态数组的，而 LinkedList 是基于双向链表的。事实上，它们很多特性的区别都是因为底层实现不同引起的。比如说： 在遍历速度上： 数组是一块连续内存空间，基于局部性原理能够更好地命中 CPU 缓存行，而链表是离散的内存空间对缓存行不友好； 在访问速度上： 数组是一块连续内存空间，支持 O(1) 时间复杂度随机访问，而链表需要 O(n) 时间复杂度查找元素； 在添加和删除操作上： 如果是在数组的末尾操作只需要 O(1) 时间复杂度，但在数组中间操作需要搬运元素，所以需要 O(n)时间复杂度，而链表的删除操作本身只是修改引用指向，只需要 O(1) 时间复杂度（如果考虑查询被删除节点的时间，复杂度分析上依然是 O(n)，在工程分析上还是比数组快）； 额外内存消耗上： ArrayList 在数组的尾部增加了闲置位置，而 LinkedList 在节点上增加了前驱和后继指针。 二、ArrayList 源码分析1、ArrayList 的属性ArrayList 的属性很好理解，底层是一个 Object 数组，我要举手提问： 🙋🏻‍♀️疑问 1： 为什么 elementData 字段不声明 private 关键字？ 🙋🏻‍♀️疑问 2： 为什么 elementData 字段声明 transient 关键字？ 🙋🏻‍♀️疑问 3： 为什么elementData 字段不声明为泛型类型 E？ 🙋🏻‍♀️疑问 4： 为什么 ArrayList 的最大容量是 Integer.MAX_VALUE，Long.MAX_VALUE 不行吗？ 🙋🏻‍♀️疑问 5： 为什么 ArrayList 的最大容量是 MAX_VALUE - 8，一定会减 8 吗？这些问题我们在分析源码的过程中回答。疑问这么多，ArrayList 瞬间不香了。public class ArrayList&lt;E&gt; extends AbstractList&lt;E&gt; implements List&lt;E&gt;, RandomAccess, Cloneable, java.io.Serializable { // new ArrayList() 默认初始容量 private static final int DEFAULT_CAPACITY = 10; // new ArrayList(0) 的全局空数组 private static final Object[] EMPTY_ELEMENTDATA = {}; // new ArrayList() 的全局空数组 private static final Object[] DEFAULTCAPACITY_EMPTY_ELEMENTDATA = {}; // 修改次数记录 protected transient int modCount = 0; // 数组的最大长度 // 疑问 4：为什么 ArrayList 的最大容量是 Integer.MAX_VALUE，Long.MAX_VALUE 不行吗？ // 疑问 5：为什么 ArrayList 的最大容量是 MAX_VALUE - 8，一定会减 8 吗？ private static final int MAX_ARRAY_SIZE = Integer.MAX_VALUE - 8 // 疑问 1：为什么不声明 private（后文回答） // 疑问 2：为什么声明 transient（后文回答） // 疑问 3：为什么不声明为泛型类型 E // 底层数组 transient Object[] elementData; // 数组的有效长度（不是 elementData.length） private int size; // size() 返回的是数组的有效长度（合理，底层数组我们不关心） public int size() { return size; }}2、ArrayList 的构造方法ArrayList 有三个构造函数： 1、带初始容量的构造方法： 如果初始容量大于 0，则创建指定长度的数组。如果初始容量是 0，则指向第 1 个全局空数组 ；EMPTY_ELEMENTDATA； 2、无参构造方法： 指向第 2 个全局空数组 DEFAULTCAPACITY_EMPTY_ELEMENTDATA； 3、带集合的构造方法： 将集合转为数组，如果数组为空，则指向第 1 个全局空数组 EMPTY_ELEMENTDATA；可以看到，除了指定大于 0 的初始容量外，ArrayList 在构造时不会创建数组，而是指向全局空数组，这是懒初始化的策略。构造器的源码不难，但小朋友总有太多的问号，举手提问 🙋🏻‍♀️： 🙋🏻‍♀️疑问 6：既然都是容量为 0 ，为什么 ArrayList 要区分出 2 个空数组？这个问题直接回答吧：ArrayList 认为无参构造函数应该使用默认行为，在首次添加数据时会创建长度为 10（DEFAULT_CAPACITY） 的默认初始数组；而显示设置初始容量为 0 是开发者的显式意图，所以不使用默认初始数组，在首次添加数据时只会创建长度为 1 （size + 1）的数组（可以结合后文源码理解下）。 🙋🏻‍♀️疑问 7： 在带集合的构造方法中，为什么会存在集合转化后的数组类型不是 Object[].class 的情况？// 带初始容量的构造方法public ArrayList(int initialCapacity) { if (initialCapacity &gt; 0) { // 创建 initialCapacity 长度的数组 this.elementData = new Object[initialCapacity]; } else if (initialCapacity == 0) { // 指向第 1 个全局空数组 this.elementData = EMPTY_ELEMENTDATA; } else { // 不合法的初始容量 throw new IllegalArgumentException(\"Illegal Capacity: \"+initialCapacity); }}// 无参构造方法public ArrayList() { // 指向第 1 个全局空数组 this.elementData = DEFAULTCAPACITY_EMPTY_ELEMENTDATA;}// 带集合的构造方法public ArrayList(Collection&lt;? extends E&gt; c) { // 将集合转为数组 elementData = c.toArray(); if ((size = elementData.length) != 0) { // 疑问 7：这一个条件语句好奇怪，toArray() 的返回值类型就是 Object[] 啊？ if (elementData.getClass() != Object[].class) elementData = Arrays.copyOf(elementData, size, Object[].class); } else { this.elementData = EMPTY_ELEMENTDATA; }}public Object[] toArray() { return Arrays.copyOf(elementData, size);}3、 ArrayList 的添加与扩容方法ArrayList 可以在数组末尾或数组中间添加元素： 如果是在数组末尾添加，均摊时间只需要 O(1) 时间复杂度； 如果在数组中间添加，由于需要搬运数据，所以需要 O(n) 时间复杂度。添加前会先检查数据容量，不足会先扩容： 在使用无参构造器初始化时，首次添加元素时会直接扩容到 10 的容量； 在其他情况，会直接扩容到旧容量的 1.5 倍，而不是扩容到最小容量要求。不管是扩容到 10 还是扩容到 1.5 倍，都是为了防止频繁扩容，避免每次 add 添加数据都要扩容一次。 🙋🏻‍♀️疑问 4：为什么 ArrayList 的最大容量是 Integer.MAX_VALUE，Long.MAX_VALUE 不行吗？数组对象的长度是记录在对象头中的 “数组长度” 字段中，这个字段是 4 字节，正好就是 Integer 也是 4 个字节，所以限制为 Integer.MAX_VALUE，而不能使用 Long.MAX_VALUE。不对啊，Java Integer 是有符号整数，所以 Integer.MAX_VALUE 只有 31 位有效位，还少了 1 位呢。没错，是少了一位。如果要榨干这 1 位容量，当然可以用 long 类型并且限制到 32 位能够表示的最大正整数上，并且在源码中到处加上数组越界判断，想想就不稳定的。相比之下，限制数组长度为 int 类型且最大长度为 Integer.MAX_VALUE，如果有超过 Integer.MAX_VALUE 存储容量的需求，那就创建两个数组呀：）你觉得哪种更好。Java 对象内存布局: 🙋🏻‍♀️疑问 5：为什么 ArrayList 的最大容量是 MAX_VALUE - 8，一定会减 8 吗？依然与对象的内存布局有关。在 Java 虚拟机垃圾回收算法中，需要计算对象的内存大小，计算结果是存储在 jint 类型变量（Java int 类型在 JNI 中的映射）中的。如果数组的长度是 MAX_VALUE，那么加上对象头之后就整型溢出了，所以 ArrayList 会预先减掉对象头可能占用的 8 个字节。对象头的具体大小取决于虚拟机实现，减 8 是相对保守的。其实，ArrayList 的最大容量也不一定会减 8，如果最小容量要求是超过 MAX_ARRAY_SIZE 的，那么还是会扩容到 MAX_VALUE 。这有点摆烂的意思，会不会溢出运行时再说。数组长度溢出:OutOfMemoryError: Requested array size exceeds VM limit 🙋🏻‍♀️疑问 8：不应该是 elementData.length - minCapacity &gt; 0 吗？ 这是考虑到整型溢出的情况，minCapacity 溢出就变成负数了// 在数组末尾添加元素public boolean add(E e) { // 先确保底层数组容量足够容纳 size + 1，不足会扩容 ensureCapacityInternal(size + 1); // Increments modCount!! // 在 size + 1 的位置赋值 elementData[size++] = e; return true;}// 在数组中间插入元素public void add(int index, E element) { // 范围检查 rangeCheckForAdd(index); // 先确保容量足够容纳 size + 1，不足会扩容 ensureCapacityInternal(size + 1); // Increments modCount!! // 先搬运数据腾出空位 System.arraycopy(elementData, index, elementData, index + 1, size - index); // 在 index 的位置赋值 elementData[index] = element; // 长度加一 size++;}// 在数组末尾添加集合public boolean addAll(Collection&lt;? extends E&gt; c) { // 集合转数组 Object[] a = c.toArray(); // 先确保底层数组容量足够容纳 size + numNew，不足会扩容 int numNew = a.length; ensureCapacityInternal(size + numNew); // Increments modCount // 搬运原数据 System.arraycopy(a, 0, elementData, size, numNew); // 长度加 numNew size += numNew; return numNew != 0;}// 在数组中间插入集合public boolean addAll(int index, Collection&lt;? extends E&gt; c) { // 略，原理类似}// 尝试扩容// （提示：源码调用了 calculateCapacity() 函数，这里做内联简化）private void ensureCapacityInternal(int minCapacity) { // 使用无参构造器初始化时，指定扩容为 10 if (elementData == DEFAULTCAPACITY_EMPTY_ELEMENTDATA) { minCapacity = Math.max(DEFAULT_CAPACITY, minCapacity); } ensureExplicitCapacity(minCapacity);}private void ensureExplicitCapacity(int minCapacity) { modCount++; // 疑问 8：不应该是 elementData.length - minCapacity &gt; 0 吗？ // 如果底层数组长度不够 minCapacity 最小容量要求，则需要扩容 if (minCapacity - elementData.length &gt; 0) grow(minCapacity);}private void grow(int minCapacity) { // 旧容量 int oldCapacity = elementData.length; // 新容量 = 旧容量 * 1.5 倍（有可能整型溢出） int newCapacity = oldCapacity + (oldCapacity &gt;&gt; 1); // 如果新容量小于最小容量要求，则使用最小容量（addAll 大集合的情况） if (newCapacity - minCapacity &lt; 0) { newCapacity = minCapacity; } // （提示：上一个 if 的 newCapacity 有可能是溢出的） // 如果新容量超出最大数组长度限制，说明无法扩容 1.5 倍，回归到 minCapacity 上 // （提示：源码调用了 hugeCapacity() 函数，这里做内联简化） if (newCapacity - MAX_ARRAY_SIZE &gt; 0) { // 最小容量要求发生整型溢出，无法满足要求，只能直接抛出 OOM if (minCapacity &lt; 0) throw new OutOfMemoryError(); // 如果最小容量要求超出最大数组长度限制，则扩容到 MAX_VALUE（说明不一定会减 8） // 否则，扩容到最大数组长度限制（MAX_VALUE - 8） newCapacity = (minCapacity &gt; MAX_ARRAY_SIZE) ? Integer.MAX_VALUE : MAX_ARRAY_SIZE; } // 扩容到 newCapacity 长度 elementData = Arrays.copyOf(elementData, newCapacity);}private static int hugeCapacity(int minCapacity) { // 已经内联简化到 grow 方法中}除了扩容之外，ArrayList 还支持缩容，将底层数组的容量缩小到实际元素的数量：// 缩容public void trimToSize() { modCount++; if (size &lt; elementData.length) { elementData = (size == 0) ? EMPTY_ELEMENTDATA : Arrays.copyOf(elementData, size); }}另外，因为扩容涉及到数据搬运操作，所以如果能事先知道数据的容量，最好在创建 ArrayList 时就显式指定数据量大小。4、 ArrayList 的迭代器Java 的 foreach 是语法糖，本质上也是采用 iterator 的方式。ArrayList 提供了 2 个迭代器： iterator():Iterator()： 单向迭代器 ListIterator listIterator()： 双向迭代器在迭代器遍历数组的过程中，有可能出现多个线程并发修改数组的情况，造成数据不一致甚至数组越界，所以 Java 很多容器类的迭代器中都有 fail-fast 机制。如果在迭代的过程中发现 expectedModCount 变化，说明数据被修改，此时就会提前抛出 ConcurrentModificationException 异常（当然也不一定是被其他线程修改）。private class Itr implements Iterator&lt;E&gt; { int cursor; // index of next element to return int lastRet = -1; // index of last element returned; -1 if no such // 创建迭代器是会记录外部类的 modCount int expectedModCount = modCount; ... Itr() {} public boolean hasNext() { return cursor != size; } @SuppressWarnings(\"unchecked\") public E next() { // 检查 checkForComodification(); ... } public void remove() { ... // 更新 expectedModCount = modCount; }} 🙋🏻‍♀️疑问 1：为什么 elementData 字段不声明 private 关键字？在注释中的解释是：“non-private to simplify nested class access”。但我们知道在 Java 中，内部类是可以访问外部类的 private 变量的，所以这就说不通的。我的理解是：因为内部类在编译后会生成独立的 Class 文件，如果外部类的 elementData 字段是 private 类型，那么编译器就需要在 ArrayList 中插入 getter / setter，并通过方法调用，而 non-private 字段就可以直接访问字段。5、ArrayList 的序列化过程 🙋🏻‍♀️疑问 2：为什么 elementData 字段声明 transient 关键字？ArrayList 重写了 JDK 序列化的逻辑，只把 elementData 数组中有效元素的部分序列化，而不会序列化整个数组。// 序列化和反序列化只考虑有效元素private void writeObject(java.io.ObjectOutputStream s) throws java.io.IOException{ // Write out element count, and any hidden stuff int expectedModCount = modCount; s.defaultWriteObject(); // 写入数组长度 s.writeInt(size); // 写入有效元素 for (int i=0; i&lt;size; i++) { s.writeObject(elementData[i]); } if (modCount != expectedModCount) { throw new ConcurrentModificationException(); }}6、 ArrayList 的 clone() 过程ArrayList 中的 elementData 数组是引用类型，因此在 clone() 中需要实现深拷贝，否则原对象与克隆对象会相互影响：public Object clone() { try { ArrayList&lt;?&gt; v = (ArrayList&lt;?&gt;) super.clone(); // 拷贝数组对象 v.elementData = Arrays.copyOf(elementData, size); // 修改计数归零 v.modCount = 0; return v; } catch (CloneNotSupportedException e) { // this shouldn't happen, since we are Cloneable throw new InternalError(e); }}7、为什么阿里巴巴要求谨慎使用 subList API？在 《阿里巴巴 Java 开发手册》中，有关于 ArrayList#subList API 的规定。为什么阿里巴巴要做这样的限制呢？ 【强制】ArrayList 的 subList 结果不可强转成 ArrayList，否则会抛出 ClassCastException 异常； 【强制】在 subList 场景中，高度注意对原集合元素的增加或删除，均会导致子列表的遍历、增加、删除产生 ConcurrentModificationException 异常。这是因为 subList API 只是提供通过起始索引 fromIndex 和终止索引 toIndex 包装了一个原 ArrayList 的 “视图窗口” ，并不是真的截取并创建了一个新的 ArrayList，所以强制类型转换就会抛出 ClassCastException 异常。此时，在 ArrayList 或 SubList 上做修改，要注意相互之间的影响： 在 ArrayList 或 SubList 上修改元素，都会同步更新到对方（因为底层都是 ArrayList 本身）； 在 SubList 上增加或删除元素，会影响到 ArrayList； 在 ArrayList 上增加或删除元素，会导致 SubList 抛出 ConcurrentModificationException 异常。ArrayList.javapublic List&lt;E&gt; subList(int fromIndex, int toIndex) { subListRangeCheck(fromIndex, toIndex, size); return new SubList(this, 0, fromIndex, toIndex);}private class SubList extends AbstractList&lt;E&gt; implements RandomAccess { // 原 ArrayList private final AbstractList&lt;E&gt; parent; private final int parentOffset; private final int offset; int size; SubList(AbstractList&lt;E&gt; parent, int offset, int fromIndex, int toIndex) { this.parent = parent; this.parentOffset = fromIndex; this.offset = offset + fromIndex; this.size = toIndex - fromIndex; // modCount 记录 this.modCount = ArrayList.this.modCount; } public E set(int index, E e) { rangeCheck(index); // 在 ArrayList 上增加或删除元素，会导致 SubList 抛出 ConcurrentModificationException 异常 checkForComodification(); // 在 SubList 上增加或删除元素，会影响到 ArrayList； E oldValue = ArrayList.this.elementData(offset + index); ArrayList.this.elementData[offset + index] = e; return oldValue; }}8、ArrayList 如何实现线程安全？有 4 种方式： 方法 1 - 使用 Vector 容器： Vector 是线程安全版本的数组容器，它会在所有方法上增加 synchronized 关键字； 方法 2 - 使用 Collections.synchronizedList 包装类： 原理也是在所有方法上增加 synchronized 关键字； 方法 3 - 使用 CopyOnWriteArrayList 容器： 基于加锁的 “读写分离” 和 “写时复制” 实现的动态数组，适合于读多写少，数据量不大的场景。 方法 4 - 使用 ArrayBlockingQueue 容器： 基于加锁的阻塞队列，适合于带阻塞操作的生产者消费者模型。三、Arrays#ArrayList事实上，在 Java 环境中有两个 ArrayList，这或许是一个隐藏的彩蛋（坑）： ArrayList： 一般认为的 ArrayList，是一个顶级类； Arrays#ArrayList： Arrays 的静态内部类，和上面这个 ArrayList 没有任何关系。其实，Arrays#ArrayList 的定位就是在数组和和 List 直接切换而已。Arrays 提供了数组转 List 的 API，而 Arrays#ArrayList 也提供了 List 转数组的 API（这些 API 第一个 ArrayList 中也都有…）回过头看剩下的 2 个问题： 🙋🏻‍♀️疑问 3：为什么 elementData 字段不声明为泛型类型 E？泛型擦除后等于 Object[] elementData，没有区别。 🙋🏻‍♀️疑问 7：在带集合的构造方法中，为什么会存在集合转化后的数组类型不是 Object[].class 的情况？这是因为有些 List 集合的底层数组不是 Object[] 类型，有可能是 String[] 类型。而在 ArrayList#toArray() 方法中，返回值的类型是 Object[] 类型，有类型错误风险。例如：在这段代码中，ArrayList 接收一个由 String 数组转化的 List，最后在 ArrayList#toArray() 返回的 Object 数组中添加一个 Object 对象，就出现异常了：// 假设没有特殊处理List&lt;String&gt; list = new ArrayList&lt;&gt;(Arrays.asList(\"list\"));// class java.util.ArrayListSystem.out.println(list.getClass());Object[] listArray = list.toArray();// 如果过没有特殊处理，实际类型是 [Ljava.langSystem.out.println(listArray.getClass());// 如果过没有特殊处理，将抛出 ArrayStoreException 异常listArray[0] = new Object();源码摘要:Arrays.javapublic static &lt;T&gt; List&lt;T&gt; asList(T... a) { return new ArrayList&lt;&gt;(a);}private static class ArrayList&lt;E&gt; extends AbstractList&lt;E&gt; implements RandomAccess, java.io.Serializable { // 泛型擦除后：Object[] a; private final E[] a; // 泛型擦除后：Object[] array; // Java 数组是协变的，能够接收 String[] 类型的数组 ArrayList(E[] array) { // 赋值 a = Objects.requireNonNull(array); } // 实际返回的数组可能是 Object[] 类型，也可能是 String[] 类型 @Override public Object[] toArray() { return a.clone(); }}ArrayList.javapublic class ArrayList&lt;E&gt; extends AbstractList&lt;E&gt; implements List&lt;E&gt;, RandomAccess, Cloneable, java.io.Serializable transient Object[] elementData; // 带集合的构造方法 public ArrayList(Collection&lt;? extends E&gt; c) { // 将集合转为数组 elementData = c.toArray(); if ((size = elementData.length) != 0) { // 疑问 7：这一个条件语句好奇怪，toArray() 的返回值类型就是 Object[] 啊？ if (elementData.getClass() != Object[].class) elementData = Arrays.copyOf(elementData, size, Object[].class); } else { this.elementData = EMPTY_ELEMENTDATA; } } public Object[] toArray() { return Arrays.copyOf(elementData, size); }}四、ArrayList 这么好用，可以完全替代数组吗大多数场景可以，但不能完全替代。ArrayList 是基于 Object 数组封装的动态数组，我们不需要关心底层数组的数据搬运和扩容等逻辑，因此在大多数业务开发场景中，除非是为了最求极致的性能，否则直接使用 ArrayList 代替数组是更好的选择。那么，ArrayList 有哪些地方上比数组差呢？ 举例 1 - ArrayList 等容器类不支持 int 等基本数据类型，所以必然存在装箱拆箱操作； 举例 2 - ArrayList 默认的扩容逻辑是会扩大到原容量的 1.5 倍，在大数据量的场景中，这样的扩容逻辑是否多余，需要打上问题； 举例 3 - ArrayList 的灵活性不够。ArrayList 不允许底层数据有空洞，所有的有效数据都会 “压缩” 到底层数组的首部。因此，当需要基于数组二次开发容器时，ArrayList 并不是一个好选择。 例如，使用 ArrayList 开发栈的结构或许合适，可以在数组的尾部操作数据。但使用 ArrayList 开发队列就不合适，因为在数组的首部入队或出队需要搬运数据； 而数组没有这些约束，我们可以将数组设计为 “环形数组”，就可以避免入队和出队时搬运数据。例如 Java 的 ArrayBlockingQueue 和 ArrayDeque 就是基于数组的队列。 五、总结1、ArrayList 是基于数组封装的动态数组，封装了操作数组时的搬运和扩容等逻辑；2、在构造 ArrayList 时，除了指定大于 0 的初始容量外，ArrayList 在构造时不会创建数组，而是指向全局空数组，这是懒初始化的策略；3、在添加数据时会先检查数据容量，不足会先扩容。首次添加默认会扩容到 10 容量，后续会扩容到旧容量的 1.5 倍，这是为了避免反复扩容；4、因为扩容涉及到数据搬运操作，所以如果能事先知道数据的容量，最好在创建 ArrayList 时就显式指定数据量大小；5、ArrayList 重写了序列化过程，只处理数组中有效的元素；6、ArrayList 的 subList API 只是提供视图窗口，并不是创建新列表；7、ArrayList 在大多数场景中可以代替数组，但在高性能和二次封装的场景中，ArrayList 无法替代数组。" }, { "title": "ArrayList 和 LinkedList 的区别", "url": "/posts/LinkedList/", "categories": "数据结构", "tags": "学习", "date": "2023-01-14 07:34:00 +0000", "snippet": "ArrayList 和 LinkedList 的区别一、LinkedList的特点1、两者的区别1、数据结构： 在数据结构上，ArrayList 和 LinkedList 都是 “线性表”，都继承于 Java 的 List 接口。另外 LinkedList 还实现了 Java 的 Deque 接口，是基于链表的栈或队列，与之对应的是 ArrayDeque 基于数组的栈或队列；2、线程安全： ...", "content": "ArrayList 和 LinkedList 的区别一、LinkedList的特点1、两者的区别1、数据结构： 在数据结构上，ArrayList 和 LinkedList 都是 “线性表”，都继承于 Java 的 List 接口。另外 LinkedList 还实现了 Java 的 Deque 接口，是基于链表的栈或队列，与之对应的是 ArrayDeque 基于数组的栈或队列；2、线程安全： ArrayList 和 LinkedList 都不考虑线程同步，不保证线程安全；3、底层实现： 在底层实现上，ArrayList 是基于动态数组的，而 LinkedList 是基于双向链表的。事实上，它们很多特性的区别都是因为底层实现不同引起的。比如说： 在遍历速度上： 数组是一块连续内存空间，基于局部性原理能够更好地命中 CPU 缓存行，而链表是离散的内存空间对缓存行不友好； 在访问速度上： 数组是一块连续内存空间，支持 O(1) 时间复杂度随机访问，而链表需要 O(n) 时间复杂度查找元素； 在添加和删除操作上： 如果是在数组的末尾操作只需要 O(1) 时间复杂度，但在数组中间操作需要搬运元素，所以需要 O(n)时间复杂度，而链表的删除操作本身只是修改引用指向，只需要 O(1) 时间复杂度（如果考虑查询被删除节点的时间，复杂度分析上依然是 O(n)，在工程分析上还是比数组快）； 额外内存消耗上： ArrayList 在数组的尾部增加了闲置位置，而 LinkedList 在节点上增加了前驱和后继指针。2、LinkedList的实现接口在数据结构上，LinkedList 不仅实现了与 ArrayList 相同的 List 接口，还实现了 Deque 接口（继承于 Queue 接口）。Deque 接口表示一个双端队列（Double Ended Queue），允许在队列的首尾两端操作，所以既能实现队列行为，也能实现栈行为。Queue 接口：Queue 的 API 可以分为 2 类，区别在于方法的拒绝策略上： 抛异常： 向空队列取数据，会抛出 NoSuchElementException 异常； 向容量满的队列加数据，会抛出 IllegalStateException 异常。 返回特殊值： 向空队列取数据，会返回 null； 向容量满的队列加数据，会返回 false。 Deque 接口：Java 没有提供标准的栈接口（很好奇为什么不提供），而是放在 Deque 接口中：除了标准的队列和栈行为，Deque 接口还提供了 12 个在两端操作的方法：二、源码分析1、LinkedList 的属性 LinkedList 底层是一个 Node 双向链表，Node 节点中会持有数据 E 以及 prev 与next 两个指针； LinkedList 用 first 和 last 指针指向链表的头尾指针。LinkedList 的属性很好理解的，不出意外的话又有小朋友出来举手提问了： 🙋🏻‍♀️ 疑问 1：为什么字段都不声明 private 关键字？这个问题直接回答吧。我的理解是：因为内部类在编译后会生成独立的 Class 文件，如果外部类的字段是 private 类型，那么编译器就需要通过方法调用，而 non-private 字段就可以直接访问字段。 🙋🏻‍♀️ 疑问 2：为什么字段都声明 transient 关键字？这个问题我们在分析源码的过程中回答。疑问比 ArrayList 少很多，LinkedList 真香（还是别高兴得太早吧）。public class LinkedList&lt;E&gt; extends AbstractSequentialList&lt;E&gt; implements List&lt;E&gt;, Deque&lt;E&gt;, Cloneable, java.io.Serializable { // 疑问 1：为什么字段都不声明 private 关键字？ // 疑问 2：为什么字段都声明 transient 关键字？ // 元素个数 transient int size = 0; // 头指针 transient Node&lt;E&gt; first; // 尾指针 transient Node&lt;E&gt; last; // 链表节点 private static class Node&lt;E&gt; { // 节点数据 // （类型擦除后：Object item;） E item; // 前驱指针 Node&lt;E&gt; next; // 后继指针 Node&lt;E&gt; prev; Node(Node&lt;E&gt; prev, E element, Node&lt;E&gt; next) { this.item = element; this.next = next; this.prev = prev; } }}2、LinkedList 的构造方法LinkedList 有 2 个构造方法： 1、无参构造方法： no-op； 2、带集合的构造： 在链表末尾添加整个集合，内部调用了 addAll 方法将整个集合添加到数组的末尾。// 无参构造方法public LinkedList() {}// 带集合的构造方法public LinkedList(Collection&lt;? extends E&gt; c) { this(); addAll(c);}// 在链表尾部添加集合public boolean addAll(Collection&lt;? extends E&gt; c) { // 索引为 size，等于在链表尾部添加 return addAll(size, c);}3、LinkedList 的添加方法LinkedList 提供了非常多的 addXXX 方法，内部都是调用一系列 linkFirst、linkLast 或 linkBefore 完成的。如果在链表中间添加节点时，会用到 node(index) 方法查询指定位置的节点。其实，我们会发现所有添加的逻辑都可以用 6 个步骤概括： 步骤 1： 找到插入位置的后继节点（在头部插入就是 first，在尾部插入就是 null）； 步骤 2： 构造新节点； 步骤 3： 将新节点的 prev 指针指向前驱节点（在头部插入就是 null，在尾部插入就是 last）； 步骤 4： 将新节点的 next 指针指向后继节点（在头部插入就是 first，在尾部插入就是 null）； 步骤 5： 将前驱节点的 next 指针指向新节点（在头部插入没有这个步骤）； 步骤 6： 将后继节点的 prev 指针指向新节点（在尾部插入没有这个步骤）。分析一下添加方法的时间复杂度，区分在链表两端或中间添加元素的情况共： 如果是在链表首尾两端添加： 只需要 O(1) 时间复杂度； 如果在链表中间添加： 由于需要定位到添加位置的前驱和后继节点，所以需要 O(n) 时间复杂度。如果事先已经获得了添加位置的节点，就只需要 O(1) 时间复杂度。添加方法public void addFirst(E e) { linkFirst(e);}public void addLast(E e) { linkLast(e);}public boolean add(E e) { linkLast(e); return true;}public void add(int index, E element) { checkPositionIndex(index); if (index == size) // 在尾部添加 linkLast(element); else // 在指定位置添加 linkBefore(element, node(index));}public boolean addAll(Collection&lt;? extends E&gt; c) { return addAll(size, c);}// 在链表头部添加private void linkFirst(E e) { // 1. 找到插入位置的后继节点（first） final Node&lt;E&gt; f = first; // 2. 构造新节点 // 3. 将新节点的 prev 指针指向前驱节点（null） // 4. 将新节点的 next 指针指向后继节点（f） // 5. 将前驱节点的 next 指针指向新节点（前驱节点是 null，所以没有这个步骤） final Node&lt;E&gt; newNode = new Node&lt;&gt;(null, e, f); // 修改 first 指针 first = newNode; if (f == null) // f 为 null 说明首个添加的元素，需要修改 last 指针 last = newNode; else // 6. 将后继节点的 prev 指针指向新节点 f.prev = newNode; size++; modCount++;}// 在链表尾部添加void linkLast(E e) { final Node&lt;E&gt; l = last; // 1. 找到插入位置的后继节点（null） // 2. 构造新节点 // 3. 将新节点的 prev 指针指向前驱节点（l） // 4. 将新节点的 next 指针指向后继节点（null） final Node&lt;E&gt; newNode = new Node&lt;&gt;(l, e, null); // 修改 last 指针 last = newNode; if (l == null) // l 为 null 说明首个添加的元素，需要修改 first 指针 first = newNode; else // 5. 将前驱节点的 next 指针指向新节点 l.next = newNode; // 6. 将后继节点的 prev 指针指向新节点（后继节点是 null，所以没有这个步骤） size++; modCount++;}// 在指定节点前添加// 1. 找到插入位置的后继节点void linkBefore(E e, Node&lt;E&gt; succ) { final Node&lt;E&gt; pred = succ.prev; // 2. 构造新节点 // 3. 将新节点的 prev 指针指向前驱节点（pred） // 4. 将新节点的 next 指针指向后继节点（succ） final Node&lt;E&gt; newNode = new Node&lt;&gt;(pred, e, succ); succ.prev = newNode; if (pred == null) first = newNode; else // 5. 将前驱节点的 next 指针指向新节点 pred.next = newNode; size++; modCount++;}// 在指定位置添加整个集合元素// index 为 0：在链表头部添加// index 为 size：在链表尾部添加public boolean addAll(int index, Collection&lt;? extends E&gt; c) { checkPositionIndex(index); // 事实上，c.toArray() 的实际类型不一定是 Object[]，有可能是 String[] 等 // 不过，我们是通过 Node中的item 承接的，所以不用担心 ArrayList 中的 ArrayStoreException 问题 Object[] a = c.toArray(); // 添加的数组为空，跳过 int numNew = a.length; if (numNew == 0) return false; // 1. 找到插入位置的后继节点 // pred：插入位置的前驱节点 // succ：插入位置的后继节点 Node&lt;E&gt; pred, succ; if (index == size) { succ = null; pred = last; } else { // 找到 index 位置原本的节点，插入后变成后继节点 succ = node(index); pred = succ.prev; } // 插入集合元素 for (Object o : a) { E e = (E) o; // 2. 构造新节点 // 3. 将新节点的 prev 指针指向前驱节点 Node&lt;E&gt; newNode = new Node&lt;&gt;(pred, e, null); if (pred == null) // pred 为 null 说明是在头部插入，需要修改 first 指针 first = newNode; else // 5. 将前驱节点的 next 指针指向新节点 pred.next = newNode; // 修改前驱指针 pred = newNode; } if (succ == null) { // succ 为 null 说明是在尾部插入，需要修改 last 指针 last = pred; } else { // 4. 将新节点的 next 指针指向后继节点 pred.next = succ; // 6. 将后继节点的 prev 指针指向新节点 succ.prev = pred; } // 数量增加 numNew size += numNew; modCount++; return true;}// 将 LinkedList 转化为 Object 数组public Object[] toArray() { Object[] result = new Object[size]; int i = 0; for (Node&lt;E&gt; x = first; x != null; x = x.next) result[i++] = x.item; return result;}在链表中间添加节点时，会用到 node(index) 方法查询指定位置的节点。可以看到维持 first 和 last 头尾节点的作用又发挥出来了： 如果索引位置小于 size/2，则从头节点开始找； 如果索引位置大于 size/2，则从尾节点开始找。虽然，我们从复杂度分析的角度看，从哪个方向查询是没有区别的，时间复杂度都是 O(n)。但从工程分析的角度看还是有区别的，从更靠近目标节点的位置开始查询，实际执行的时间会更短。查询指定位置节点// 寻找指定位置的节点，时间复杂度：O(n)Node&lt;E&gt; node(int index) { if (index &lt; (size &gt;&gt; 1)) { // 如果索引位置小于 size/2，则从头节点开始找 Node&lt;E&gt; x = first; for (int i = 0; i &lt; index; i++) x = x.next; return x; } else { // 如果索引位置大于 size/2，则从尾节点开始找 Node&lt;E&gt; x = last; for (int i = size - 1; i &gt; index; i--) x = x.prev; return x; }}LinkedList 的删除方法其实就是添加方法的逆运算，我们就不重复分析了。// 删除头部元素public E removeFirst() { final Node&lt;E&gt; f = first; if (f == null) throw new NoSuchElementException(); return unlinkFirst(f);}// 删除尾部元素public E removeLast() { final Node&lt;E&gt; l = last; if (l == null) throw new NoSuchElementException(); return unlinkLast(l);}// 删除指定元素public E remove(int index) { checkElementIndex(index); return unlink(node(index));}4、LinkedList 的迭代器Java 的 foreach 是语法糖，本质上也是采用 iterator 的方式。由于 LinkedList 本身就是双向的，所以 LinkedList 只提供了 1 个迭代器： ListIterator listIterator()： 双向迭代器与其他容器类一样，LinkedList 的迭代器中都有 fail-fast 机制。如果在迭代的过程中发现 expectedModCount 变化，说明数据被修改，此时就会提前抛出 ConcurrentModificationException 异常（当然也不一定是被其他线程修改）。public ListIterator&lt;E&gt; listIterator(int index) { checkPositionIndex(index); return new ListItr(index);}// 非静态内部类private class ListItr implements ListIterator&lt;E&gt; { private Node&lt;E&gt; lastReturned; private Node&lt;E&gt; next; private int nextIndex; // 创建迭代器时会记录外部类的 modCount private int expectedModCount = modCount; ListItr(int index) { next = (index == size) ? null : node(index); nextIndex = index; } public E next() { // 更新 expectedModCount checkForComodification(); ... } ...}\t5、LinkedList 的序列化过程 🙋🏻‍♀️ 疑问 2：为什么字段都声明 transient 关键字？LinkedList 重写了 JDK 序列化的逻辑，不序列化链表节点，而只是序列化链表节点中的有效数据，这样序列化产物的大小就有所降低。在反序列时，只需要按照对象顺序依次添加到链表的末尾，就能恢复链表的顺序。// 序列化和反序列化只考虑有效数据// 序列化过程private void writeObject(java.io.ObjectOutputStream s) throws java.io.IOException { // Write out any hidden serialization magic s.defaultWriteObject(); // 写入链表长度 s.writeInt(size); // 写入节点上的有效数据 for (Node&lt;E&gt; x = first; x != null; x = x.next) s.writeObject(x.item);}// 反序列化过程private void readObject(java.io.ObjectInputStream s) throws java.io.IOException, ClassNotFoundException { // Read in any hidden serialization magic s.defaultReadObject(); // 读取链表长度 int size = s.readInt();\t\t // 读取有效元素并用 linkLast 添加到链表尾部 for (int i = 0; i &lt; size; i++) linkLast((E)s.readObject());}6、LinkedList 的 clone() 过程LinkedList 中的 first 和 last 指针是引用类型，因此在 clone() 中需要实现深拷贝。否则，克隆后两个 LinkedList 对象会相互影响：private LinkedList&lt;E&gt; superClone() { try { return (LinkedList&lt;E&gt;) super.clone(); } catch (CloneNotSupportedException e) { throw new InternalError(e); }}public Object clone() { LinkedList&lt;E&gt; clone = superClone(); // Put clone into \"virgin\" state clone.first = clone.last = null; clone.size = 0; clone.modCount = 0; // 将原链表中的数据依次添加到新立案表中 for (Node&lt;E&gt; x = first; x != null; x = x.next) clone.add(x.item); return clone;}7、LinkedList 如何实现线程安全？有 5 种方式： 方法 1 - 使用 Collections.synchronizedList 包装类： 原理也是在所有方法上增加 synchronized 关键字； 方法 2 - 使用 ConcurrentLinkedQueue 容器类： 基于 CAS 无锁实现的线程安全队列； 方法 3 - 使用 LinkedBlockingQueue 容器： 基于加锁的阻塞队列，适合于带阻塞操作的生产者消费者模型； 方法 4 - 使用 LinkedBlockingDeque 容器： 基于加锁的阻塞双端队列，适合于带阻塞操作的生产者消费者模型； 方法 5 - 使用 ConcurrentLinkedDeque 容器类： 基于 CAS 无锁实现的线程安全双端队列。三、总结1、LinkedList 是基于链表的线性表，同时具备 List、Queue 和 Stack 的行为；2、在查询指定位置的节点时，如果索引位置小于 size/2，则从头节点开始找，否则从尾节点开始找；3、LinkedList 重写了序列化过程，只处理链表节点中有效的元素；4、LinkedList 和 ArrayList 都不考虑线程同步，不保证线程安全。" }, { "title": "全方面了解MySQL", "url": "/posts/AllMySQL/", "categories": "技术科普", "tags": "学习", "date": "2023-01-07 05:09:00 +0000", "snippet": "全方面了解MySQL一、整体结构浅析MySQL与我们开发项目时相同，为了能够合理的规划整体架构设计，也会将整个MySQL服务抽象成几个大的模块，然后在内部进行实现，因此先来看看MySQL的整体架构，开局先上一张图：从上往下看，依次会分为网络连接层、系统服务层、存储引擎层、以及文件系统层，往往编写SQL后，都会遵守着MySQL的这个架构往下走。 连接层：主要是指数据库连接池，会负责处理所有客...", "content": "全方面了解MySQL一、整体结构浅析MySQL与我们开发项目时相同，为了能够合理的规划整体架构设计，也会将整个MySQL服务抽象成几个大的模块，然后在内部进行实现，因此先来看看MySQL的整体架构，开局先上一张图：从上往下看，依次会分为网络连接层、系统服务层、存储引擎层、以及文件系统层，往往编写SQL后，都会遵守着MySQL的这个架构往下走。 连接层：主要是指数据库连接池，会负责处理所有客户端接入的工作。 服务层：主要包含SQL接口、解析器、优化器以及缓存缓冲区四块区域。 存储引擎层：这里是指MySQL支持的各大存储引擎，如InnoDB、MyISAM等。 文件系统层：涵盖了所有的日志，以及数据、索引文件，位于系统硬盘上。OK~，除了上述的四层外，还有客户端，这个客户端可以是各类编程语言，如Java、Go、Python、C/C++、PHP、Node、.Net....，也可以是一些数据库的可视化软件，例如Navicat、SQLyog等，也可以是mysql-cli命令行工具。总之，只要能与MySQL建立网络连接，都可以被称为是MySQL的客户端。 MySQL-Server就是上述图中的那玩意儿，一般来说，客户端负责编写SQL，而服务端则负责SQL的执行与数据的存储。二、网络连接层当一个客户端尝试与MySQL建立连接时，MySQL内部都会派发一条线程负责处理该客户端接下来的所有工作。而数据库的连接层负责的就是所有客户端的接入工作，MySQL的连接一般都是基于TCP/IP协议建立网络连接，因此凡是可以支持TCP/IP的语言，几乎都能与MySQL建立连接。 其实MySQL还支持另一种连接方式，就是Unix系统下的Socket直连，但这种方式一般使用的较少。虽然MySQL是基于TCP/IP协议栈实现的连接建立工作，但并非使用HTTP协议建立连接的，一般建立连接的具体协议，都会根据不同的客户端实现，如jdbc、odbc...这类的。在这里先暂且不纠结连接MySQL时的协议类型，先来看看一般是怎么连接MySQL的？如下： mysql -h 127.0.0.1 -uroot -p123456例如上述这条指令，-h表示MySQL所在的服务器IP地址，-u表示本次连接所使用的用户名，-p则代表着当前用户的账号密码，当执行这条指令后，会与MySQL-Server建立网络连接，也就是会经历TCP的三次握手过程。当然，MySQL也支持SSL加密连接，如果采用这种方式建立连接，那还会经过SSL多次握手过程，当握手结束，网络建立成功后，则会开始正式的数据库连接建立工作。TCP网络连接建立成功后，MySQL服务端与客户端之间会建立一个session会话，紧接着会对登录的用户名和密码进行效验，MySQL首先会查询自身的用户表信息，判断输入的用户名是否存在，如果存在则会判断输入的密码是否正确，如若密码错误或用户名不存在就会返回1045的错误码，如下信息： ERROR 1045 (28000): Access denied for user ‘zhuzi’@’localhost’ (using password: YES)如果你在连接数据库的过程中，出现了上述的错误信息，那绝对是你输入的用户名或密码错误导致的，当账号及密码正确时，此时就会进入MySQL的命令行，接下来可以执行SQL操作。 但实际上，在用户名和密码都正确的情况下，MySQL还会做一些些小动作，也就是会进行授权操作，查询每个用户所拥有的权限，并对其授权，后续SQL执行时，都会先判断是否具备执行相应SQL语句的权限，然后再执行。OK~，经过上述流程后数据库连接就建立成功了，数据库连接建立成功后，MySQL与客户端之间会采用半双工的通讯机制工作，与之对应的还有“全双工、单工”的工作模式： 全双工：代表通讯的双方在同一时间内，即可以发送数据，也可以接收数据。 半双工：代表同一时刻内，单方要么只能发送数据，要么只能接受数据。 单工：当前连接只能发送数据或只能接收数据，也就是“单向类型的通道”。到这里，MySQL也会“安排”一条线程维护当前客户端的连接，这条线程也会时刻标识着当前连接在干什么工作，可以通过show processlist;命令查询所有正在运行的线程： Id：当前线程的ID值，可以利用这个ID，使用kill强杀线程。 User：当前线程维护的数据库连接，与之对应的用户是谁。 Host：与当前线程保持连接关系的客户端地址（IP+Port）。 db：目前线程在哪个数据库中执行SQL。 Command：当前线程正在执行的SQL类型，如： Create DB：正在执行创建数据库的操作。 Drop DB：正在执行删除数据库的操作。 Execute：正在执行预编译的SQL（PreparedStatement）。 Close Stmt：正在关闭一个PreparedStatement。 Query：正在执行普通的SQL语句。 Sleep：正在等待客户端发送SQL语句。 Quit：当前客户端正在退出连接。 Shutdown：正在关闭MySQL服务端。 Time：表示当前线程处于目前状态的时间，单位是秒。 State：表示当前线程的状态，有如下几种： Updating：当前正在执行update语句，匹配数据做修改操作。 Sleeping：正在等待客户端发送新的SQL语句。 Starting：目前正在处理客户端的请求。 Checking table：目前正在表中查询数据。 Locked：当前线程被阻塞，其他线程获取了执行需要的锁资源。 Sending Data：目前执行完成了Select语句，正在将结果返回给客户端。 Info：一般记录当前线程正在执行的SQL，默认显示前一百个字符，查看完整的SQL可以使用show full processlist;命令。其实从这个结果上来看，我们能够很明显的看到数据库中各个线程的信息，这条指令对于以后做线上排查时有很大的作用，目前先简单了解，接着来看看数据库连接池。数据库连接池Connection Pool翻译过来的意思就是连接池，那为什么需要有这个东西呢？因为前面聊到过，所有的客户端连接都需要一条线程去维护，而线程资源无论在哪里都属于宝贵资源，因此不可能无限量创建，所以这里的连接池就相当于Tomcat中的线程池，主要是为了复用线程、管理线程以及限制最大连接数的。连接池的最大线程数可以通过参数max-connections来控制，如果到来的客户端连接超出该值时，新到来的连接都会被拒绝，关于最大连接数的一些命令主要有两条： show variables like '%max_connections%';：查询目前DB的最大连接数。 set GLOBAL max_connections = 200;：修改数据库的最大连接数为指定值。对于不同的机器配置，可以适当的调整连接池的最大连接数大小，以此可以在一定程度上提升数据库的性能。除了可以查询最大连接数外，MySQL本身还会对客户端的连接数进行统计，对于这点可以通过命令show status like \"Threads%\";查询：其中各个字段的释义如下： Threads_cached：目前空闲的数据库连接数。 Threads_connected：当前数据库存活的数据库连接数。 Threads_created：MySQL-Server运行至今，累计创建的连接数。 Threads_running：目前正在执行的数据库连接数。对于几个字段很容易理解，额外要说明的一点是Threads_cached这个字段，从名称上来看，似乎跟缓存有关系，其实也没错，因为这里是有一个数据库内部的优化机制。当一个客户端连接断开后，对于数据库连接却不会立马销毁，而是会先放入到一个缓存连接池当中。这样就能在下次新连接到来时，省去了创建线程、分配栈空间等一系列动作，但这个值不会是无限大的，一般都在32左右。 连接池的优化思想与Java线程池相同，会将数据库创建出的连接对象放入到一个池中，一旦出现新的访问请求会复用这些连接，一方面提升了性能，第二方面还节省了一定程度上的资源开销。三、系统服务层学习了MySQL网络连接层后，接下来看看系统服务层，MySQL大多数核心功能都位于这一层，包括客户端SQL请求解析、语义分析、查询优化、缓存以及所有的内置函数（例如：日期、时间、统计、加密函数…），所有跨引擎的功能都在这一层实现，譬如存储过程、触发器和视图等一系列服务。也就是上述这几部分，主要包含SQL接口、解析器、优化器以及缓存相关的这些部分。当然，也许你会问我还有一个[管理服务&amp;工具组件]呢，这块其实属于全局的，属于MySQL的基础设施服务，接下来一个个的讲一下服务层的各个细节吧。1、SQL接口SQL接口组件，这个名词听上去似乎不太容易理解，其实主要作用就是负责处理客户端的SQL语句，当客户端连接建立成功之后，会接收客户端的SQL命令，比如DML、DDL语句以及存储过程、触发器等，当收到SQL语句时，SQL接口会将其分发给其他组件，然后等待接收执行结果的返回，最后会将其返回给客户端。 简单来说，也就是SQL接口会作为客户端连接传递SQL语句时的入口，并且作为数据库返回数据时的出口。对于这个组件没太多好聊的，简单展开两点叙述一下后就结束这个话题，第一点是对于SQL语句的类型划分，第二点则是触发器。在SQL中会分为五大类： DML：数据库操作语句，比如update、delete、insert等都属于这个分类。 DDL：数据库定义语句，比如create、alter、drop等都属于这个分类。 DQL：数据库查询语句，比如最常见的select就属于这个分类。 DCL：数据库控制语句，比如grant、revoke控制权限的语句都属于这个分类。 TCL：事务控制语句，例如commit、rollback、setpoint等语句属于这个分类。再来聊一聊MySQL的触发器，这东西估计大部分小伙伴没用过，但它在有些情景下还较为实用，不过想要了解触发器是什么，首先咱们还得先理解存储过程。 存储过程：是指提前编写好的一段较为常用或复杂SQL语句，然后指定一个名称存储起来，然后先经过编译、优化，完成后，这个“过程”会被嵌入到MySQL中。也就是说，[存储过程]的本质就是一段预先写好并编译完成的SQL，而我们要聊的触发器则是一种特殊的存储过程，但[触发器]与[存储过程]的不同点在于：存储过程需要手动调用后才可执行，而触发器可由某个事件主动触发执行。在MySQL中支持INSERT、UPDATE、DELETE三种事件触发，同时也可以通过AFTER、BEFORE语句声明触发的时机，是在操作执行之前还是执行之后。 说简单一点，[MySQL触发器]就类似于Spring框架中的AOP切面。2、解析器客户端连接发送的SQL语句，经过SQL接口后会被分发到解析器，解析器这东西其实在所有语言中都存在，Java、C、Go...等其他语言都有，解析器的作用主要是做词法分析、语义分析、语法树生成…这类工作的。而解析器这一步的作用主要是为了验证SQL语句是否正确，以及将SQL语句解析成MySQL能看懂的机器码指令。稍微拓展一点大家就明白了，好比如我们编写如下一条SQL：select * form user;然后运行会得到如下错误信息： ERROR 1064 (42000): You have an error in your SQL syntax; check….在上述SQL中，我们将from写成了form，结果运行时MySQL提示语法错误了，MySQL是如何发现的呢？就是在词法分析阶段，检测到了存在语法错误，因此抛出了对应的错误码及信息。当然，如果SQL正确，则会进行下一步工作，生成MySQL能看懂的执行指令。3、优化器解析器完成相应的词法分析、语法树生成….等一系列工作后，紧接着会来到优化器，优化器的主要职责在于生成执行计划，比如选择最合适的索引，选择最合适的join方式等，最终会选择出一套最优的执行计划。 当然，在这里其实有很多资料也会聊到，存在一个执行器的抽象概念，实际上执行器是不存在的，因此前面聊到过，每个客户端连接在MySQL中都用一条线程维护，而线程是操作系统的最小执行单位，因此所谓的执行器，本质上就是线程本身。优化器生成了执行计划后，维护当前连接的线程会负责根据计划去执行SQL，这个执行的过程实际上是在调用存储引擎所提供的API。4、缓存&amp;缓冲这块较为有趣，主要分为了读取缓存与写入缓冲，读取缓存主要是指select语句的数据缓存，当然也会包含一些权限缓存、引擎缓存等信息，但主要还是select语句的数据缓存，MySQL会对于一些经常执行的查询SQL语句，将其结果保存在Cache中，因为这些SQL经常执行，因此如果下次再出现相同的SQL时，能从内存缓存中直接命中数据，自然会比走磁盘效率更高，对于Cache是否开启可通过命令查询。 show global variables like \"%query_cache_type%\";：查询缓存是否开启。 show global variables like \"%query_cache_size%\";：查询缓存的空间大小。 同时还可以通过show status like'%Qcache%';命令查询缓存相关的统计信息。 Qcache_free_blocks：查询缓存中目前还有多少剩余的blocks。 Qcache_free_memory：查询缓存的内存大小。 Qcache_hits：表示有多少次查询SQL命中了缓存。 Qcache_inserts：表示有多少次查询SQL未命中缓存然后走了磁盘。 Qcache_lowmem_prunes：这个值表示有多少条缓存数据从内存中被淘汰。 Qcache_not_cached：表示由于自己设置了缓存规则后，有多少条数据不符合缓存条件。 Qcache_queries_in_cache：表示当前缓存中缓存的数据数量。 Qcache_total_blocks：当前缓存区中blocks的数量。当然，由于我是MySQL5.7版本，因此对于这些依旧可以查询到，但是在高版本的MySQL中，移除了查询缓存区，毕竟命中率不高，而且查询缓存这一步还要带来额外开销，同时一般程序都会使用Redis做一次缓存，因此结合多方面的原因就移除了查询缓存的设计。简单了解了查询缓存后，再来看看写入缓冲，这也是我说的比较有趣的点，缓冲区的设计主要是：为了通过内存的速度来弥补磁盘速度较慢对数据库造成的性能影响。在数据库中读取某页数据操作时，会先将从磁盘读到的页存放在缓冲区中，后续操作相同页的时候，可以基于内存操作。一般来说，当你对数据库进行写操作时，都会先从缓冲区中查询是否有你要操作的页，如果有，则直接对内存中的数据页进行操作（例如修改、删除等），对缓冲区中的数据操作完成后，会直接给客户端返回成功的信息，然后MySQL会在后台利用一种名为Checkpoint的机制，将内存中更新的数据刷写到磁盘。 MySQL在设计时，通过缓冲区能减少大量的磁盘IO，从而进一步提高数据库整体性能。毕竟每次操作都走磁盘，性能自然上不去的。PS：后续高版本的MySQL移除了查询缓存区，但并未移除缓冲区，这是两个概念，请切记！ 同时缓冲区是与存储引擎有关的，不同的存储引擎实现也不同，比如InnoDB的缓冲区叫做innodb_buffer_pool，而MyISAM则叫做key_buffer。四、存储引擎层存储引擎也可以理解成MySQL最重要的一层，在前面的服务层中，聚集了MySQL所有的核心逻辑操作，而引擎层则负责具体的数据操作以及执行工作。如果有小伙伴研究过Oracle、SQLServer等数据库的实现，应该会发现这些数据库只有一个存储引擎，因为它们是闭源的，所以仅有官方自己提供的一种引擎。而MySQL则因为其开源特性，所以存在很多很多款不同的存储引擎实现，MySQL为了能够正常搭载不同的存储引擎运行，因此引擎层是被设计成可拔插式的，也就是可以根据业务特性，为自己的数据库选择不同的存储引擎。 MySQL的存储引擎主要分为官方版和民间版，前者是MySQL官方开发的，后者则是第三方开发的。存储引擎在MySQL中，相关的规范标准被定义成了一系列的接口，如果你也想要使用自己开发的存储引擎，那么只需要根据MySQL AB公司定义的准则，编写对应的引擎实现即可。MySQL目前有非常多的存储引擎可选择，其中最为常用的则是InnoDB与MyISAM引擎，可以通过show variables like '%storage_engine%';命令来查看当前所使用的引擎。其他引擎如下：存储引擎是MySQL数据库中与磁盘文件打交道的子系统，不同的引擎底层访问文件的机制也存在些许细微差异，引擎也不仅仅只负责数据的管理，也会负责库表管理、索引管理等，MySQL中所有与磁盘打交道的工作，最终都会交给存储引擎来完成。五、文件系统层这一层则是MySQL数据库的基础，本质上就是基于机器物理磁盘的一个文件系统，其中包含了配置文件、库表结构文件、数据文件、索引文件、日志文件等各类MySQL运行时所需的文件，这一层的功能比较简单，也就是与上层的存储引擎做交互，负责数据的最终存储与持久化工作。 这一层主要可分为两个板块：①日志板块。②数据板块。1、日志模块在MySQL中主要存在七种常用的日志类型，如下： ①binlog二进制日志，主要记录MySQL数据库的所有写操作（增删改）。 ②redo-log重做/重写日志，MySQL崩溃时，对于未落盘的操作会记录在这里面，用于重启时重新落盘（InnoDB专有的）。 ③undo-logs撤销/回滚日志：记录事务开始前[修改数据]的备份，用于回滚事务。 ④error-log：错误日志：记录MySQL启动、运行、停止时的错误信息。 ⑤general-log常规日志，主要记录MySQL收到的每一个查询或SQL命令。 ⑥slow-log：慢查询日志，主要记录执行时间较长的SQL。 ⑦relay-log：中继日志，主要用于主从复制做数据拷贝。上述列出了MySQL中较为常见的七种日志，但实际上还存在很多其他类型的日志，不过一般对调优、排查问题、数据恢复/迁移没太大帮助，用的较少，因此不再列出。2、数据模块前面聊到过，MySQL的所有数据最终都会落盘（写入到磁盘），而不同的数据在磁盘空间中，存储的格式也并不相同，因此再列举出一些MySQL中常见的数据文件类型： db.opt文件：主要记录当前数据库使用的字符集和验证规则等信息。 .frm文件：存储表结构的元数据信息文件，每张表都会有一个这样的文件。 .MYD文件：用于存储表中所有数据的文件（MyISAM引擎独有的）。 .MYI文件：用于存储表中索引信息的文件（MyISAM引擎独有的）。 .ibd文件：用于存储表数据和索引信息的文件（InnoDB引擎独有的）。 .ibdata文件：用于存储共享表空间的数据和索引的文件（InnoDB引擎独有）。 .ibdata1文件：这个主要是用于存储MySQL系统（自带）表数据及结构的文件。 .ib_logfile0/.ib_logfile1文件：用于故障数据恢复时的日志文件。 .cnf/.ini：MySQL的配置文件，Windows下是.ini，其他系统大多为.cnf。 ......上述列举了一些MySQL中较为常见的数据文件类型，无论是前面的日志文件，亦或是现在的数据文件，这些都是后续深入剖析MySQL时会遇到的，因此在这里先有个简单认知，方便后续更好的理解MySQL。 当然，上述并没有完全列出MySQL所有的日志类型和文件类型，大家有兴趣的可以去自行翻看一下安装MySQL的目录，你会找其中找到很多其他类型的日志或数据文件~六、小结​\t本文的主要目的是在于先对MySQL的整体架构有一个基本认知，这也为后续的学习打下了坚实的基础，因为毕竟想要深入研究一个技术，那定然不能如同管中窥豹一般，仅看一个细节点，而是更应该是先窥其全貌，再深入细节。" }, { "title": "单调栈", "url": "/posts/MonStack/", "categories": "数据结构与算法", "tags": "学习", "date": "2022-12-31 12:34:00 +0000", "snippet": "单调栈一、何为单调栈？ 在LeetCode刷题时，遇见[84. 柱状图中最大的矩形]问题所学习到的一种方法，故在此记录下来。从名字上就听的出来，单调栈中存放的数据应该是有序的，所以单调栈也分为单调递增栈和单调递减栈 单调递增栈：单调递增栈就是从栈底到栈顶数据是从大到小 单调递减栈：单调递减栈就是从栈底到栈顶数据是从小到大显而易见，从单调栈的这种结构很容易联想到，在算法中，合理运用单调栈...", "content": "单调栈一、何为单调栈？ 在LeetCode刷题时，遇见[84. 柱状图中最大的矩形]问题所学习到的一种方法，故在此记录下来。从名字上就听的出来，单调栈中存放的数据应该是有序的，所以单调栈也分为单调递增栈和单调递减栈 单调递增栈：单调递增栈就是从栈底到栈顶数据是从大到小 单调递减栈：单调递减栈就是从栈底到栈顶数据是从小到大显而易见，从单调栈的这种结构很容易联想到，在算法中，合理运用单调栈，能够将O(n^2)的时间复杂度优化到O(n)，这就是技巧。相对的，空间复杂度会增加，因为需要动态维护一个栈。这里需要明白一点，算法里面，都是时间和空间的取舍，所谓的时空间转换指的就是这个，所以要根据具体场景去选择。 将O(n^2)的时间复杂度优化到O(n)，则在暴力迭代的思想上进一步优化。伪代码：Deque&lt;Integer&gt; stack = new ArrayDeque&lt;&gt;();//此处一般需要给数组最后添加结束标志符，具体下面例题会有详细讲解for (遍历这个数组){\tif (栈空 || 栈顶元素大于等于当前比较元素)\t{\t\t入栈;\t}\telse\t{\t\twhile (栈不为空 &amp;&amp; 栈顶元素小于当前元素)\t\t{\t\t\t栈顶元素出栈;\t\t\t更新结果;\t\t}\t\t当前数据入栈;\t}}二、具体应用1、接雨水给定 n 个非负整数表示每个宽度为 1 的柱子的高度图，计算按此排列的柱子，下雨之后能接多少雨水解题思路： 维护一个单调栈，单调栈存储的是下标，满足从栈底到栈顶的下标对应的数组height中的元素递减 从左到右遍历数组，遍历到下标 i时，如果栈内至少有两个元素，记栈顶元素为 top，top的下面一个元素是 left，则一定有 height[left]≥height[top]。如果 height[i]&gt;height[top]，则得到一个可以接雨水的区域，该区域的宽度是 i−left−1，高度是 min⁡(height[left],height[i])−height[top]，根据宽度和高度即可计算得到该区域能接的雨水量 为了得到 left，需要将 top出栈。在对 top 计算能接的雨水量之后，left变成新的 top，重复上述操作，直到栈变为空，或者栈顶下标对应的 height 中的元素大于或等于 height[i] 对下标 i处计算能接的雨水量之后，将 i入栈，继续遍历后面的下标，计算能接的雨水量。遍历结束之后即可得到能接的雨水总量。代码实现：class Solution { //使用单调栈的方法进行求解 public int trap(int[] height) { //维护一个变量用于记录能接受多少雨水 int result = 0; //获取数组的长度 int n = height.length; //创建一个单调栈进行遍历 Deque&lt;Integer&gt; stack = new LinkedList&lt;Integer&gt;(); //遍历数组 进行求解结果 for(int i = 0; i &lt; n; i++){ //当栈不为空时且要当前下标对应的值大于栈顶下标对应的值时 while(!stack.isEmpty() &amp;&amp; height[i] &gt; height[stack.peek()]){ //获取栈顶元素 int top = stack.pop(); //此时若栈为空则直接跳过 if(stack.isEmpty()){ break; } //获取当前宽度 int curHeight = Math.min(height[i],height[stack.peek()]) - height[top]; //获取当前高度 int curWidth = i - stack.peek() -1; //更新最大面积 result += curHeight * curWidth; } stack.push(i); } return result; }}2、柱状图中最大的矩形给定 n 个非负整数，用来表示柱状图中各个柱子的高度。每个柱子彼此相邻，且宽度为 1 。求在该柱状图中，能够勾勒出来的矩形的最大面积。解题思路： 维护一个单调栈，单调栈存储的是下标，满足从栈底到栈顶的下标对应的数组heights中的元素递增 遍历数组时，若遇到了当前柱形的高度比上一个柱形的高度小的时候，则一定要可以确定之前的某些柱形的最大宽度，并且他的顺序是从右往左的 一次遍历数组后，栈内仍然可能存在元素，继续使用相同的方法确定其最大宽度，更新面积。代码实现：class Solution { public int largestRectangleArea(int[] heights) { //获取数组的长度 int len = heights.length; //特殊情况判断:len == 1 if (len == 1) { return heights[0]; } //维护一个变量用于记录最大矩形的面积 int result = 0; //创建栈用于存储下标 达到优化时间复杂度的效果 Deque&lt;Integer&gt; stack = new ArrayDeque&lt;&gt;(len); //一次遍历数组 获取最大面积 for (int i = 0; i &lt; len; i++) { //当栈不为空时(因为不止一个柱形的最大宽度可以被计算出来)且小于栈顶元素时 //出栈栈顶元素并确定其最大宽度 从而得出对应的面积 while (!stack.isEmpty() &amp;&amp; heights[i] &lt; heights[stack.peekLast()]){ //获取当前高度 int curHeight = heights[stack.pollLast()];//返回元素并弹出 //如果栈顶以下元素对应的高度与栈顶元素对应的高度相同 则要一同弹出 while (!stack.isEmpty() &amp;&amp; curHeight == heights[stack.peekLast()]){ stack.pollLast(); } //计算当前高度对应的最大宽度 int curWidth = 0; if (stack.isEmpty()){ curWidth = i; }else{ curWidth = i - stack.peekLast() -1; } //更新最大面积 result = Math.max(result,curHeight * curWidth); } //之后将元素压入栈中 stack.addLast(i); } //一次遍历后 可能还存在留在栈中的元素 while (!stack.isEmpty()){ //获取当前高度 int curHeight = heights[stack.pollLast()];//返回元素并弹出 //如果栈顶以下元素对应的高度与栈顶元素对应的高度相同 则要一同弹出 while (!stack.isEmpty() &amp;&amp; curHeight == heights[stack.peekLast()]){ stack.pollLast(); } //计算当前高度对应的最大宽度 int curWidth = 0; if (stack.isEmpty()){ curWidth = len; }else{ curWidth = len - stack.peekLast() -1; } //更新最大面积 result = Math.max(result,curHeight * curWidth); } return result; }}3、每日温度给定一个整数数组 temperatures ，表示每天的温度，返回一个数组 answer ，其中 answer[i] 是指对于第 i 天，下一个更高温度出现在几天后。如果气温在这之后都不会升高，请在该位置用 0 来代替。代码实现：class Solution { public int[] dailyTemperatures(int[] temperatures) { //获取温度数组的长度 int length = temperatures.length; //创建结果数组 int[] answer = new int[length]; //不进行设置数组元素默认为零 //创建数据结构栈进行辅助 Deque&lt;Integer&gt; stack = new LinkedList&lt;Integer&gt;(); //循环数组进行比较 for(int i = 0; i &lt; length; ++i){ int temperature = temperatures[i]; //判断当前温度与栈中存储温度的大小 while(!stack.isEmpty() &amp;&amp; temperature &gt; temperatures[stack.peek()]){ //获取栈顶元素并弹出 int prevIndex = stack.pop(); answer[prevIndex] = i - prevIndex; } stack.push(i); } return answer; }}三、单调栈的优化–哨兵模式在问题[柱状图中最大的矩形]中，我们使用普通的单调栈进行求解时，需要考虑两种情况：①谈栈的时候，栈为空；②遍历完成后，栈还有元素。因此，我们可以在数组的两边加上高度为0的柱形，从而避免这两种情况的讨论。站在两边的柱形有一个很形象的名词：哨兵代码实现：class Solution { public int largestRectangleArea(int[] heights) { //获取数组的长度 int len = heights.length; //特殊情况判断:len == 1 if (len == 1) { return heights[0]; } //维护一个变量用于记录最大矩形的面积 int result = 0; //在原先数组的左右添加哨兵,创建一个新的数组 int[] newHeights = new int[len+2]; //拷贝数组数组 左右哨兵默认为空 System.arraycopy(heights,0,newHeights,1,len); heights = newHeights; //创建栈用于存储下标 达到优化时间复杂度的效果 Deque&lt;Integer&gt; stack = new ArrayDeque&lt;&gt;(len); //左哨兵先入栈 stack.addLast(0); //遍历数组获取最大值 for (int i = 1; i &lt; len+2; i++) { //当栈不为空时(因为不止一个柱形的最大宽度可以被计算出来)且小于栈顶元素时 //出栈栈顶元素并确定其最大宽度 从而得出对应的面积 while (heights[i] &lt; heights[stack.peekLast()]){ //获取当前高度 int curHeight = heights[stack.pollLast()];//返回元素并弹出 //计算当前高度对应的最大宽度 int curWidth = i - stack.peekLast() -1; //更新最大面积 result = Math.max(result,curHeight * curWidth); } //将元素压入栈中 stack.addLast(i); } return result; }}" }, { "title": "计算机网络体系", "url": "/posts/Network/", "categories": "技术科普", "tags": "学习", "date": "2022-12-29 07:00:00 +0000", "snippet": "计算机网络体系一、简介简单来说，计算机网络就是用于满足不同机器之间通信、共享的一种系统。其实一开始计算机网络是用于军事化目的的，因美国军方需要一种多节点的通信技术，确保在其一部分节点被摧毁后，也能够确保通信依旧正常，因此组建了一个部门ARPA并在1966年完成了ARPANET（阿帕网）项目，该项目也是最早的计算机网络之一。 当然，后续随着科技不断进步，各种网络相关的技术百家争鸣，出现了各式各...", "content": "计算机网络体系一、简介简单来说，计算机网络就是用于满足不同机器之间通信、共享的一种系统。其实一开始计算机网络是用于军事化目的的，因美国军方需要一种多节点的通信技术，确保在其一部分节点被摧毁后，也能够确保通信依旧正常，因此组建了一个部门ARPA并在1966年完成了ARPANET（阿帕网）项目，该项目也是最早的计算机网络之一。 当然，后续随着科技不断进步，各种网络相关的技术百家争鸣，出现了各式各样的网络技术，但同时也带来了很大的局限性，例如：不同的计算机网络之间并不能相互通信、不同操作系统之间无法通信、覆盖范围及其有限等。因此，ARPA需要一种技术将不同的计算机局域网互联，最终ARPA组织创建了一个新的项目被称为internetwork因特网，也被称为“互联网”。在internetwork项目的研发过程中，1974年，Robert E. Kahn以及Vinton G. Cerf两位教授正式提出了新的传输协议：TCP/IP协议，用于满足不同计算机网络之间的互联通信，ARPA到1982年接受TCP/IP选定为Internet主要的计算机通信系统，作为因特网通信的“基石”。二、 组成与核心功能1、组成计算机网络主要由主机、协议、传输介质以及软件四部分组成 主机：可以是手机、电脑、服务器、电子手表等任意计算机硬件 协议：各类定义归法的网络通信协议，如TCP/IP、IPX/SPX、AppleTalk等 传输介质：传输数据的通道，可以是实体铜线、电缆、光纤，也可以是无形的电磁波空间介质 软件：涵盖所有联网的应用，如QQ、微信、支付宝、淘宝、京东等各类软件2、核心功能计算机网络体系主要提供的两个核心功能就在于： 资源共享：资源子网实现 数据通信：通信子网实现资源子网以及通信子网也对应着OSI中的上三层和下三层三、网络分层与三种模型 法定标准体系：OSI七层结构 实际应用体系：TCP/IP四层结构 原理教学体系：五层结构 1、OSI模型​\t计算机网络是个非常复杂的系统，由于最初各个计算机网络体系结构不同，导致双方网络之间无法互通，因此，20世纪90年代，ISO国际标准化推出了OSI模型打算制定计算机网络体系标准。2、TCP/IP体系结构​\t但由ISO组织制定的OSI网络七层模型结构并没有得到广泛应用，实际中应用最广泛的是TCP/IP体系结构。换句话说，OSI七层模型只是理论上官方制定的国际标准，而TCP/IP体系结构才是事实上的国际标准。3、五层模型体系结构​\t但因为TCP/IP体系中的最后一层没有制定规范的标准，所以对于学习计算机网络完善体系时会缺失一部分，所以又提出了一种折中方案，也就是综合OSI以及TCP/IP两个体系的优缺点，提出了一种五层结构的原理体系。因此在接触计算机网络体系时，通常都会存在三种分层结构：四、IP协议 属于网络成协议当应用层的数据被封装后，想要将数据在网络上传输，数据究竟要被发往何处，又该如何精准的在网络上定位目标机器，此时起到关键作用的就是“IP协议”。 IP协议的作用在于把各种数据包准确无误的传递给目标方，其中两个重要的条件是IP地址和MAC地址。其中IP地址就是所有主机在网络通信中的唯一标识，但由于IP地址是稀有资源，不可能每个主机都拥有一个IP地址，因此路由器里面会记录我们主机的MAC地址，通常的IP地址是路由器根据MAC地址生成的，而MAC地址是全球唯一的。 IP地址就如同是物流线路上的驿站地址，而MAC地址就是具体货架上货物的位置1、组成与分类IP地址一般由网络标识(NetID)*和*主机标识(HostID) 两部分组成，其中网络标识对应着网络地址，表示其局域网属于互联网中的哪一个网络；主机标识对应着一台机器的主机地址，表示机器属于该局域网络中的哪一台主机。 通常情况下，一个IP存在四组数字，每组数字对应着八位二进制数字（一个IP地址共计32Bit），每组之间分别用.隔开，其中不同类型的IP地址，表示网络标识和主机标识的数字段也不同，目前的IP地址主要可分为A、B、C、D、E五大类，如下：因不同类型的网络IP规模不同，所以它们也分别应用于不同的场景，如： ①A类IP适用于大型网络，由于单个网络中可容纳的主机数非常巨大，因此常被保留给政府机构使用。 ②B类IP适用于中型网络，一般会被分配给公益组织、中大型企业等。 ③C类IP适用于小型网络，这种IP适用于所有需要网络的个体和小集体，如网吧、家庭、个人电脑等。 ④D类IP用于组播。 ⑤E类IP用于保留和实验。2、子网掩码子网掩码又被称为网络掩码、地址掩码、子网络遮罩，它的作用主要有两个： 一、区分IP中网络地址和主机地址。 二、划分子网，扩大网段内的可用IP数目。但默认的子网掩码值也并非都相同，不同的网络类型存在不同的默认掩码，如： A类网络的默认子网掩码：255.0.0.0 B类网络的默认子网掩码：255.255.0.0 C类网络的默认子网掩码：255.255.255.0一个子网掩码决定着一个子网（独立的单个网络）内可容纳的主机数量，计算公式为：可容纳的主机数量=(2的n次方)-2。这个n可以理解为二进制掩码中0的数量，例如： A类默认掩码：255.0.0.0转换为二进制为：11111111.00000000.00000000.00000000，后面有24个0，因此可容纳的主机数量为(2的24次方)-2，即16777214台主机。 C类默认掩码：255.255.255.0转换为二进制为：11111111.11111111.11111111.00000000，后面存在8个0，因此C类网可容纳主机数量为(2的8次方)-2，即254台主机。3、IPv4与IPv6目前的网络几乎大部分还是基于IPv4版本，但同时大部分应用程序也开始支持IPv6，IPv6是“Internet Protocol Version 6（互联网协议第6版）”的缩写，是用于替代IPv4的下一代IP协议，也就是下一代互联网的协议。IPv6相较于IPv4而言，主要不同点在于： 地址空间不同，IPv4地址采用32位长度，IPv6的地址则采用128位长度。 IPv6的路由表会比IPv4更小、更精细。 IPv6的组播支持以及对流的支持要强于IPv4。 IPv6的安全性更高，使用IPv6的用户可对网络层数据进行加密。 协议扩充不同，IPv6允许协议进行扩充，而IPv4不允许。前述的IP协议分析都是基于IPv4版本而言的，因为目前主流的网络版本还是IPv4，但如今也逐步向IPv6过渡。五、TCP协议与UDP协议 两者都属于传输层协议1、TCP协议TCP(Transmission Control Protocol)传输控制协议是面向连接的可靠传输协议，是位于传输层的核心协议之一，在不可靠的互联网络上，IP协议只提供了简单不可靠的包交换，但网络中不同主机之间经常需要一种可靠的、类似于管道一样的连接、流机制，去稳定传输一些数据，如视频、音频、图片等大文件数据。 因此TCP应运而生，TCP协议是为了在不可靠的互联网络上提供可靠的端到端之间，字节流传输而专门设计的一个传输协议，TCP中采用字节流传输数据。1、TCP报文头结构当应用层向传输层传递数据时，TCP会首先对数据流进行分段，将大的数据拆分成一个个的数据报文段，然后会将封装好的数据包传递给网络层的IP层。同时，为了防止数据在网络传输中丢包，TCP也会对每个数据包分配一个序号，当接收方成功收到发送的数据后，会返回一个ACK确认，如果发送方在规定的合理时间（RTT）内未收到接收方的ACK，那么对应的数据包会被认定为已丢失，发送方会将该段数据重新传输。2、连接管理机制 - 三次握手与四次挥手​\t由于TCP是基于管道连接式通信的协议，因此在数据传递之前，必须要先建立连接，当数据传输完毕后，也必须要关闭连接。因此，这就引出了面试过程中人尽皆知的问题：“为什么TCP是三次握手，四次挥手！”三次握手：所谓的TCP三次握手，其实是指TCP建立连接的过程，因为TCP属于可靠性的传输协议，因此在发送数据前必须要先确保发送/接收数据的双方状态正常，因此需要经过“三次握手”的过程，具体如下：四次挥手：当“三次握手”完成后，客户端和服务端之间会成功建立连接，从此开启双方端到端之间的数据传输，当一方数据传输完成后，会尝试中断连接，因此又会经历“四次挥手”的过程，如下： 为什么TCP是三次握手，四次挥手? 因为建立连接“握手”时，当服务端接收到“客户端想与服务端建立连接”的请求后，可以立马返回“同意+与客户端建立连接”报文，客户端也确认建立连接后，就可以称为“握手完成”。 但关闭连接时的“挥手”，因为一方数据传输完成后就会提出关闭连接，不过另外一方可能还依旧存在数据未发送完成，因此服务端就不能在“确认关闭”连接的时候，也同时发出“关闭连接”的请求，因为自己的数据还没发送完成呢，所以会等到自身的数据全部传输后，再主动向客户端发起一次“关闭连接”的请求，等待客户端“确认关闭”后，从而完成整个“挥手”动作。2、UDP协议UDP(User Datagram Protocol)协议是传输层的一个不可靠传输协议，它为应用程序提供了一种无需建立连接就可以发送封装的IP数据包的方法。在传输层中，与TCP协议互补，UDP除了给应用层提供了发送数据包的功能外，几乎没有做任何其他事情。而面向连接的TCP恰恰相反，几乎做了所有的事情。刚刚提到过，UDP仅为应用层提供了发送数据报的功能，主要就是指UDP对IP协议的扩充： ①建立在IP协议的基础上，扩展出端口号，可使数据分发到具体的应用程序。 ②建立在IP协议的基础上，扩展出数据传输过程中的数据差错效验机制。1、UDP报文头对比TCP复杂的报文头结构，UDP的头部就显得比较简单了，整个头共8字节： ①源端口/目的端口：指数据发送方的应用进程端口号及接收方的进程端口号。 ②报文长度：Header+Data的总长度，因为UDP头为8字节，所以该值最小为8。 效验和：检测UDP数据报在传输中是否有错，有则丢弃（UDP检验和并非必须的），就算效验时检测出错误，也仅只是丢弃数据包，不会对数据进行纠正，也就是不会重发数据报。2、核心流程UDP是一个无连接的协议，因此采用UDP传输协议的程序，在传递数据时，不会存在建立/释放连接的过程。当数据需要传输时，会对于应用层的数据简单的封装，也就是加上自己的UDP头后，直接会将数据丢给IP层，然后交由链路传输。 正因为如上特性，因此UDP的传输速度仅受到数据生成的速度、计算机算力和传输带宽的限制。在接收端，UDP会把每个消息段放在队列/缓冲区中，程序每次从队列中读一个消息段。当然，接收端收到数据后，也会对数据做效验，但效验完成后，如若数据存在差错，那UDP只会单纯的丢弃该数据包，不会要求发送端重发数据。 因为由于UDP高效的传输性能，因此常备应用在广播通知、音频通话、视频传输等多媒体数据流业务，而且这类业务中，如果有一个数据包丢失，在很短的时间内就会有另一个新的数据就会替换它，因此就算数据传输不可靠也无关紧要。" }, { "title": "加密算法", "url": "/posts/Password/", "categories": "技术科普", "tags": "学习", "date": "2022-12-28 07:00:00 +0000", "snippet": "加密算法一、加密算法的种类1、对称加密​\t加密和解密用的都是同一个秘钥的加密形式，就叫对称加密。2、非对称加密 公钥负责加密，私钥负责解密。公钥人人可得，私钥永远不泄露​\t加密和解密用到的不是同一个秘钥，而是两个不一样的秘钥，分别是公钥和私钥，就叫做非对称加密。二、 为啥不能用公钥解密？ 说白了加密就是将一个已知的数字根据一定的规则转换变成另一个数字，以前这些数字放在一起都可读，但是经过...", "content": "加密算法一、加密算法的种类1、对称加密​\t加密和解密用的都是同一个秘钥的加密形式，就叫对称加密。2、非对称加密 公钥负责加密，私钥负责解密。公钥人人可得，私钥永远不泄露​\t加密和解密用到的不是同一个秘钥，而是两个不一样的秘钥，分别是公钥和私钥，就叫做非对称加密。二、 为啥不能用公钥解密？ 说白了加密就是将一个已知的数字根据一定的规则转换变成另一个数字，以前这些数字放在一起都可读，但是经过这么一转换，就变得不可读了。也就是说加密的本质就是 num -&gt; x （num是已知数，x是未知数） 从数学原理也能看出，公钥和私钥加密是安全的，但这件事情的前提是建立在”现在的计算机计算速度还不够快”这个基础上。因此，如果有一天科技变得更发达了，我们变成了更高维度的科技文明，可能现在的密文就跟明文没啥区别了。因为大数取模运算是不可逆的，因此他人无法暴力解密。但是结合欧拉定理，我们可以选取出合适的p（公钥）, q（私钥）, N（用于取模的大数），让原本不可逆的运算在特定情况下，变得有那么点“可逆”的味道。数学原理决定了我们用公钥加密的数据，只有私钥能解密。反过来，用私钥加密的数据，也只有公钥能解密。三、HTTPS的原理 了解了对称加密和非对称机密之后，我们就可以聊聊HTTPS的加密原理了首先是建立TCP连接，毕竟HTTP是基于TCP的应用层协议(TCP三次握手)在TCP成功建立完协议后，就可以开始进入HTTPS的加密流程。总的来说。整个加密流程其实分为两阶段。第一阶段是TLS四次握手，这一阶段主要是利用非对称加密的特性各种交换信息，最后得到一个”会话秘钥”。第二阶段是则是在第一阶段的”会话秘钥“基础上，进行对称加密通信。我们先来看下第一阶段的TLS四次握手是怎么样的。第一次握手： Client Hello：是客户端告诉服务端，它支持什么样的加密协议版本，比如 TLS1.2，使用什么样的加密套件，比如最常见的RSA，同时还给出一个客户端随机数。第二次握手： Server Hello：服务端告诉客户端，服务器随机数 + 服务器证书 + 确定的加密协议版本（比如就是TLS1.2）。第三次握手： Client Key Exchange: 此时客户端再生成一个随机数，叫 pre_master_key 。从第二次握手的服务器证书里取出服务器公钥，用公钥加密 pre_master_key，发给服务器。 Change Cipher Spec: 客户端这边已经拥有三个随机数： 客户端随机数，服务器随机数和pre_master_key，用这三个随机数进行计算得到一个”会话秘钥“。此时客户端通知服务端，后面会用这个会话秘钥进行对称机密通信。 Encrypted Handshake Message：客户端会把迄今为止的通信数据内容生成一个摘要，用”会话秘钥“加密一下，发给服务器做校验，此时客户端这边的握手流程就结束了，因此也叫Finished报文。第四次握手： Change Cipher Spec：服务端此时拿到客户端传来的 pre_master_key（虽然被服务器公钥加密过，但服务器有私钥，能解密获得原文），集齐三个随机数，跟客户端一样，用这三个随机数通过同样的算法获得一个”会话秘钥“。此时服务器告诉客户端，后面会用这个”会话秘钥”进行加密通信。 Encrypted Handshake Message：跟客户端的操作一样，将迄今为止的通信数据内容生成一个摘要，用”会话秘钥“加密一下，发给客户端做校验，到这里，服务端的握手流程也结束了，因此这也叫Finished报文。 https都于对称加密与非对称加密两种方式都使用到了，前期4次握手，本质上就是在利用非对称加密的特点，交换三个随机数。 目的就是为了最后用这三个随机数生成对称加密的”会话秘钥”。后期就一直用对称机密的方式进行通信。四、服务器证书与CA证书 HTTPS连接过程中涉及到了两对公钥与秘钥的使用服务器本身的公钥和私钥：在第二次握手中，服务器将自己的公钥（藏在数字证书里）发给客户端。第三次握手中用这个服务器公钥来加密第三个随机数 pre_master_key。服务器拿到后用自己的私钥去做解密。CA的公钥和私钥：第二次握手中，传的数字证书里，包含了被CA的私钥加密过的服务器公钥。客户端拿到后，会用实现内置在操作系统或浏览器里的CA公钥去进行解密。CA秘钥加密服务器公钥：CA公钥解密服务器证书：CA公钥内置在操作系统或浏览器里：" }, { "title": "如何快速地上线一个项目？", "url": "/posts/IssueBlog/", "categories": "技术科普", "tags": "学习", "date": "2022-12-22 06:19:00 +0000", "snippet": "如何快速地上线一个项目？ 本文将提供一种快速上线项目的方法，适合已经了解上线项目流程的读者进行阅读，且项目以前后端分离的项目为例。一、多环境 多环境是指同一套代码在不同的阶段需要根据实际情况来调整配置部署到不同的机器上1.环境的分类： 本地环境：自己的电脑 开发环境：团队开发 拥有远程服务器 测试环境：性能测试/功能测试/系统集成测试** 独立的数据库与服务器 与生产环境并不相同...", "content": "如何快速地上线一个项目？ 本文将提供一种快速上线项目的方法，适合已经了解上线项目流程的读者进行阅读，且项目以前后端分离的项目为例。一、多环境 多环境是指同一套代码在不同的阶段需要根据实际情况来调整配置部署到不同的机器上1.环境的分类： 本地环境：自己的电脑 开发环境：团队开发 拥有远程服务器 测试环境：性能测试/功能测试/系统集成测试** 独立的数据库与服务器 与生产环境并不相同 预发布环境：调用的后端接口、连接的数据库、服务等都 和线上项目一致 ，和线上唯一的区别就是前端访问的域名不同 类似体验服 生产环境：正式服 沙箱环境：测试环境的一种 为了测试某个功能而单独开辟的环境 用完即删2.多环境的实现方式 抽象配置类 配置文件化 注入环境参数3.为什么需要多环境 每个环境互不影响 区分项目开发的不同阶段 方便对项目进行优化4.前端多环境实战 请求地址修改 开发环境：localhost 生产环境：xxxx.com 项目如何知道何时请求哪种地址 startFront(env){\tif(env === 'prod'){\t\t//使用生成环境\t\t//不输出注释\t\t//修改请求地址\t}\tif(env === 'ex'){\t\t//使用开发环境\t}} 启动方式 开发环境：npm run start （本地启动，监听端口，自动更新） 线上环境：npm run build（由umi为我们提供的） 配置文件的不同 开发环境与生产环境 配置文件的不同 eg：config.dev.ts config.prod.ts(以umi框架举例) 是否选择静态化 5.后端多环境实战 主要适用于Spring Boot项目 配置文件的修改 添加不同的后缀名 application-prod.yml **作为生产环境版本 ** 添加相对应的配置文件 修改生产环境下数据库的配置 （等其他中间件也类似，例如缓存redis） 注意不同环境下的版本最好一致 生产环境下的版本如何在本地运行 将项目打包为jar包 （使用idea提供的maven可视化工具） 启动项目：命令行输入 java -jar + 环境变量(–spring.profiles.active = prod) 二、项目上线的前置条件前置条件 已经能够上线的项目源码 前端项目build成dist目录，后端项目打包成jar包 一台云服务服务器 市面上主流的云服务器都可以，以下将给出主流服务器的区别 腾讯云与阿里云的区别腾讯云： 价格实惠、独享带宽多、100%CPU 但是经常促销老机型，有时候性能跟不上 适合个人开发学习使用阿里云： 技术全面、综合能力强、机型崭新 但是会限制CPU而且续费价格昂贵 适合团队、整体开发生产三、宝塔Linux可视化界面部署 登陆宝塔Linux面板，可以使用云服务器自带的宝塔面板，也可以自己自行到官网进行下载，环境稳定版本以文中图片为主。1.前端环境配置点击软件商店安装nginx2.后端环境配置点击软件商店安装tomcat(主要是为了安装java，安装后记得将tomcat关闭避免占用8080端口)3.其他环境配置 一般根据自己的项目所使用到的技术栈进行配置 mysql配置 redis配置 rabbitMQ配置 等等以MySQL为例，点击软件商店选择对应的版本后进行下载即可，在这之后再将相关的数据库表数据上传到云服务器中4.运行项目点击网站模块进行部署项目前端项目 将前端构建好的项目上传到对应的目录，并在根目录绑定点击添加站点，输入对应的域名/端口号吗，填写好备注以及根目录，其余不用做修改，点击提交，项目则部署成功后端项目 将前端项目打包好的jar包上传到对应的目录，并在项目jar路径进行绑定点击网站的Java项目模块，根据要求选择要运行的jar包，再设置项目的运行端口号，项目的执行指令也可以自己进行修改(JVM的配置或者使用prod配置文件)，点击提交后端项目则部署成功 至此前后端项目已经最基本的部署已经成功了，在此基础之上还可以去去添加域名，对项目的业务进一步进行优化。四、绑定域名1.域名解析流程用户输入网址 =&gt; 域名解析服务器(把网址解析为ip地址(其对应的端口号服务)/交给其他的域名解析服务) =&gt; 服务接受请求，返回文件给前端 =&gt; 前端加载文件到浏览器(js、css)/后端返回对应的数据 进入域名的注册商 （DNSPod服务） 查询文档进行相关配置即可 记得修改nginx的域名管理2.ip地址与域名的区别 ip地址通常指定的是服务器，也就是主机，即我们在云服务厂商购买的轻量级云服务器 建立网站是需要域名和主机(云服务器)两者的 上线项目时可以将域名与服务器的ip地址的端口号进行绑定 域名可以理解为人的名字 ip地址可以理解为人的身份证号 我们与别人交流通常是通过叫对方的名字 而不是通过身份证号 但人必须有身份证号(即必须有自己的云服务器)" }, { "title": "HTML5 Web存储", "url": "/posts/WebStorage/", "categories": "技术科普", "tags": "学习", "date": "2022-12-04 03:13:00 +0000", "snippet": "HTML5 Web存储一、简介​\tHTML5 Web 存储（webStorage）是本地存储，存储在客户端，包括localStorage和SessionStorage。HTML5 Web 存储是以键/值对的形式存储的，通常以字符串存储二、localStorage​\tlocalStorage生命周期是永久，除非主动清除localStorage信息，否则这些信息将永远存在。存放数据大小为一般为5...", "content": "HTML5 Web存储一、简介​\tHTML5 Web 存储（webStorage）是本地存储，存储在客户端，包括localStorage和SessionStorage。HTML5 Web 存储是以键/值对的形式存储的，通常以字符串存储二、localStorage​\tlocalStorage生命周期是永久，除非主动清除localStorage信息，否则这些信息将永远存在。存放数据大小为一般为5MB,而且它仅在客户端（即浏览器）中保存，不参与和服务器的通信使用方法:// 1、保存数据到本地// 第一个参数是保存的变量名，第二个是赋给变量的值localStorage.setItem('Author', 'local'); // 2、从本地存储获取数据localStorage.getItem('Author'); // 3、从本地存储删除某个已保存的数据localStorage.removeItem('Author'); // 4、清除所有保存的数据localStorage.clear();三、sessionStorage​\tsessionStorage 属性允许你访问一个对应当前源的 session Storage 对象。它与 localStorage 相似，不同之处在于 localStorage 里面存储的数据没有过期时间设置，而存储在 sessionStorage 里面的数据在页面会话结束时会被清除 页面会话在浏览器打开期间一直保持，并且重新加载或恢复页面仍会保持原来的页面会话 在新标签或窗口打开一个页面时会复制顶级浏览会话的上下文作为新会话的上下文，这点和 session cookies 的运行方式不同 打开多个相同的URL的Tabs页面，会创建各自的sessionStorage 关闭对应浏览器窗口（Window）/ tab，会清除对应的sessionStorage使用方法:// 1、保存数据到本地// 第一个参数是保存的变量名，第二个是赋给变量的值sessionStorage.setItem('Author', 'YKFire'); // 2、从本地存储获取数据sessionStorage.getItem('Author'); // 3、从本地存储删除某个已保存的数据sessionStorage.removeItem('Author'); // 4、清除所有保存的数据sessionStorage.clear();四、复杂数据存储 以上示例展示的都是简单数据的存储，当要存储的数据是一个对象或数组的时候，直接存储是不行的错误代码：var user = { username: 'YKFire', password: '123456' };sessionStorage.setItem('user', user);console.log(sessionStorage.getItem('user'));因此我们在存取数据的时候需要转换格式： 存储数据前：利用JSON.stringify将对象转换成字符串 获取数据后：利用JSON.parse将字符串转换成对象正确代码：var user = { username: 'YKFire', password: '123456'};user = JSON.stringify(user);sessionStorage.setItem('user', user); var account = sessionStorage.getItem('user');console.log(account); account = JSON.parse(account)console.log(account);五、跨页面传值的方式 此部分讲解的是客户端中页面与页面的传值方式，切忌与客户端与服务端之间的传值方式搞混1、通过url传值上一个页面发送：location.href=\"跨页面1-2.html?age=18&amp;gender=man\";下一个页面接受：//1、location.search获取get请求的参数 获取到的数据，是以?开头的var search=location.search;//2、如果还想要获取确定的数据，可以解析字符串function parse(search){ //从第二个字符开始截取 ，获取到第二个开始后面所有的字符 var str=search.substring(1); var result={}; //分割字符串 --&gt;产生字符串数组 var strs=str.split(\"&amp;\"); //遍历数组中的每一个元素 strs.forEach(function(v){ //伪代码：v=\"age=18\" var keyvalue=v.split(\"=\"); var name=keyvalue[0]; var value=keyvalue[1]; result[name]=value; }) return result;} var r=parse(search);2、使用WebStorage传值 localStroage和sessionStorage使用大致相同，他们的不同之处在于，localstroage是永久保存，而sessionstroage是会话存在，当会话结束，sessionstroage保存值也会清空存储对象的正确的方式：var user={name:\"YKFire\",age:16};var user1=JSON.stringify(user1); //将对象\"序列化\"为JSON数据(字符串格式)localStorage.setItem(\"user\",user1); //以字符串格式存储信息var a=localStorage.getItem(\"user\"); //获取存储的信息，也是字符串格式var b=JSON.parse(a); //将JSON数据反序列化为对象3、使用cookie保存//1、保存一条数据document.cookie=\"name=abc\";document.cookie=\"age=18\";//2、获取所有数据var cookie=document.cookie;console.log(cookie); //\"name=abc; age=18; PHPSESSID=fr1njdv6apf3neoj5nehntrps7\"//之后可以解析字符串，获取指定的数据内容//3、设置cookie的有效期document.cookie=\"id=666;expires=\"+new Date(\"2017-10-22 08:00\");六、cookie存储与Web存储的区别与应用场景1、cookie与session1.简介​\tcookie和session都是用来跟踪浏览器用户身份的会话方式2.区别保持状态​\tcookie保存在浏览器端，session保存在服务器端使用方式cookie机制： ​\t如果不在浏览器中设置过期时间，cookie被保存在内存中，生命周期随浏览器的关闭而结束，这种cookie简称会话cookie。如果在浏览器中设置了cookie的过期时间，cookie被保存在硬盘中，关闭浏览器后，cookie数据仍然存在，直到过期时间结束才消失 ​\tCookie是服务器发给客户端的特殊信息，cookie是以文本的方式保存在客户端，每次请求时都带上它session机制： ​\t当服务器收到请求需要创建session对象时，首先会检查客户端请求中是否包含sessionid。如果有sessionid，服务器将根据该id返回对应session对象。如果客户端请求中没有sessionid，服务器会创建新的session对象，并把sessionid在本次响应中返回给客户端。通常使用cookie方式存储sessionid到客户端，在交互中浏览器按照规则将sessionid发送给服务器。如果用户禁用cookie，则要使用URL重写，可以通过response.encodeURL(url) 进行实现；API对encodeURL的结束为，当浏览器支持Cookie时，url不做任何处理；当浏览器不支持Cookie的时候，将会重写URL将SessionID拼接到访问地址后存储内容​\tcookie只能保存字符串类型，以文本的方式；session通过类似与Hashtable的数据结构来保存，能支持任何类型的对象(session中可含有多个对象)存储大小​\t单个cookie保存的数据不能超过4kb；session大小没有限制安全性​\tsession的安全性大于cookie，针对cookie所存在的攻击：Cookie欺骗，Cookie截获原因： sessionID存储在cookie中，若要攻破session首先要攻破cookie sessionID是要有人登录，或者启动session_start才会有，所以攻破cookie也不一定能得到sessionID 第二次启动session_start后，前一次的sessionID就是失效了，session过期后，sessionID也随之失效 sessionID是加密的3.应用场景cookie： 判断用户是否登陆过网站，以便下次登录时能够实现自动登录（或者记住密码）。如果我们删除cookie，则每次登录必须从新填写登录的相关信息 保存上次登录的时间等信息 保存上次查看的页面 浏览计数session： Session用于保存每个用户的专用信息，变量的值保存在服务器端，通过SessionID来区分不同的客户 网上商城中的购物车 将某些数据放入session中，供同一用户的不同页面使用4.缺点cookie： 大小受限 用户可以操作（禁用）cookie，使功能受限 安全性较低 有些状态不可能保存在客户端 每次访问都要传送cookie给服务器，浪费带宽 cookie数据有路径（path）的概念，可以限制cookie只属于某个路径下session： Session保存的东西越多，就越占用服务器内存，对于用户在线人数较多的网站，服务器的内存压力会比较大 依赖于cookie（sessionID保存在cookie），如果禁用cookie，则要使用URL重写，不安全 创建Session变量有很大的随意性，可随时调用，不需要开发者做精确地处理，所以，过度使用session变量将会导致代码不可读而且不好维护2、localStorage与sessionStorage1.简介 WebStorage的目的是克服由cookie所带来的一些限制，当数据需要被严格控制在客户端时，不需要持续的将数据发回服务器。 WebStorage两个主要目标：（1）提供一种在cookie之外存储会话数据的路径。（2）提供一种存储大量可以跨会话存在的数据的机制。 HTML5的WebStorage提供了两种API：localStorage（本地存储）和sessionStorage（会话存储）2.区别生命周期localStorage: 其生命周期是永久的，关闭页面或浏览器之后localStorage中的数据也不会消失。localStorage除非主动删除数据，否则数据永远不会消失。sessionStorage： 其生命周期是在仅在当前会话下有效；sessionStorage引入了一个“浏览器窗口”的概念，sessionStorage是在同源的窗口中始终存在的数据。只要这个浏览器窗口没有关闭，即使刷新页面或者进入同源另一个页面，数据依然存在。但是sessionStorage在关闭了浏览器窗口后就会被销毁。同时独立的打开同一个窗口同一个页面，sessionStorage也是不一样的。存储大小​\tlocalStorage和sessionStorage的存储数据大小一般都是：5MB存储位置​\tlocalStorage和sessionStorage都保存在客户端，不与服务器进行交互通信存储内容类型​\tlocalStorage和sessionStorage只能存储字符串类型，对于复杂的对象可以使用ECMAScript提供的JSON对象的stringify和parse来处理应用场景localStoragese：​\t常用于长期登录（+判断用户是否已登录），适合长期保存在本地的数据sessionStorage：​\t敏感账号一次性登录3.优点 存储空间更大：cookie为4KB，而WebStorage是5MB 省网络流量：WebStorage不会传送到服务器，存储在本地的数据可以直接获取，也不会像cookie一样每次请求都会传送到服务器，所以减少了客户端和服务器端的交互，节省了网络流量 对于那种只需要在用户浏览一组页面期间保存而关闭浏览器后就可以丢弃的数据，sessionStorage会非常方便 快速显示：获取数据时可以从本地获取会比从服务器端获取快得多，所以速度更快 安全性：WebStorage不会随着HTTP header发送到服务器端，所以安全性相对于cookie来说比较高一些，不会担心截获，但是仍然存在伪造问题 WebStorage提供的数据操作方法比cookie方便" }, { "title": "Java DTO类与VO类的区别", "url": "/posts/JavaDTOVO/", "categories": "技术科普", "tags": "学习", "date": "2022-11-27 02:53:00 +0000", "snippet": "Java DTO类与VO类的区别一、联系​\tDTO类与VO类都是在java后端开发过程中用来接收数据和返回数据的类二、区别1、VO类 VO代表展示层需要显示的数据​\tVO即View Object表示层对象，主要体现在视图的对象，对于一个WEB页面将整个页面的属性封装成一个对象。然后用一个VO对象将在控制层得到的数据传输到视图层。2、DTO类 DTO代表服务层需要接收的数据和返回的数据​\t...", "content": "Java DTO类与VO类的区别一、联系​\tDTO类与VO类都是在java后端开发过程中用来接收数据和返回数据的类二、区别1、VO类 VO代表展示层需要显示的数据​\tVO即View Object表示层对象，主要体现在视图的对象，对于一个WEB页面将整个页面的属性封装成一个对象。然后用一个VO对象将在控制层得到的数据传输到视图层。2、DTO类 DTO代表服务层需要接收的数据和返回的数据​\tDTO即Data Transfer Object数据传输对象，用于后端接收前端传递过来的参数，并进行业务逻辑操作，有些接收参数要用对象来接收，但是发现哪个Entity都不合适，就有了DTO；是经过处理后的PO，可能增加或减少了PO的属性。3、其他相关的概念 POJO：Plain Ordinary Java Object无规则简单Java对象，可以转化为VO、DTO、PO PO：Persistent Object持久化对象，它跟数据表形成一一对应的映射关系 Entity：实体，和PO的功能类似，和数据表一一对应，一个实体一张表三、应用场景完整的订单PO类 与一张数据表形成对应关系/** * 订单表 * @TableName orders */@TableName(value =\"orders\")@Datapublic class Orders implements Serializable { /** * 订单id */ @TableId(type = IdType.AUTO) private Long id; /** * 订单发起者id */ private Long launchId; /** * 收货人电话 */ private String conPhone; /** * 收货人名称 */ private String consignee; /** * 订单接受者id */ private Long acceptId; /** * 快递员电话 */ private String couPhone; /** * 快递员姓名 */ private String courier; /** * 产品id */ private Long productId; /** * 订单价格 */ private Integer price; /** * 备注信息 */ private String remarks; /** * 地址信息 */ private String address; /** * 订单状态(0-已取消, 1-待接单,2-待确认,3-已接单, 4-订单完成) */ private Integer orderStatus; /** * 创建时间 */ private Date createTime; /** * 更新时间 */ private Date updateTime; /** * 是否删除(0-未删, 1-已删) */ @TableLogic private Integer isDeleted; @TableField(exist = false) private static final long serialVersionUID = 1L;}1、VO类的应用场景​\t假设我们在一个订单管理系统中，当前端发送获取订单具体信息的请求，我们可以将搜索到的数据记录装载到PO类中进行返回，但往往前端的数据只需要一部分的数据信息即可，而且如果直接将PO类进行返回还会泄露系统中对应表的数据库表字段，会造成安全性问题，因此我们可以创建VO类装载只需要的信息进行返回@Datapublic class OrdersDetailsVO { /** * 收货人名称 */ private String consignee; /** * 收货人电话 */ private String conPhone; /** * 订单价格 */ private Integer price; /** * 地址信息 */ private String address; /** * 备注信息 */ private String remarks; /** * 订单状态(0-已取消, 1-待接单,2-待确认,3-已接单, 4-订单完成) */ private Integer orderStatus; //------------------------------------------ /** * 产品名称 */ private String name; /** * 产品图片 */ private String image;}控制层使用VO类向前端返回数据：\t/** * 首页分页获取订单信息接口 * * @param id * @param page * @param pageSize * @return R&lt;PageVO&gt; */ @GetMapping(\"/homeList/{id}\") public R&lt;PageVO&gt; homeList(@PathVariable Long id, int page, int pageSize) { PageVO pageVO = ordersService.getHomeList(id, page, pageSize); return R.success(pageVO); }2、DTO类的应用场景​\t同样在一个订单管理系统中，当前端传递过来需要添加的相应订单信息，后端可以使用对应的对象来进行接受，但实际前端传递过来的信息仅仅只有一部分，如果直接使用PO类来接受，便会造成浪费，因而我们可以创建对应的DTO类进行接受@Datapublic class AddOrderDto { /** * 序列化ID */ private static final long serialVersionUID = 1L; /** * 收货人电话 */ private String conPhone; /** * 收货人名称 */ private String consignee; /** * 订单价格 */ private Integer price; /** * 备注信息 */ private String remarks; /** * 地址信息 */ private String address; /** * 产品名称 */ private String name; /** * 产品图片 */ private String image;}控制层使用DTO类进行接受参数：\t/** * 发布订单接口 * * @param addOrderDto * @return R&lt;String&gt; */ @PostMapping(\"/add/{id}\") private R&lt;String&gt; add(@PathVariable Long id, @RequestBody AddOrderDto addOrderDto) { return ordersService.addOrder(id, addOrderDto); } 被@RequestBody 修饰的请求参数必须为实体类，且实体类必须实现序列化接口，如果不序列化就会出错，一般的参数不需要添加@RequestBody" }, { "title": "KnowWhy(算法学习为例)", "url": "/posts/KnowWhy/", "categories": "随笔", "tags": "学习", "date": "2022-11-20 14:35:00 +0000", "snippet": "KnowWhy(算法学习为例) 最近在系统地学习数据结构与算法，经过一段时间的沉淀，算是对其有了一定的见解。本文包括笔者这段时间的学习心得、思考以及方法。本文所讲到的学习方法、思维方式是通用的，这里只是以算法学习为例一、解题思维1、联想​\t当我们拿到一条的算法题时，我们的大脑底层便自动搜索之前做过与之类似的题目，我们暂且把它称为“联想”。联想是一种强大的思维捷径，在任何时候都会抢占大脑的工...", "content": "KnowWhy(算法学习为例) 最近在系统地学习数据结构与算法，经过一段时间的沉淀，算是对其有了一定的见解。本文包括笔者这段时间的学习心得、思考以及方法。本文所讲到的学习方法、思维方式是通用的，这里只是以算法学习为例一、解题思维1、联想​\t当我们拿到一条的算法题时，我们的大脑底层便自动搜索之前做过与之类似的题目，我们暂且把它称为“联想”。联想是一种强大的思维捷径，在任何时候都会抢占大脑的工作记忆，且这是自发运行的，由不得我们控制。这种思维某种程度上可以说是“混乱”的（虽然从一个更根本的层面上说是有规则的），所谓混乱是指很多时候并不确定联想到的做法最终是否可行，这些联想也许只是基于题目中的某个词语、语法结构、问题的某个切片、一些零星局部的信息。这个过程是试探性的。最后也许有很大一部分被证明是不可行的。很多时候我们解决问题用的都是这种思维，简言之就是首先枚举你关于这个问题能够想到的所有你学过的知识，然后一一往上套看看能否解决手头的问题。这种思维方式受限于人脑联想能力本身的局限性。​\t换言之，这就是我们经常采用的“题海战术”，只要我们的刷的题目够多，我的大脑从总能从零星的记忆碎片找到与之类似的题目并类比解决。2、演绎&amp;归纳​\t而“演绎&amp;归纳”是另一种思维方式，它是一种“必然”的推理，却并不“必然”地指向算法问题的结论，它也是试错的，只不过是比联想更加靠谱。它们远比联想有根据。其中演绎是严格的，必然的。归纳也是有一定根据的。在面对一个问题的时候，我们有意无意的对问题中的各个条件进行着演绎；譬如福尔摩斯著名的“狗叫”推理——狗+生人=&gt;吠叫 &amp; 昨晚狗没有叫 =&gt; 那个人是熟人。就是一个典型的对问题的各个条件进行演绎的推理过程。还有就是通过对一些特殊形式的观察来进行归纳，试图总结问题中的规律。然而，不幸的是，面对复杂的问题，演绎&amp;归纳也并不总是“直奔”问题的解决方案的。人的思维毕竟只能一下子看到有限的几步逻辑结论，一条逻辑演绎路径是否直奔答案，不走到最后往往是不知道的，只要答案还未出现，我们大脑中的逻辑演绎之树的末端就始终隐藏在黑暗之中。而当最终答案出现了之后，我们会发现，这棵演绎之树的很多分支实际上都并不通往答案。​\t但这并不代表逻辑演绎之树的其他分支推导出来的结果并没有意义，虽然对解决这道具体的“算法题”来说并没有意义，但在这一过程确确实实锻炼了我们的逻辑推导能力。更加具体的表现形式为：当我们以后遇到同样类似的题目，即便我们的大脑的“联想”功能并不能解决问题，我们也很快重新“演绎&amp;归纳”出正确的答案，而这次过程往往会比第一次推导快上许多倍。3、两者的使用​\t对于在硬试考核的情况下，如考试、竞赛、面试等等，“联想”的思维方式确实帮助我们更快地解决这道问题，而且在这种情况下，紧迫的时间往往无法支撑我们顺其逻辑地进行“演绎&amp;归纳”。换言之，当我们在进行硬试考核，“联想”确实是我们主要的思维方式与解题手段，但这不代表“演绎&amp;归纳”这一方式并没有用，在我们平时的算法学习，养成优先从问题的本质入手进行考察的好习惯，在进行“演绎&amp;归纳”的思考，能帮助我们对同类知识有更加深刻，也能帮助我们的“联想”速度更加的迅速。二、问题的转化 以算法题目为例：将问题的内容使用数据结构进行表示，在使用算法方法进行求解​\t学会将问题转化成另一种易于解决的方式或者抽象于一种状态，也是解题能力的重要体现之一eg：​\t一只猩猩想要过河 ———联想：曾经从横在河上的树木上走过———-&gt; 找到一颗横在河上面的树木 ——— 并没有，但发现周围有着不少的树木 ———-&gt; 如何将这些树木躺在河上面​\t从而猩猩从“过河的问题” 就转换成了 “如何将树木躺在河上面” 的问题，当然这个问题可以继续转化为更加精确的子问题，这里只是举例，便不一一赘述了。三、解决问题的思考方法 以下思考方法出自于《How To Solve It》中的启发式思考方法 时刻不忘记未知量 想要什么 即问题是什么 用特例启发思考 泛化的关系有不确定 使用某个特例进行求解 反过来推导 结论往往蕴含着丰富的条件，譬如对什么样的解才是满足题意的解的约束。一般来说，借助结论中蕴含的知识，我们便可以更为“智能地”搜索解空间,即学会减枝 笛卡尔的万能解题法就是首先将问题转化为代数问题，然后设出未知数，列出方程，最后解这组（个）方程。其中设未知数本质上就是一种倒推：通过设出一个假想的结论x，来将题目对x的需求表达出来，然后顺势而下推导出x。仔细想想设未知数这种手法所蕴含的深刻思想，也就难怪笛卡尔会认为它是那个解决所有问题的一般性钥匙了。 试错 你拿到一个题目，里面有一些条件，你需要求解一个未知量。于是你对题目这里捅捅那里捣捣，你用上所有的已知量，或使用所有你想到的操作手法，尝试着看看能不能得到有用的结论，能不能离答案近一步。 调整题目的条件 有时候，通过调整题目的条件，我们往往迅速能够发现条件和结论之间是如何联系的。通过扭曲问题的内部结构，我们能发现原本结构里面重要的东西 求解一个类似的问题 即我们上文所提到的“联想” 通过考察或回忆一个类似的题目是如何解决的，也许就能够借用一些重要的点子 列出所有可能跟问题有关的定理或性质 考察反面，考察其他所有情况 将问题泛化，并求解这个泛化后的问题 刚才不是说过，应该通过特例启发思考吗？为什么现在又反倒要泛化呢？实际上，有少数题目，泛化之后更容易解决。即，解决一类问题，比解决这类问题里面某个特定的问题还要容易。 下意识孵化法 我们先把问题的吃透，放在脑子里，然后等着我们的下意识把它解出来。不过，不宜将这个方法的条件拉伸过远，实际上，除非能够一直保持一种思索的状态（金出武雄所谓“思维体力”），或者问题很简单，否则一转头去做别的事情之后，你的下意识很容易就把问题丢开了。 烫手山芋法 说白了，就是把问题扔给别人解决,人生有限，我们不可能将面临的问题都一一解决，学会“求助”他人 四、知识的抽象​\t我们的大脑无时无刻不在对事物进行归类，实际上，不仅是事物，一切知识，都在被自动的归类。在有关对世界的认知方面，被称为认知图式，我们根据既有的知识结构来理解这个世界，会带来很大的优势。实际上，模块化是一个重要的降低复杂性的手段。然而，知识是一把双刃剑，一方面，它们提供给了我们解决问题的无以伦比的捷径优势，“砖头是砌墙的”，于是我们遇到砌墙这个问题的时候就可以迅速利用砖头。然而另一方面，知识却也是思维的桎梏，思维定势就是指下意识遵循既有知识框架思考的过程，每一个知识都是一个优势，同时又是一个束缚。​\t因此为了解决此问题，就要引出本文最重要的一种思维方式：“抽象&amp;总结”，其实就是人们常说的温故而知新，当我们通过“联想”或者“演绎&amp;归纳”来加深知识印象或者新知识，我们都需要时不时地停下来，进行“抽象&amp;总结”，用于更深刻地理解巩固已用的知识，泛化出更深层次的本质，形成属于我们自己的知识体系，避免被知识本身所束缚住了。" }, { "title": "数据结构--栈", "url": "/posts/Stcak/", "categories": "数据结构与算法", "tags": "学习", "date": "2022-11-12 11:15:00 +0000", "snippet": "数据结构–栈一、栈简介​\t栈（Stack）是一种线性数据结构，它就像一个单通道的容器，其中的元素只能先进后出（First In Last Out，简称 FILO）。由于其是单通道的，所以决定了最早进入容器的元素只能最晚出。最早进入的元素存放的位置叫做栈底，最后进入的元素存放的位置叫做栈顶。二、栈的实现 栈既可以用数组进行实现，也可以使用链表来实现栈的数组实现：栈的链表实现：三、栈的基本操作...", "content": "数据结构–栈一、栈简介​\t栈（Stack）是一种线性数据结构，它就像一个单通道的容器，其中的元素只能先进后出（First In Last Out，简称 FILO）。由于其是单通道的，所以决定了最早进入容器的元素只能最晚出。最早进入的元素存放的位置叫做栈底，最后进入的元素存放的位置叫做栈顶。二、栈的实现 栈既可以用数组进行实现，也可以使用链表来实现栈的数组实现：栈的链表实现：三、栈的基本操作 栈的基本操作包括入栈和出栈两种 入栈： stack.push() 出栈：stack.peek() 获取栈顶元素但不弹出1、入栈​\t入栈操作（push）就是把新元素放入栈中，只允许从栈顶一侧放入元素，新元素的位置将会称为新的栈顶2、出栈​\t出栈操作（pop），也叫弹栈操作，就是把元素从栈中弹出，只有栈顶元素才允许出栈，出栈之后，出栈元素的前一个元素将会成为新的栈顶元素3、最小栈 设计一个支持 push ，pop ，top 操作，并能在常数时间内检索到最小元素的栈。 实现 MinStack 类: MinStack() 初始化堆栈对象。void push(int val) 将元素val推入堆栈。void pop() 删除堆栈顶部的元素。int top() 获取堆栈顶部的元素。int getMin() 获取堆栈中的最小元素。解题思路： 创建一个辅助栈MinStack用于记录当前元素栈内的最小值 且辅助栈与元素栈同步进行插入、删除操作代码实现：class MinStack { //创建元素栈与最小栈 Deque&lt;Integer&gt; stack; Deque&lt;Integer&gt; minStcak; //初始化最小栈 public MinStack() { stack = new LinkedList&lt;Integer&gt;(); minStcak = new LinkedList&lt;Integer&gt;(); minStcak.push(Integer.MAX_VALUE); } public void push(int val) { stack.push(val); minStcak.push(Math.min(val,minStcak.peek())); //存放最小值 } public void pop() { stack.pop(); minStcak.pop(); } public int top() { return stack.peek(); } public int getMin() { return minStcak.peek(); }}/** * Your MinStack object will be instantiated and called as such: * MinStack obj = new MinStack(); * obj.push(val); * obj.pop(); * int param_3 = obj.top(); * int param_4 = obj.getMin(); */4、用栈实现队列 请你仅使用两个栈实现先入先出队列。队列应当支持一般队列支持的所有操作（push、pop、peek、empty）： 实现 MyQueue 类： void push(int x) 将元素 x 推到队列的末尾int pop() 从队列的开头移除并返回元素int peek() 返回队列开头的元素boolean empty() 如果队列为空，返回 true ；否则，返回 false解题思路： 创建一个inStack栈用于存放元素，另一个outStack栈用于取出元素 当取出元素时(pop、peek)记得先判断outStack栈是否为空 单独创建将inStack栈中的元素转移到outStack栈中的方法：in2out();代码实现：class MyQueue { Deque&lt;Integer&gt; inStack; //存放元素的栈 Deque&lt;Integer&gt; outStack; //取元素的栈 //初始化栈 public MyQueue() { inStack = new ArrayDeque&lt;Integer&gt;(); outStack = new ArrayDeque&lt;Integer&gt;(); } public void push(int x) { inStack.push(x); } public int pop() { if(outStack.isEmpty()){ in2out(); } return outStack.pop(); } public int peek() { if(outStack.isEmpty()){ in2out(); } return outStack.peek(); } public boolean empty() { return inStack.isEmpty() &amp;&amp; outStack.isEmpty(); } //将inStack栈中元素存放在outStack栈中 public void in2out(){ while(!inStack.isEmpty()){ outStack.push(inStack.pop()); } }}/** * Your MyQueue object will be instantiated and called as such: * MyQueue obj = new MyQueue(); * obj.push(x); * int param_2 = obj.pop(); * int param_3 = obj.peek(); * boolean param_4 = obj.empty(); */5、栈的定义//在实际使用中,通常使用双端队列Deque来实现栈Deque&lt;T&gt; stack = new LinkedList&lt;T&gt;();四、栈与深度优先搜索1、经典问题1.有效的括号 给定一个只包括 ‘(‘，’)’，’{‘，’}’，’[‘，’]’ 的字符串 s ，判断字符串是否有效。 有效字符串需满足： 左括号必须用相同类型的右括号闭合。左括号必须以正确的顺序闭合。每个右括号都有一个对应的相同类型的左括号。解题思路： 首先判断字符串的长度是否为偶数，不为偶数则代表此字符串无效 创建Map集合用于记录相对应的键值对，即对应的左括号与右括号，便于进行判断 创建辅助栈，当遍历字符串时元素为左括号，则将字符压入栈中 当遍历字符串时元素为右括号，则进行判断（栈为空或栈顶元素与右括号不匹配则代表此字符串无效） 最终根据栈是否为空来判断字符串是否有效代码实现：class Solution { public boolean isValid(String s) { //获取字符串的长度 int len = s.length(); //当字符串长度为奇数的情况 if(len % 2 == 1){ return false; } //创建Map集合用于记录相对应的键值对 由于博客部署出现问题,代码部分缺失 Map&lt;Character,Character&gt; map = new HashMap&lt;Character,Character&gt;(); //创建数据结构栈用于存储左括号 Deque&lt;Character&gt; stack = new LinkedList&lt;Character&gt;(); //循环遍历字符串进行判断 for(int i = 0; i &lt; len; i++){ char ch = s.charAt(i); if(map.containsKey(ch)){ //当前字符为右括号时 if(stack.isEmpty() || stack.peek() != map.get(ch)){//当左右括号不匹配时 return false; } stack.pop(); } else{ //当前字符为左括号时 stack.push(ch); } } return stack.isEmpty(); }}2.每日温度 给定一个整数数组 temperatures ，表示每天的温度，返回一个数组 answer ，其中 answer[i] 是指对于第 i 天，下一个更高温度出现在几天后。如果气温在这之后都不会升高，请在该位置用 0 来代替。 1 &lt;= temperatures.length &lt;= 105 30 &lt;= temperatures[i] &lt;= 100 解题思路： 创建栈用于存储元素下标，用于表示尚未找到比此温度低的下标 遍历数组的每个元素，遍历栈判断当前元素对应的温度是否比栈顶元素对应的温度大，成立则弹出栈顶元素prevIndex，并且answer[prevIndex] = i - prevIndex 不成立的话将该元素压入栈中 最终返回结果answer数组代码实现：class Solution { public int[] dailyTemperatures(int[] temperatures) { //获取温度数组的长度 int length = temperatures.length; //创建结果数组 int[] answer = new int[length]; //不进行设置数组元素默认为零 //创建数据结构栈进行辅助 Deque&lt;Integer&gt; stack = new LinkedList&lt;Integer&gt;(); //循环数组进行比较 for(int i = 0; i &lt; length; ++i){ int temperature = temperatures[i]; //判断当前温度与栈中存储温度的大小 while(!stack.isEmpty() &amp;&amp; temperature &gt; temperatures[stack.peek()]){ //注意为遍历栈 //获取栈顶元素并弹出 int prevIndex = stack.pop(); answer[prevIndex] = i - prevIndex; } stack.push(i); } return answer; }}3.逆波兰表达式求值 根据 逆波兰表示法，求表达式的值。 有效的算符包括 +、-、*、/ 。每个运算对象可以是整数，也可以是另一个逆波兰表达式。 注意 两个整数之间的除法只保留整数部分。 可以保证给定的逆波兰表达式总是有效的。换句话说，表达式总会得出有效数值且不存在除数为 0 的情况。 1 &lt;= tokens.length &lt;= 104tokens[i] 是一个算符（”+”、”-“、”*” 或 “/”），或是在范围 [-200, 200] 内的一个整数解题思路： 遍历字符串，当元素为数字时，将其压入栈中，当元素为运算符，弹出栈顶的两个元素进行运算后重新压入栈中 难点在于数据类型之间的转换 Integer.parseInt(s)：将字符串s转换为整数类型 Integer.valueOf(c)：将字符c转化为整数类型 代码实现：class Solution { public int evalRPN(String[] tokens) { //获取字符串的长度 int len = tokens.length; //创建栈进行辅助 Deque&lt;Integer&gt; stack = new LinkedList&lt;Integer&gt;(); //循环字符串 for(int i = 0; i &lt; len; ++i){ //获取当前字符 String token = tokens[i]; if(!(\"+\".equals(token) || \"-\".equals(token) || \"*\".equals(token) || \"/\".equals(token))){ stack.push(Integer.parseInt(token)); } else{ //弹出数字进行运算 int a = stack.pop(); int b = stack.pop(); int result = operation(a,b,token); stack.push(result); } } //返回结果 return stack.peek(); } //创建运算函数方便进行运算 public int operation(int a, int b, String token){ //结果变量 int result = 0; switch(token){ case \"+\": result = a + b; break; case \"-\": result = b - a; break; case \"*\": result = a * b; break; case \"/\": result = b / a; break; } return result; }} 逆波兰表达式是一种后缀表达式，所谓后缀就是指算符写在后面 平常使用的算式则是一种中缀表达式，如 ( 1 + 2 ) * ( 3 + 4 ) 该算式的逆波兰表达式写法为 ( ( 1 2 + ) ( 3 4 + ) * ) 逆波兰表达式主要有以下两个优点： 去掉括号后表达式无歧义，上式即便写成 1 2 + 3 4 + * 也可以依据次序计算出正确结果 适合用栈操作运算：遇到数字则入栈；遇到算符则取出栈顶两个数字进行计算，并将结果压入栈中 4.字符串解码 给定一个经过编码的字符串，返回它解码后的字符串。 编码规则为: k[encoded_string]，表示其中方括号内部的 encoded_string 正好重复 k 次。注意 k 保证为正整数。 你可以认为输入字符串总是有效的；输入字符串中没有额外的空格，且输入的方括号总是符合格式要求的。 此外，你可以认为原始数据不包含数字，所有的数字只表示重复的次数 k ，例如不会出现像 3a 或 2[4] 的输入。 1 &lt;= s.length &lt;= 30 s 由小写英文字母、数字和方括号 ‘[]’ 组成 s 保证是一个 有效 的输入。 s 中所有整数的取值范围为 [1, 300] 解题思路： 本文采用栈进行辅助求解，同时使用不定长数组来模拟栈操作，方便从栈底向栈顶遍历、 LinkedList&lt;String&gt; stk = new LinkedList&lt;String&gt;();stk.addLast()：压入元素stk.peekLast()：返回栈顶元素不弹出stk.removeLast()：返回栈顶元素并弹出 遍历字符串，当当前字符为“数字”时，则解析出一个数字字符串(有可能为多位数)进栈 Character.isDigit(c) //判断当前字符c是否为'数字'，即'2' 当当前字符为普通字母和左括号’[‘时,直接进栈 Character.isLetter(c) //判断当前字符是否为普通字母 即'a' 当前字符为右括号’]’时，开始出栈，直至左括号’[‘出栈，此时取出栈顶的数字，根据这个数字与弹出的元素构造成一个新的字符串压入栈中 本题的难点在于各种类型之间的转换 ** 先统一栈的类型为String类型** String.valueOf(c) //将字符类型转换为字符串类型 String类型不可变，操作字符串时要使用StringBuffer进行操作 代码实现：class Solution { //创建全局指针,用于遍历字符串 int ptr; public String decodeString(String s) { //使用不定长数组来模拟栈操作，方便从栈底向栈顶遍历 LinkedList&lt;String&gt; stk = new LinkedList&lt;String&gt;(); ptr = 0; //遍历字符串 while(ptr &lt; s.length()){ //获取当前指针指向的字符 char cur = s.charAt(ptr); if(Character.isDigit(cur)){ //当字符为数字时 //获取一个数字并进栈 String digits = getDigits(s); stk.addLast(digits); } else if(Character.isLetter(cur) || cur == '['){ //当当前字符为 普通字母 或 '[' 时 //将该字符进栈 stk.addLast(String.valueOf(s.charAt(ptr++))); } else{ //当前字符为']' 弹出栈中的元素进行解码 构建成一个新的字符串装入栈中 ptr++; //创建一个临时的栈用于接受弹出的元素 LinkedList&lt;String&gt; sub = new LinkedList&lt;String&gt;(); while(!\"[\".equals(stk.peekLast())){ //弹出\"[\"之前的全部元素 //接受弹出的元素 sub.addLast(stk.removeLast()); } //翻转临时栈内的元素 Collections.reverse(sub); //弹出左括号 '[' stk.removeLast(); //此时栈顶为当前sub对应的字符串应该出现的次数 int repTime = Integer.parseInt(stk.removeLast()); //获取当前字符串要重复的次数 //创建一个StringBuffer 用于接受当前 k[]解码后的字符串 StringBuffer t = new StringBuffer(); String o = getString(sub); //将临时栈内的全部元素转化为一个字符串 //构造字符串 即k[] while(repTime-- &gt; 0){ //重复k次 t.append(o); } //将构造好的字符串重新压入栈 因为可能会出现重复的情况 3[a2[c]] stk.addLast(t.toString()); } } return getString(stk); } //获取字符为数字的方法 public String getDigits(String s){ StringBuffer ret = new StringBuffer(); while(Character.isDigit(s.charAt(ptr))){//k可能为多位数 ret.append(s.charAt(ptr++)); } return ret.toString(); } //将当前集合内的单个字符串转化为一个整体字符串 public String getString(LinkedList&lt;String&gt; v){ StringBuffer ret = new StringBuffer(); for(String s: v){ ret.append(s); } return ret.toString(); }}2、深度优先搜索1.简介 ​\t深度优先搜索是将当前状态按照一定的规则顺序，先拓展一步得到一个新状态，再对这个新状态递归拓展下去。如果无法拓展,则退回一步到上一个状态，再按照原先设定的规则顺序重新寻找一个状态拓展。如此搜索,直至找到目标状态,或者遍历完所有状态。所以,深度优先搜索也是一种“盲目”搜索 ​\t通常使用递归进行实现深度优先搜索，递归会使用到系统提供的隐式栈，也称为调用栈，但如果递归的深度太深，将会导致栈溢出，这种情况下需要使用“显示栈”2.岛屿数量 给你一个由 ‘1’（陆地）和 ‘0’（水）组成的的二维网格，请你计算网格中岛屿的数量。 岛屿总是被水包围，并且每座岛屿只能由水平方向和/或竖直方向上相邻的陆地连接形成。 此外，你可以假设该网格的四条边均被水包围。 m == grid.length n == grid[i].length 1 &lt;= m, n &lt;= 300 grid[i][j] 的值为 '0' 或 '1' 解题思路： 本文采用深度优先搜索的思想进行求解 递归实现 遍历二维数组，当遇到“1”时，岛屿数量+1 以此为起点开始进行深度优先搜索(上下左右递归)，每遇到一个1重新标记为0，终止条件为： if(r &lt; 0 || c &lt; 0 || r &gt;= nr || c &gt;= nc || grid[r][c] == '0') 最终深度优先搜索的次数即为岛屿数量 代码实现：class Solution { public int numIslands(char[][] grid) { //判断二维数组为空的情况 if(grid == null &amp;&amp; grid.length == 0){ return -1; } //获取二维数组的行和列的个数 int nr = grid.length; int nc = grid[0].length; //初始化岛屿的数量 int num_islands = 0; //遍历二维数组 for(int r = 0; r &lt; nr; ++r){ for(int c = 0; c &lt; nc; ++c){ if(grid[r][c] == '1'){ //岛屿数量+1 num_islands++; //进行深度优先搜索 即递归 dfs(r,c,grid); } } } return num_islands; } //创建深度优先搜索的方法 public void dfs(int r, int c, char[][] grid){ //获取二维数组的行和列的个数 int nr = grid.length; int nc = grid[0].length; //终止条件 if(r &lt; 0 || c &lt; 0 || r &gt;= nr || c &gt;= nc || grid[r][c] == '0'){ return; } //将当前值修改为0 grid[r][c] = '0'; //按顺序 上下左右 进行深度优先搜索 dfs(r-1,c,grid); dfs(r+1,c,grid); dfs(r,c-1,grid); dfs(r,c+1,grid); }}4.克隆图 给你无向 连通 图中一个节点的引用，请你返回该图的 深拷贝（克隆）。 图中的每个节点都包含它的值 val（int） 和其邻居的列表（list[Node]） class Node { public int val; public List&lt;Node&gt; neighbors;} 简单起见，每个节点的值都和它的索引相同。例如，第一个节点值为 1（val = 1），第二个节点值为 2（val = 2），以此类推。该图在测试用例中使用邻接列表表示。 邻接列表 是用于表示有限图的无序列表的集合。每个列表都描述了图中节点的邻居集。 给定节点将始终是图中的第一个节点（值为 1）。你必须将 给定节点的拷贝 作为对克隆图的引用返回。解题思路： 理解题目中图的深拷贝：图的深拷贝是为了构建一张与原图结构、值均一样的图，但是其中的节点不再是原图节点的引用 深拷贝的条件：①图的结构 ②对应节点的值 题目只给了我们一个节点的引用，所以我们需要从给定的节点出发，进行图的遍历，并在遍历中不断完善图的深拷贝 本文采用深度优先搜索的方式进行解决 全局创建hashMap用于存储已经访问过的节点 搜索图，判断当前节点是否在hashMap中，(单层逻辑)不存在则克隆节点，并将其加入哈希表中，并遍历该节点的邻居并更新克隆节点的邻居节点 终止条件：当前节点存在hashMap中，返回对应的克隆节点代码实现：/*// Definition for a Node.class Node { public int val; public List&lt;Node&gt; neighbors; public Node() { val = 0; neighbors = new ArrayList&lt;Node&gt;(); } public Node(int _val) { val = _val; neighbors = new ArrayList&lt;Node&gt;(); } public Node(int _val, ArrayList&lt;Node&gt; _neighbors) { val = _val; neighbors = _neighbors; }}*/class Solution { //创建哈希表用于存储已经访问过的节点和克隆的节点 key是原始图中的节点 value是克隆图中的节点 HashMap&lt;Node,Node&gt; visited = new HashMap&lt;&gt;(); public Node cloneGraph(Node node) { //判断节点为空的情况 if(node == null){ return node; } //终止条件：如果该节点已经被访问过了，则直接从哈希表中取出对应的克隆节点并返回 if(visited.containsKey(node)){ return visited.get(node); } //创建原节点的克隆节点 Node cloneNode = new Node(node.val,new ArrayList()); //将原节点以及对应的克隆节点存入哈希表 visited.put(node,cloneNode); //遍历该节点的邻居并更新克隆节点的邻居节点 for(Node neighbor:node.neighbors){ cloneNode.neighbors.add(cloneGraph(neighbor)); //递归更新克隆节点的邻居列表 } return cloneNode; }}5.目标和 给你一个整数数组 nums 和一个整数 target 。 向数组中的每个整数前添加 ‘+’ 或 ‘-‘ ，然后串联起所有整数，可以构造一个 表达式 ： 例如，nums = [2, 1] ，可以在 2 之前添加 ‘+’ ，在 1 之前添加 ‘-‘ ，然后串联起来得到表达式 “+2-1” 。返回可以通过上述方法构造的、运算结果等于 target 的不同 表达式 的数目。 1 &lt;= nums.length &lt;= 200 &lt;= nums[i] &lt;= 10000 &lt;= sum(nums[i]) &lt;= 1000-1000 &lt;= target &lt;= 1000解题思路： 本题有回溯和动态规划两种解题方法，本文采用回溯的方式 创建回溯方法遍历所有的表达式 共有2的n次方条表达式(n为数组个数) 当index下标与nums.length相等时，进行比较sum与target，判断count是否加1 否则调用自身方法进入下一层，直至index == nums.length，同时更新sum代码实现：class Solution { //创建count用于记录目标表达式的个数 int count = 0; public int findTargetSumWays(int[] nums, int target) { //调用回溯方法 back(nums,target,0,0); return count; } //回溯方法 public void back(int[] nums,int targer, int index, int sum){ //如果达到数组尾部 index代表数组下标 sum代表当前元素和 if(index == nums.length){ if(sum == targer){ count++; } } else{ //调用方法进行递归 back(nums,targer,index+1,sum + nums[index]); back(nums,targer,index+1,sum - nums[index]); } }} 回溯是深度优先搜索思想的一种应用6.钥匙和房间 有 n 个房间，房间按从 0 到 n - 1 编号。最初，除 0 号房间外的其余所有房间都被锁住。你的目标是进入所有的房间。然而，你不能在没有获得钥匙的时候进入锁住的房间。 当你进入一个房间，你可能会在里面找到一套不同的钥匙，每把钥匙上都有对应的房间号，即表示钥匙可以打开的房间。你可以拿上所有钥匙去解锁其他房间。 给你一个数组 rooms 其中 rooms[i] 是你进入 i 号房间可以获得的钥匙集合。如果能进入 所有 房间返回 true，否则返回 false。 n == rooms.length2 &lt;= n &lt;= 10000 &lt;= rooms[i].length &lt;= 10001 &lt;= sum(rooms[i].length) &lt;= 30000 &lt;= rooms[i][j] &lt; n所有 rooms[i] 的值 互不相同解题思路： 本文采用深度优先搜索的方式进行求解 创建全局数组用于记录该位置是否被经历过 创建全局变量用于记录钥匙的个数，最终判断获得的钥匙个数是否与房间数相等，返回结果代码实现：class Solution { //创建全局数组和变量用于记录数组是否访问过即长度为多少 boolean[] visted; int nums; public boolean canVisitAllRooms(List&lt;List&lt;Integer&gt;&gt; rooms) { //获取集合长度 int n = rooms.size(); nums = 0; visted = new boolean[n]; dfs(rooms,0); return nums == n; } //递归方法 public void dfs(List&lt;List&lt;Integer&gt;&gt; rooms,int x){ visted[x] = true; nums++; //访问房间加1 for(int it : rooms.get(x)){ if(!visted[it]){ //该房间并没有访问过 dfs(rooms,it); } } }}" }, { "title": "数据结构--队列", "url": "/posts/Queue/", "categories": "数据结构与算法", "tags": "学习", "date": "2022-11-07 04:03:00 +0000", "snippet": "数据结构–队列一、队列简介​\t队列（queue）是一种线性数据结构。与同为线性数据结构的栈不同的是队列中的元素遵循的是先进先出（First In First Out）的规则，和在现实中排队一样，讲究的是先来后到的原则。队列出口端叫做队头（front），队列的入口端叫做队尾（rear）二、队列的实现 队列这种数据结构既可以用数组来实现，也可以用链表来实现。用数组来实现时，我们可以自行规定队尾...", "content": "数据结构–队列一、队列简介​\t队列（queue）是一种线性数据结构。与同为线性数据结构的栈不同的是队列中的元素遵循的是先进先出（First In First Out）的规则，和在现实中排队一样，讲究的是先来后到的原则。队列出口端叫做队头（front），队列的入口端叫做队尾（rear）二、队列的实现 队列这种数据结构既可以用数组来实现，也可以用链表来实现。用数组来实现时，我们可以自行规定队尾的位置，比如说规定最后一个元素的下一个位置为队尾，或者规定最后一个元素所在的位置为队尾，这两种都行队列的链表实现：队列的数组实现：三、队列的基本操作 队列的基本操作包括入队(enqueue)和出队(dequeue)两种 本文以数组实现的队列为例进行介绍1、入队​\t入队就是把新元素放入到队列中，只允许从队尾方向入队，入队之后，新元素的下一个元素所在的位置称为队尾。例如：一个已有元素3、5、1、4、9、6的队列，现需要将元素 7 进行入队，那么入队之后的新队列为：2、出队​\t出队就是把元素从队头方向移出队列，只允许在队头一侧进行出队操作。出队之后，出队元素的新一个元素作为新的队头。例如：现有一个元素为3、5、1、4、9、6、7的队列，需要将队头元素 3 进行出队操作，那么出队之后新队列即为：3、设计循环列表​\t很显然，队列的出队操作我们只是将队头元素的指向进行了移动，并没有将原有的队头元素进行删除，那这样的话就会导致出现一个问题，不删除原有元素，一直进行出队操作，导致队头左边的空间失去意义，如果不进行数组扩容的话，队列容量只会越来越小的。针对这种情况，便出现了一种特殊的队列——循环队列 设计你的循环队列实现。 循环队列是一种线性数据结构，其操作表现基于 FIFO（先进先出）原则并且队尾被连接在队首之后以形成一个循环。它也被称为“环形缓冲器”。 循环队列的一个好处是我们可以利用这个队列之前用过的空间。在一个普通队列里，一旦一个队列满了，我们就不能插入下一个元素，即使在队列前面仍有空间。但是使用循环队列，我们能使用这些空间去存储新的值 实现以下这些操作： MyCircularQueue(k): 构造器，设置队列长度为 k 。 Front: 从队首获取元素。如果队列为空，返回 -1 。 Rear: 获取队尾元素。如果队列为空，返回 -1 。 enQueue(value): 向循环队列插入一个元素。如果成功插入则返回真。 deQueue(): 从循环队列中删除一个元素。如果成功删除则返回真。 isEmpty(): 检查循环队列是否为空。 isFull(): 检查循环队列是否已满。 解题思路： 本题可以采用数组或链表的方式进行创建循环队列，本文采用数组方式进行实现 创建一系列变量用于表示队列的首尾指针、容量以及初始化数组 (规定最后一个元素的下一个位置为队尾) 两种关键情况的判断条件 队列为空的条件： front == rear 队列为满的条件： (rear + 1) % capacity == front 其余的操作，如插入、删除、获取队首、队尾元素等等，都要建立在队列不为空的情况下代码实现：class MyCircularQueue { //创建相关变量 private int front; //队列首元素对应的的数组的索引 private int rear; //队列尾元素对应的索引的下一个索引 private int capacity; //循环队列的容量，即队列中最多可以容纳的元素数量 private int[] elements; //一个固定大小的数组，用于保存循环队列的元素 //构造器 初始化队列 public MyCircularQueue(int k) { capacity = k + 1; //规定循环队列最多只能有capacity-1 个元素 即规定最后一个元素的下一个位置为队尾 elements = new int[capacity]; //初始化数组 rear = front = 0; //初始化双指针 } //向循环队列中插入一个元素 public boolean enQueue(int value) { if(isFull()){ //判断是否为空 return false; } elements[rear] = value; rear = (rear + 1) % capacity; return true; } //向循环队列中删除一个元素 public boolean deQueue() { if(isEmpty()){ //判断是否为空 return false; } front = (front +1) % capacity; return true; } //获取队首元素 public int Front() { if(isEmpty()){ //判断是否为空 return -1; } return elements[front]; } //获取队尾元素 public int Rear() { if(isEmpty()) { //判断是否为空 return -1; } return elements[(rear-1 + capacity) % capacity]; } //判断队列是否为空 public boolean isEmpty() { return rear == front; } //判断队列是否满 public boolean isFull() { return ((rear+1) % capacity) == front; //相等则返回true }}4、用队列实现栈 请你仅使用两个队列实现一个后入先出（LIFO）的栈，并支持普通栈的全部四种操作（push、top、pop 和 empty）。 实现 MyStack 类： void push(int x) 将元素 x 压入栈顶。int pop() 移除并返回栈顶元素。int top() 返回栈顶元素。boolean empty() 如果栈是空的，返回 true ；否则，返回 false 。解题思路： 使用两个队列实现栈的功能，一个队列用于模拟栈，另一个队列用于存放元素时进行辅助操作 存放元素时，先放入辅助队列，之后在进行交换 每一次存放元素时，都要判断代表”栈“的队列是否为空，如果不为空要将当前队列中的元素转移到辅助队列（实现先进后出）代码实现：class MyStack { Queue&lt;Integer&gt; queue1; Queue&lt;Integer&gt; queue2; //初始化 public MyStack() { queue1 = new LinkedList&lt;Integer&gt;(); //用于模拟栈 queue2 = new LinkedList&lt;Integer&gt;(); //用于存放元素时进行辅助操作 } public void push(int x) { queue2.offer(x); while (!queue1.isEmpty()) { //注意使用while queue2.offer(queue1.poll()); } Queue&lt;Integer&gt; temp = queue1; queue1 = queue2; queue2 = temp; } public int pop() { return queue1.poll(); } public int top() { return queue1.peek(); } public boolean empty() { return queue1.isEmpty(); }}5、队列的初始化Queue&lt;T&gt; queue = new LinkedList&lt;&gt;();四、队列与广度优先搜索1、广度优先搜索简介 广度优先搜索（BFS）的一个常见应用是找出从根结点到目标结点的最短路径。与树的层序遍历类似，越是接近根结点的结点将越早地遍历。它是从初始结点开始,应用产生式规则和控制策略生成第一层结点,同时检查目标结点是否在这些生成的结点中。若没有,再用产生式规则将所有第一层结点逐一拓展,得到第二层结点,并逐一检査第二层结点是否包含目标结点。若没有,再用产生式规则拓展第二层结点。如此依次拓展,检査下去,直至发现目标结点为止。如果拓展完所有结点,都没有发现目标结点,则问题无解2、经典问题1.岛屿数量 给你一个由 ‘1’（陆地）和 ‘0’（水）组成的的二维网格，请你计算网格中岛屿的数量。 岛屿总是被水包围，并且每座岛屿只能由水平方向和/或竖直方向上相邻的陆地连接形成。 此外，你可以假设该网格的四条边均被水包围。 m == grid.length n == grid[i].length 1 &lt;= m, n &lt;= 300 grid[i][j] 的值为 '0' 或 '1' 解题思路： 本文采用队列+广度优先搜索的方式进行求解 遍历二维数组，当遇见 ‘1’ 时，岛屿的数量+1 创建队列用于获取当前为 ‘1’ 元素在二维数组的位置信息 neighbors.add(r * nc + c); r代表横坐标 nc 代表总列数 c代表纵坐标 用id接受队列的位置信息， 可得： 横坐标 r = id / nc 纵坐标 c = id % nc 且每当遇见一个 ‘1’ 时，要将对当前 ‘1’ 的周围另外一些 ‘1’ 进行搜索，采用广度优先搜索的方式 实现广度优先遍历：遍历至队列为空，每使用一次队列即进行一次广度优先遍历，依次遍历当前元素上下左右的元素是否为 ‘1’ ，为 ‘1’ 则添加进队列之中，并将其置为 ‘0’ 最终岛屿的数量等于广度优先优先搜索的次数代码实现：class Solution { public int numIslands(char[][] grid) { //判断二维数组为空的情况 if(grid == null || grid.length == 0){ return -1; } //获取二位数组的行和列长度 int nr = grid.length; //行 int nc = grid[0].length; //列 //创建变量用于记录岛屿个数 且广度优先搜索遍历的次数即为岛屿的个数 int num_islands = 0; //遍历二维数组 for(int r = 0; r &lt; nr; ++r){ for(int c = 0; c &lt; nc; ++c){ //当出先一次 ‘1’ 时,则岛屿数量加 1,并开始广度优先搜索附近的\"1\" if(grid[r][c] == '1'){ //岛屿数加一 num_islands++; //创建队列辅助进行搜索 Queue&lt;Integer&gt; neighbors = new LinkedList&lt;&gt;(); //将当前元素置为0 grid[r][c] = '0'; //将当前二位数组的位置信息存进队列之中 neighbors.add(r * nc + c); //遍历队列直到为空 while(!neighbors.isEmpty()){ //获取出当前的元素在二位数组中的位置信息并删除 int id = neighbors.remove(); //获取当前元素在二维数组中的位置信息 int row = id / nc; //横坐标 int col = id % nc; //纵坐标 //依次遍历上下左右四个位置的元素，如果为‘1’添加到队列之中 并且置为0 if(row-1 &gt;= 0 &amp;&amp; grid[row-1][col] == '1'){ //上 neighbors.add((row-1) * nc + col); grid[row-1][col] = '0'; } if(row+1 &lt; nr &amp;&amp; grid[row+1][col] == '1'){ //下 neighbors.add((row+1) *nc + col); grid[row+1][col] = '0'; } if(col-1 &gt;= 0 &amp;&amp; grid[row][col-1] == '1'){ //左 neighbors.add(row * nc + col - 1); grid[row][col-1] = '0'; } if(col+1 &lt; nc &amp;&amp; grid[row][col+1] == '1'){ //右 neighbors.add(row * nc + col + 1); grid[row][col+1] = '0'; } } } } } return num_islands; }}2.打开转盘锁 你有一个带有四个圆形拨轮的转盘锁。每个拨轮都有10个数字： ‘0’, ‘1’, ‘2’, ‘3’, ‘4’, ‘5’, ‘6’, ‘7’, ‘8’, ‘9’ 。每个拨轮可以自由旋转：例如把 ‘9’ 变为 ‘0’，’0’ 变为 ‘9’ 。每次旋转都只能旋转一个拨轮的一位数字。 锁的初始数字为 ‘0000’ ，一个代表四个拨轮的数字的字符串。 列表 deadends 包含了一组死亡数字，一旦拨轮的数字和列表里的任何一个元素相同，这个锁将会被永久锁定，无法再被旋转。 字符串 target 代表可以解锁的数字，你需要给出解锁需要的最小旋转次数，如果无论如何不能解锁，返回 -1 。 1 &lt;= deadends.length &lt;= 500 deadends[i].length == 4 target.length == 4 target 不在 deadends 之中 target 和 deadends[i] 仅由若干位数字组成 解题思路： 判断两种特殊情况：①初始值等于目标值；②死亡列表包含初始值 核心思路：对当前值进行广度优先搜索，对其每一位的字符都向前向后翻转，一共会获得8个值 通过创建集合seen收集已经遍历过的字符串和存储死亡列表的集合来判断是否包含返回的8个值，当都不包含时则进一步判断是否与目标值相等 创建队列来存储这些值，赋值进行广度优先遍历代码实现：class Solution { public int openLock(String[] deadends, String target) { //判断初始值直接等于目标值的情况 if(\"0000\".equals(target)){ return 0; } //创建一个新集合用于存储死亡数字 Set&lt;String&gt; dead = new HashSet&lt;String&gt;(); //将列表deadends内的数值添加到集合中 for(String deadend : deadends){ dead.add(deadend); } //判断致命列表存在初始值的情况 if(dead.contains(\"0000\")){ return -1; } //创建变量用于记录操作步数 int step = 0; //创建队列用于辅助广度优先搜索 Queue&lt;String&gt; queue = new LinkedList&lt;String&gt;(); queue.offer(\"0000\"); //将初始值添加到队列之中 //创建一个集合用于接受遍历过的数值 Set&lt;String&gt; seen = new HashSet&lt;String&gt;(); seen.add(\"0000\"); //将初始值添加到已经访问的集合中 //使用队列进行广度优先遍历 while(!queue.isEmpty()){ ++step; //步数+1 //获取队列的长度 int size = queue.size(); //遍历当前队列 for(int i = 0; i &lt; size; ++i){ String status = queue.poll(); //获取队头元素 //判断进行广度优先搜索返回的8个值是否符合条件 for(String nextStatus : get(status)){ //当 已访问集合 和 死亡数字列表中不包含当前值 进行判断是否与目标值相同 if(!seen.contains(nextStatus) &amp;&amp; !dead.contains(nextStatus)){ if(nextStatus.equals(target)){ return step; } queue.offer(nextStatus); seen.add(nextStatus); } } } } return -1; //如何上述遍历无法找到目标值,则代表无法解锁 } //字符向前翻转 public char numPrev(char x){ return x == '0' ? '9' : (char) (x-1); //x-1会自动转换为整数类型,必须重新转换回来 } //字符向后翻转 public char numSucc(char x){ return x == '9' ? '0' : (char) (x+1); } //枚举status通过一次旋转得到的数字 并返回对应的集合 达到广度优先搜索的效果 public List&lt;String&gt; get(String status){ //创建集合用于接受返回值 总共会返回8个字符串 List&lt;String&gt; ret = new ArrayList&lt;String&gt;(); //将status转化为字符数组，便于操作每一位上的数 char[] array = status.toCharArray(); //遍历每一个字符，获取向前向后移动的数字 for(int i = 0; i &lt; 4; i++){ char num = array[i]; array[i] = numPrev(num); ret.add(new String(array)); //添加这一位向前翻转的字符串 array[i] = numSucc(num); ret.add(new String(array)); //添加这一位向后翻转的字符串 array[i] = num; //重新恢复当前字符的值 } return ret; }}3.完全平方数 给你一个整数 n ，返回 和为 n 的完全平方数的最少数量 。 完全平方数 是一个整数，其值等于另一个整数的平方；换句话说，其值等于一个整数自乘的积。例如，1、4、9 和 16 都是完全平方数，而 3 和 11 不是。 1 &lt;= n &lt;= 104 解题思路： 本题可以采用广度优先搜索和动态规划的方式进行求解 但是使用第一种的方式会导致程序的运行时间过长，导致效率低效，本文便采取动态规划的方式进行求解，算是一种扩展 （切记动态规划的方式同样可以实现） 由本题的内容可以写出动态规划方程： f(n) = min{f(n-1)、f(n-4)、f(n-9)·····} + 1 f(n)表示使用凑n的安全平方数的最少个数 创建数组用于记录组成每一个数(1~n)所需要平方数的最少个数 利用动态规划的思想，即 f(n) 是由 f(n-1)、f(n-4)等等决定的，从小到大，最终可以求解出 f(n)代码实现：//采用动态规划实现class Solution { public int numSquares(int n) { //创建数组f[] f[i]表示使用平方数来组成i的最少个数 f[0] = 0 int[] f = new int[n+1]; //长度为n+1 //f[n]的大小是有 f[n-1] f[n-4]来决定的 所以从小到大进行遍历 for(int i = 1; i &lt;= n; ++i){ //初始化最小个数为无穷多个 int minn = Integer.MAX_VALUE; //f[i] = min{f[i-1],f[i-4]....} + 1 for(int j = 1; j * j &lt;= i; j++){ minn = Math.min(minn,f[i - j * j]); } f[i] = minn + 1; } return f[n]; }}4.图像渲染 有一幅以 m x n 的二维整数数组表示的图画 image ，其中 image[i][j] 表示该图画的像素值大小。 你也被给予三个整数 sr , sc 和 newColor 。你应该从像素 image[sr][sc] 开始对图像进行 上色填充 。 为了完成 上色工作 ，从初始像素开始，记录初始坐标的 上下左右四个方向上 像素值与初始坐标相同的相连像素点，接着再记录这四个方向上符合条件的像素点与他们对应 四个方向上 像素值与初始坐标相同的相连像素点，……，重复该过程。将所有有记录的像素点的颜色值改为 newColor 。 最后返回 经过上色渲染后的图像 。 m == image.length n == image[i].length 1 &lt;= m, n &lt;= 50 0 &lt;= image[i][j], newColor &lt; 216 0 &lt;= sr &lt; m 0 &lt;= sc &lt; n 解题思路： 在本文采用队列+广度优先搜索的方式进行求解 本题与上文中的岛屿数量问题类似，同样是创建一个队列用于存储元素在二维数组中的位置信息 queue.offer(sr * nc + sc); 判断两种特殊情况 判断二维数组是否为空：if(image == null &amp;&amp; image.length == 0) 判断前后上色前后颜色是否相等：if(oldColor == color) 遍历队列直至为空，依次遍历上下左右的元素，如果与当前元素为相连像素点则对其进行上色并将其存入队列中 最终返回二维数组即可 tip：队列移除元素的操作 remove()：获取队头元素并将其移除，如果队列为空则会抛出异常 poll()：获取队头元素并将其移除，如果队列为空则返回null 代码实现：class Solution { public int[][] floodFill(int[][] image, int sr, int sc, int color) { //判断二维数组为空的情况 if(image == null &amp;&amp; image.length == 0){ return null; } //获取旧颜色的值 int oldColor = image[sr][sc]; if(oldColor == color){ //前后上色颜色相同的情况 return image; } //获取二维数组的总行数与总列数 int nr = image.length; //行 int nc = image[0].length; //列 //为当前位置进行上色 image[sr][sc] = color; //创建队列用于进行广度优先搜索 Queue&lt;Integer&gt; queue = new LinkedList&lt;&gt;(); queue.offer(sr * nc + sc); //将当前的位置信息存入队列之中 //遍历队列进行广度优先遍历 while(!queue.isEmpty()){ //获取队列头部的元素位置信息 int id = queue.remove(); //得出当前元素的位置信息 int r = id / nc; int c = id % nc; //依次遍历上下左右的元素，如果与当前元素为相连像素点则对其进行上色并将其存入队列中 if(r - 1 &gt;= 0 &amp;&amp;image [r-1][c] == oldColor){ //上 image[r-1][c] = color; queue.offer((r-1) * nc + c); } if(r + 1 &lt; nr &amp;&amp; image[r+1][c] == oldColor){ //下 image[r+1][c] = color; queue.offer((r+1) * nc + c); } if(c - 1 &gt;= 0 &amp;&amp; image[r][c-1] == oldColor){ //左 image[r][c-1] = color; queue.offer(r * nc + c - 1); } if(c + 1 &lt; nc &amp;&amp; image[r][c+1] == oldColor){ //右 image[r][c+1] = color; queue.offer(r * nc + c + 1); } } return image; }}5.01矩阵 给定一个由 0 和 1 组成的矩阵 mat ，请输出一个大小相同的矩阵，其中每一个格子是 mat 中对应位置元素到最近的 0 的距离。 两个相邻元素间的距离为 1 。 m == mat.length n == mat[i].length 1 &lt;= m, n &lt;= 104 1 &lt;= m * n &lt;= 104 mat[i][j] is either 0 or 1 mat 中至少有一个 0 解题思路： 当遇到类似01矩阵、岛屿数量这样的搜索问题时，要考虑记录已经访问过的位置 解决方法如下： 创建已访问数组seen记录 （打开转盘锁） 根据题目的条件进行对应的修改 (岛屿问题：将访问过的1置0 图像渲染：将旧颜色修改为新颜色) 创建一个静态数组便于当前位置进行上下左右进行访问 创建队列，一开始将所有的零加入到队列之中 遍历队列直至为空，依次遍历当前位置的上下左右四个位置，符合条件的判断： 当前的坐标符合二维数组：ni &gt;= 0 &amp;&amp; nj &gt;= 0 &amp;&amp; ni &lt; m &amp;&amp; nj &lt; n 当前位置并没有被访问过 符合条件的操作：修改其到0的距离(即在原来的基础上加一)，并加入到队列之中代码实现:class Solution { //创建静态数组 用于进行当前坐标的上下左右移动 分别对应 下 上 左 右 public int[][] updateMatrix(int[][] mat) { int m = mat.length; //获取行数 int n = mat[0].length; //获取列数 //创建结果数组 int[][] dist = new int[m][n]; //创建二维数组用于记录当前位置是否被访问过 boolean[][] seen = new boolean[m][n]; //创建队列用于广度优先搜索 Queue&lt;int[]&gt; queue = new LinkedList&lt;int[]&gt;(); //将所有的0都添加进队列之中 for(int i = 0; i &lt; m; ++i){ for(int j = 0; j &lt; n; ++j){ if(mat[i][j] == 0){ queue.offer(new int[]{i,j}); seen[i][j] = true; //将所有的0都记录为访问过 } } } //进行广度优先搜索 while(!queue.isEmpty()){ //获取队头元素 int[] cell = queue.remove(); //获取坐标位置 int i = cell[0], j = cell[1]; //依次访问当前位置的上下左右位置 for(int k = 0; k &lt; 4; k++){ int ni = i + dirs[k][0]; int nj = j + dirs[k][1]; if(ni &gt;= 0 &amp;&amp; nj &gt;= 0 &amp;&amp; ni &lt; m &amp;&amp; nj &lt; n &amp;&amp; !seen[ni][nj]){ dist[ni][nj] = dist[i][j] + 1; //将当前位置在原来的基础上+1 queue.offer(new int[]{ni,nj}); //将该位置添加到队列之中 便于在此基础之上访问附近的位置 seen[ni][nj] = true; } } } return dist; }}" }, { "title": "跨域问题详解", "url": "/posts/Cross/", "categories": "技术科普", "tags": "学习", "date": "2022-10-30 02:22:00 +0000", "snippet": "一、什么是跨域 跨域请求出现错误的条件：浏览器同源策略 &amp;&amp; Ajax请求​\tCORS全称Cross-Origin Resource Sharing，意为跨域资源共享。当一个资源去访问另一个不同域名或者同域名不同端口的资源时，就会发出跨域请求。如果此时另一个资源不允许其进行跨域资源访问，那么访问就会遇到跨域问题。​\t跨域指的是浏览器不能执行其它网站的脚本。是由浏览器的同源策...", "content": "一、什么是跨域 跨域请求出现错误的条件：浏览器同源策略 &amp;&amp; Ajax请求​\tCORS全称Cross-Origin Resource Sharing，意为跨域资源共享。当一个资源去访问另一个不同域名或者同域名不同端口的资源时，就会发出跨域请求。如果此时另一个资源不允许其进行跨域资源访问，那么访问就会遇到跨域问题。​\t跨域指的是浏览器不能执行其它网站的脚本。是由浏览器的同源策略造成的，是浏览器对JavaScript 施加的安全限制。二、同源策略1、定义​\t同源策略是由 Netscape 提出的一个安全策略，它是浏览器最核心也最基本的安全功能，如果缺少了同源策略，则浏览器的正常功能可能都会受到影响。 可以说Web是构建在同源策略基础之上的，浏览器只是针对同源策略的一种实现。所有支持JavaScript的浏览器都会使用这个策略2、同源的条件 规定：浏览器要求，在解析Ajax请求时，要求浏览器的请求路径与Ajax请求的路径必须满足三个条件，则满足同源策略，才可以访问服务器。满足同源的三个条件： 域名 协议 端口号3、为什么要同源限制同源策略存在的意义： 非同源下的 cookie 等隐私数据可以被随意获取 非同源下的 DOM 可以的随意操作 ajax 可以任意请求的话，用户的各种隐私肯定会泄露，对用户造成不同程度的损失4、限制范围同源策略的限制范围： 不能获取不同源的 cookie，LocalStorage 和 indexDB 不能获取不同源的 DOM() 不能发送不同源的 ajax 请求 （可以向不同源的服务器发起请求，但是返回的数据会被浏览器拦截）三、跨域的解决方法1、CORS CORS方案，就是通过设置服务器响应头来实现跨域 CORS 才是解决跨域的真正解决方案 跨域资源共享 只服务端设置Access-Control-Allow-Origin即可，前端无须设置，若要带cookie请求：前后端都需要设置/** 导入包：import javax.servlet.http.HttpServletResponse;* 接口参数中定义：HttpServletResponse response*/// 允许跨域访问的域名：若有端口需写全（协议+域名+端口），若没有端口末尾不用加'/'response.setHeader(\"Access-Control-Allow-Origin\", \"http://www.domain1.com\"); // 允许前端带认证cookie：启用此项后，上面的域名不能为'*'，必须指定具体的域名，否则浏览器会提示response.setHeader(\"Access-Control-Allow-Credentials\", \"true\"); // 提示OPTIONS预检时，后端需要设置的两个常用自定义头response.setHeader(\"Access-Control-Allow-Headers\", \"Content-Type,X-Requested-With\");2、JSONPJSONP(JSON with Padding)是JSON的一种“使用模式”，可用于解决主流浏览器的跨域数据访问的问题。 由于同源策略，一般来说位于 server1.example.com 的网页无法与不是 server1.example.com的服务器沟通，而 HTML 的 script 元素是一个例外。利用 //原生js实现方式 还可以通过jQuery、Vue等方式实现&lt;script&gt; var script = document.createElement('script'); script.type = 'text/javascript'; // 传参一个回调函数名给后端，方便后端返回时执行这个在前端定义的回调函数 script.src = 'http://www.domain2.com:8080/login?user=admin&amp;callback=handleCallback'; document.head.appendChild(script); // 回调执行函数 function handleCallback(res) { alert(JSON.stringify(res)); } &lt;/script&gt;JSONP的特点： 利用&lt;script&gt;标签没有跨域的限制 通过&lt;script&gt;标签src属性,发送带有callback参数的GET请求 服务端将接口返回数据拼凑到callback函数中 JSONP方案和Ajax没有任何关系 JSONP方案只支持GET请求 JSONP没有浏览器兼容问题，任何浏览器都支持3、浏览器修改同源策略 跨域是浏览器的一个安全限制,我们可以通过修改一些设置，让被设置的浏览器没有这个同源的限制,自然可以实现跨域,但这种方式往往是治标不治本，通常不采用其进行实现4、代理服务器跨域 以nginx为例 通过Nginx配置一个代理服务器域名与domain1相同，端口不同，做跳板机，反向代理访问domain2接口，并且可以顺便修改cookie中domain信息，方便当前域cookie写入，实现跨域访问#proxy服务器server { listen 81; server_name www.domain1.com; location / { proxy_pass http://www.domain2.com:8080; #反向代理 proxy_cookie_domain www.domain2.com www.domain1.com; #修改cookie里域名 index index.html index.htm; # 当用webpack-dev-server等中间件代理接口访问nignx时，此时无浏览器参与，故没有同源限制，下面的跨域配置可不启用 add_header Access-Control-Allow-Origin http://www.domain1.com; #当前端只跨域不带cookie时，可为* add_header Access-Control-Allow-Credentials true; }}原理： 代理服务和前端服务之间由于协议域名端口三者统一不存在跨域的问题,可以直接发送请求 代理服务和后端服务之间并不通过浏览器没有同源策略的限制,可以直接发送请求四、项目实战中的解决方法 以SpringBoot项目为例，有以下三种具体解决方法1、添加 @CrossOrigin 注解 这种是最简单但不常用的一种，只需要在后端接口方法上添加 @CrossOrigin 注解，即可解决对这个接口方法的请求跨域问题，但是在实际开发中一般都会有很多的方法，在每一个方法上都添加这个注解的话明显就会很影响使用感受。当然还可以将注解添加到类上，表示类中的所有方法都解决了跨域问题2、过滤器统一处理 配置过滤器进行统一处理//注意包是否导入正确import org.springframework.context.annotation.Bean;import org.springframework.web.cors.CorsConfiguration;import org.springframework.web.cors.UrlBasedCorsConfigurationSource;import org.springframework.web.filter.CorsFilter;@Configurationpublic class CorsConfig { @Bean public CorsFilter corsFilter() { CorsConfiguration corsConfiguration = new CorsConfiguration(); // 使用通配符* 允许所有的域请求 corsConfiguration.addAllowedOrigin(\"*\"); // 使用通配符* 允许所有请求头字段 corsConfiguration.addAllowedHeader(\"*\"); // 使用通配符* 允许所有请求头方法类型 corsConfiguration.addAllowedMethod(\"*\"); UrlBasedCorsConfigurationSource source = new UrlBasedCorsConfigurationSource(); // 处理请求映射 source.registerCorsConfiguration(\"/**\", corsConfiguration); return new CorsFilter(source); }}3、使用WebMvc的配置类 实现WebMvc的配置 添加允许请求的域、方法、请求头字段//注意包是否导入正确import org.springframework.context.annotation.Configuration;import org.springframework.web.servlet.config.annotation.CorsRegistry;import org.springframework.web.servlet.config.annotation.WebMvcConfigurer;@Configurationpublic class WebMvcConfig implements WebMvcConfigurer { @Override public void addCorsMappings(CorsRegistry registry) { registry.addMapping(\"/**\") // 设置映射 .allowedOriginPatterns(\"*\") // 设置域 .allowedMethods(\"*\") // 设置请求的方式GET、POST等 .allowCredentials(true) // 设置是否携带cookie .maxAge(3600) // 设置设置的有效期 秒单位 .allowedHeaders(\"*\"); // 设置头 }}" }, { "title": "详解Java类中serialVersionUID", "url": "/posts/serialVersionUID/", "categories": "技术科普", "tags": "学习", "date": "2022-10-22 14:15:00 +0000", "snippet": "详解Java类中serialVersionUID 一直以来在制作项目的过程中，使用Mybatis-X插件生成的Java实体类都自动会实现 Serializable这个接口，并自动生成了serialVersionUID这一属性 最近得闲查阅了相关资料，了解了其存在的作用，便决定编写相应笔记记录下来一、serialVersionUID简介​\tserialVersionUID属性是用来序列的标...", "content": "详解Java类中serialVersionUID 一直以来在制作项目的过程中，使用Mybatis-X插件生成的Java实体类都自动会实现 Serializable这个接口，并自动生成了serialVersionUID这一属性 最近得闲查阅了相关资料，了解了其存在的作用，便决定编写相应笔记记录下来一、serialVersionUID简介​\tserialVersionUID属性是用来序列的标识符/反序列化的对象序列化类​\t序列化运行时可与每个可序列化的类关联一个版本号，称为serialVersionUID，在反序列化期间使用该版本号来验证序列化对象的发送者和接收者是否已加载了该对象的与序列化兼容的类。二、serialVersionUID的作用​\tserialVersionUID适用于Java的序列化机制。简单来说，Java的序列化机制是通过判断类的serialVersionUID来验证版本一致性的。在进行反序列化时，JVM会把传来的字节流中的serialVersionUID与本地相应实体类的serialVersionUID进行比较，如果相同就认为是一致的，可以进行反序列化，否则就会出现序列化版本不一致的异常，即是InvalidClassExceptions。三、serialVersionUID的实现方式1、手动定义 当我们在编写程序的时候，可以为可序列化的类显式地声明一个serialVersionUID，并且其必须使用static、final两个关键字进行声明，数据类型必须为longprivate static final long serialVersionUID = 1L;2、自动生成 当一个类实现Serializable类接口，如果没有显示定义serialVersionUID，程序序列化运行时会根据包名，类名，继承关系，非私有的方法和属性，以及参数，返回值等诸多因子计算得出的，极度复杂生成的一个64位的哈希字段作为默认的serialVersionUID值。3、建议 强烈建议所有可序列化的类显式声明serialVersionUID值，因为默认serialVersionUID计算对类详细信息高度敏感，而类详细信息可能会根据编译器的实现而有所不同，因此可能在InvalidClassExceptions反序列化期间导致意外情况 因此，为了保证serialVersionUID在不同Java编译器实现之间的值一致，可序列化的类建议声明一个显式serialVersionUID值。还强烈建议在serialVersionUID的声明中尽可能使用private修饰符，因为此类声明仅适用于立即声明的类-serialVersionUID字段作为继承成员没有用。四、应用场景1、代码实现序列化实体类：public class Persion implements Serializable { private static final long serialVersionUID = 4359709211352400087L; public Long id; public String name; public final String userName; public Persion(Long id, String name){ this.id = id; this.name = name; userName = \"dddbbb\"; } public String toString() { return id.toString() + \"--\" + name.toString(); }}序列化过程：public class SerialTest { public static void main(String[] args) { Persion p = new Persion(1L, \"陈俊生\"); System.out.println(\"person Seria:\" + p); try { FileOutputStream fos = new FileOutputStream(\"Persion.txt\"); ObjectOutputStream oos = new ObjectOutputStream(fos); oos.writeObject(p); oos.flush(); oos.close(); } catch (IOException e) { e.printStackTrace(); } }}反序列化过程：public class DeserialTest { public static void main(String[] args) { Persion p; try { FileInputStream fis = new FileInputStream(\"Persion.txt\"); ObjectInputStream ois = new ObjectInputStream(fis); p = (Persion) ois.readObject(); ois.close(); System.out.println(p.toString()); System.out.println(p.userName); } catch (IOException | ClassNotFoundException e) { e.printStackTrace(); } }}2、具体情况​\tPersion类序列化之后，假定从A端传递到B端，然后在B端进行反序列化,之后所有的情况都建立在这基础之上情况一 在序列化Persion和反序列化Persion的时候A和B端都需要一个相同的类，如果两处的serialVersionUID不一致，会产生什么样的效果呢？​\t最终报出InvalidClassExceptions异常java.io.InvalidClassException:serializable.Persion; local class incompatible: stream classdesc serialVersionUID = 4359709211352400087, local class serialVersionUID = 4359709211352400082\tat java.io.ObjectStreamClass.initNonProxy(ObjectStreamClass.java:616)\tat java.io.ObjectInputStream.readNonProxyDesc(ObjectInputStream.java:1843)\tat java.io.ObjectInputStream.readClassDesc(ObjectInputStream.java:1713)\tat java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2000)\tat java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1535)\tat java.io.ObjectInputStream.readObject(ObjectInputStream.java:422)情况二 假设两处serialVersionUID一致，如果A端增加一个字段，然后序列化，而B端不变，然后反序列化，会是什么情况呢?​\tB端进行反序列化且反序列化正常，A端中新增加的字段会丢失（被B端忽略）情况三 假设两处serialVersionUID一致，如果B端丢失一个字段，A端不变，会是什么情况呢?​\tB端进行反序列化且反序列化正常，A端会减少与B段丢失的相同字段（被B端忽略）情况四 假设两处serialVersionUID一致，如果B端增加一个字段，A端不变，会是什么情况呢?​\tB端进行反序列化且反序列化正常，A端获得与B段增加的相同字段五、总结​\t实现serialVersionUID接口的目的是为实现类可持久化，比如在网络传输或本地存储，为系统的分布和异构部署提供先决条件。若没有序列化，现在我们所熟悉的远程调用，对象数据库都不可能存在。" }, { "title": "数据结构--字符串", "url": "/posts/String/", "categories": "数据结构与算法", "tags": "学习", "date": "2022-10-13 04:15:00 +0000", "snippet": "数据结构–字符串一、字符串简介 字符串是由零个或多个字符组成的有限序列。一般记为 s = a1a2…an。它是编程语言中表示文本的数据类型。 字符串的基本操作对象通常是字符串整体或者其子串 对于不同的编程语言中，字符串可能是可变的，也可能是不可变的。不可变意味着一旦字符串被初始化，你就无法改变它的内容 在某些语言（如 C ++）中，字符串是可变的， 也就是说，你可以像在数组中那样修改...", "content": "数据结构–字符串一、字符串简介 字符串是由零个或多个字符组成的有限序列。一般记为 s = a1a2…an。它是编程语言中表示文本的数据类型。 字符串的基本操作对象通常是字符串整体或者其子串 对于不同的编程语言中，字符串可能是可变的，也可能是不可变的。不可变意味着一旦字符串被初始化，你就无法改变它的内容 在某些语言（如 C ++）中，字符串是可变的， 也就是说，你可以像在数组中那样修改字符串在其他一些语言（如 Java、Python）中，字符串是不可变的二、字符串的操作1、字符串的比较操作 在C++、Python中我们可以使用 == 来比较两个字符串 在Java中，我们无法用 == 来进行比较，可以使用 .equals()方法 来进行比较2、字符串的连接操作 C++：可以像在数组中那样修改字符串 Java： 可以使用 toCharArray 将其转换为字符数组 可以使用其他数据结构，如：StringBuilde3、字符串常用的api str.charAt(i)：获取字符串的第i个字符 str.substring(0,i)：截取字符串从0到i的字符子串 str.length()：获取字符串的长度 (ps:获取数组的长度方法为：nums.length ) str.toCharArray()：将字符串对象转化为字符数组 str.setCharAt(i,’A’)：将下标为s的字符改成‘A’三、经典问题1、最长公共前缀 编写一个函数来查找字符串数组中的最长公共前缀。 如果不存在公共前缀，返回空字符串 \"\"。 1 &lt;= strs.length &lt;= 2000 &lt;= strs[i].length &lt;= 200strs[i] 仅由小写英文字母组成解题思路：本题有横向扫描和纵向扫描解法： 纵向扫描：从前往后遍历字符串的每一列，比较相同列上的字符是否相同，相同则继续比较，不相同则停止访问，当前列之前的部分即为公共前缀 横向扫描：创建一个公共前缀，依次遍历字符串数组中的每一个字符串，通过判断字符串中的字符是否相等以此更新最长公共前缀，用第一个字符串与其余字符串一一进行比较代码实现：//纵向解法class Solution { public String longestCommonPrefix(String[] strs) { //判断字符串为空的情况 if(strs == null || strs.length == 0){ return \"\"; } //获取字符串数组的第一个字符串的长度，最长公共前缀的最大值只可能等于一个子串的长度 int length = strs[0].length(); //获取字符串数组的长度 int count = strs.length; //纵向对比 进行遍历 for(int i = 0; i &lt; length; ++i){ //获取第一个字符串的第i个字符 char c = strs[0].charAt(i); //对字符串数组的每一个字符串进行比较 for(int j = 1; j &lt; count; ++j){ //终止条件：当其他字符串的相同列字符与c不相等时或 i恰好等于当前字符串的长度 if(i == strs[j].length() || strs[j].charAt(i) != c){ return strs[0].substring(0,i); } } } return strs[0]; }}//横向解法class Solution { public String longestCommonPrefix(String[] strs) { //判断字符串为空的情况 if(strs == null &amp;&amp; strs.length == 0){ return \"\"; } //获取字符串数组的第一个字符串,以此为基准与其余字符串进行比较 String prefix = strs[0]; //获取字符串数组的长度 int length = strs.length; //遍历每一个字符串进行比较 从数组的第二个字符串开始遍历 for(int i = 1; i &lt; length; ++i){ prefix = longestCommonPrefix(prefix,strs[i]); //如果最长公共前缀的长度为0,则直接跳出循环，没有公共前缀 if(prefix.length() == 0){ break; } } return prefix; }\t//返回两个字符串的公共前缀 public String longestCommonPrefix(String str1, String str2){ //获取两组字符串中短的长度 int length = Math.min(str1.length(),str2.length()); //创建下标变量 int index = 0; //两个字符串中每一个字符进行比较 如果字符相等则进行比较 while(index &lt; length &amp;&amp; str1.charAt(index) == str2.charAt(index)){ index++; } return str1.substring(0,index); }}2、最长回文子串 给你一个字符串 s，找到 s 中最长的回文子串。 1 &lt;= s.length &lt;= 1000 s 仅由数字和英文字母组成 解题思路：回文串的定义：是一种特殊的字符串，从左往右读和从右往左读是一样的，本题的要求则是找出给定字符串中最长的回文子串本题有动态规划、暴力解法和中心扩散三种解法，其中动态规划的解法在文章《动态规划详解》已经详细讲解，这里不过多赘述，本文主要讲解暴力解法和中心扩散两种解法 暴力解法： 创建maxlen用于记录回文子串的长度、begin用于记录回文子串的开始位置 获取字符串的字符数组，便于操作记录回文子串 枚举所有子串长度大于2的情况，并记录下最长长度和起始位置 获取回文子串并返回 中心扩散解法： 创建maxlen用于记录回文子串的长度、begin用于记录回文子串的开始位置 获取字符串的字符数组，便于操作记录回文子串 枚举出所有可能的回文子串的中心位置，字符串的最后一个字符右边界无法进行扩散，第一个字符串可以一起纳入枚举中，并不影响 中心扩散的思想：回文串直观上看具有中心对称的特点，所以判断两个位置对称字符是否相等，相等的话则向左右扩张，直至不相等并返回长度 回文子串长度为奇数时，中心位置是一个字符；长度为偶数时，中心位置为两个相等的字符串 代码实现：//暴力解法class Solution { public String longestPalindrome(String s) { //获取字符串的长度 int len = s.length(); //判断字符串长度等于1或者0的情况 if(len &lt; 2){ return s; } int maxlen = 1; int begin = 0; //将字符串转换为二维数组 char[] charArray = s.toCharArray(); //枚举所有子串的长度大于1的情况,即子串的长度从2开始 for(int i = 0; i &lt; len - 1; ++i){ for(int j = i + 1; j &lt; len; ++j){ if(j - i + 1 &gt; maxlen &amp;&amp; check(charArray,i,j)){ maxlen = j - i + 1; begin = i; } } } return s.substring(begin,begin+ maxlen); } //检查子串是否为回文串的方法 private boolean check(char[] charArray,int left,int right){ while(left &lt; right){ if(charArray[left] != charArray[right]){ return false; } left++; right--; } return true; }}//中心扩散解法class Solution { public String longestPalindrome(String s) { //获取字符串的长度 int len = s.length(); //判断字符串长度等于1或者0的情况 if(len &lt; 2){ return s; } int maxlen = 1; int begin = 0; //将字符串转换为二维数组 char[] charArray = s.toCharArray(); //开始遍历 找寻所有的中心位置 for(int i = 0; i &lt; len-1; ++i){//右边界无法再向有扩散 int oddLen = expandAroundCenter(charArray,i,i);//子串长度为奇数 int evenLen = expandAroundCenter(charArray,i,i+1);//子串长度为偶数 int curMaxLen = Math.max(oddLen,evenLen); if(curMaxLen &gt; maxlen){ maxlen = curMaxLen; //重新获取begin的起始地点 i为回文串的中心点 maxlen为回文串的长度 //包括了子串长度为奇数和偶数的情况 begin = i - (maxlen - 1)/2; } } return s.substring(begin,begin+ maxlen); } //中心扩散 private int expandAroundCenter(char[] charArray,int left,int right){ int len = charArray.length; int i = left; int j = right; while(i &gt;=0 &amp;&amp; j &lt; len){ if(charArray[i] == charArray[j]){ i--; j++; } else{ break; } } //跳出循环的条件为charArray[i] != charArray[j] //不包含i j 则回文串的长度为j-i+1-2 = j-i-1 return j-i-1; }}3、反转字符串里的单词 你一个字符串 s ，请你反转字符串中 单词 的顺序。 单词 是由非空格字符组成的字符串。s 中使用至少一个空格将字符串中的 单词 分隔开。 返回 单词 顺序颠倒且 单词 之间用单个空格连接的结果字符串。 注意：输入字符串 s中可能会存在前导空格、尾随空格或者单词间的多个空格。返回的结果字符串中，单词间应当仅用单个空格分隔，且不包含任何额外的空格。 1 &lt;= s.length &lt;= 104 s 包含英文大小写字母、数字和空格 ' ' s 中 至少存在一个 单词 解题思路：本题有先翻转字符串，再翻转单词和使用双端队列求解两种解法 先翻转字符串，再翻转单词：（可以自己编写函数实现，或者使用Java内置的API进行实现） 由于Java中的字符串是不可变的，必须先将String类型转化为StringBuilder类型，并同时将原先字符串的首位空格、以及单词间的多余空格去除 先翻转字符串的整体顺序 在依次翻转每一个单词(单词之间以一个空格隔开) 使用双端队列求解 双端队列的定义：是一种具有队列和栈的性质的数据结构，双端队列中的元素可以从双端弹出，其限定插入删除操作在表的两端进行 添加元素到双端队列中：boolean offerFirst(E,e)：在队列的左边添加元素 核心步骤：用StringBuilder装载每一个单词，在将每一个单词在双端队列的左边插入 将双端队列转化为字符串进行返回：return String.join(“ “, d); 代码实现：//先翻转字符串，再翻转单词//自己编写函数实现class Solution { public String reverseWords(String s) { //将String类型 转化为 可以操作的StringBuilder类型 StringBuilder sb = trimSpaces(s); //翻转字符串 reverse(sb,0,sb.length() - 1); //翻转每一个单词 reverseWord(sb); //返回字符串 return sb.toString(); } public StringBuilder trimSpaces(String s){ int left = 0, right = s.length() - 1; //去掉字符串开头的空白字符 while(left &lt;= right &amp;&amp; s.charAt(left) == ' '){ ++left; } //去掉字符串末尾的空白字符 while(left &lt;= right &amp;&amp; s.charAt(right) == ' '){ --right; } //将字符串间多余的空格去除 StringBuilder sb = new StringBuilder(); while(left &lt;= right){ char c = s.charAt(left); //循环字符串，将每一个单词添加到sb中 if(c != ' '){ sb.append(c); } else if(sb.charAt(sb.length() - 1) != ' '){//每个单词之间只添加一个空格 sb.append(c); } ++left; } return sb; } public void reverse(StringBuilder sb, int left, int right){ while(left &lt; right){ char tmp = sb.charAt(left); //进行翻转 记得翻转后 left、right值才会+1 sb.setCharAt(left++,sb.charAt(right)); sb.setCharAt(right--,tmp); } } public void reverseWord(StringBuilder sb){ int n = sb.length(); int start = 0, end = 0; while(start &lt; n){ //循环至单词的末尾 while(end &lt; n &amp;&amp; sb.charAt(end) != ' '){ ++end; } //翻转单词 reverse(sb,start,end - 1); //更新start,去找下一个单词 start = end + 1; ++end; } }}//使用Java内置的API进行实现class Solution { public String reverseWords(String s) { //去除首位空白字符 s = s.trim(); //正则匹配连续的空白字符作为分隔符切割 List&lt;String&gt; wordList = Arrays.asList(s.split(\"\\\\s+\")); Collections.reverse(wordList); return String.join(\" \",wordList); }}-----------------------------------------------------------------------//使用双端队列求解class Solution { public String reverseWords(String s) { int left = 0, right = s.length() - 1; // 去掉字符串开头的空白字符 while (left &lt;= right &amp;&amp; s.charAt(left) == ' ') { ++left; } // 去掉字符串末尾的空白字符 while (left &lt;= right &amp;&amp; s.charAt(right) == ' ') { --right; } //创建双端队列用于存储单词 Deque&lt;String&gt; d = new ArrayDeque&lt;String&gt;(); //创建StringBuilder用于记录每一个单词 StringBuilder word = new StringBuilder(); //循环字符串 while(left &lt;= right){ char c = s.charAt(left); //将每一个单词作为元素放到双端队列中 if((word.length() != 0) &amp;&amp; (c == ' ')){ //将单词push到头部 d.offerFirst(word.toString()); //记得将word转换为字符串 word.setLength(0);//将word长度置零 } else if(c != ' '){ word.append(c); } ++left; } //添加最后一个单词到双端队列中 d.offerFirst(word.toString()); //将队列转换为字符串返回 return String.join(\" \", d); }}4、翻转字符串中的单词III 给定一个字符串 s ，你需要反转字符串中每个单词的字符顺序，同时仍保留空格和单词的初始顺序。 1 &lt;= s.length &lt;= 5 * 104s 包含可打印的 ASCII 字符。s 不包含任何开头或结尾空格。s 里 至少 有一个词。s 中的所有单词都用一个空格隔开解题思路： 由于Java中字符串不可变，先将字符串转化为StringBuilder类型，在进行修改 创建reverseWord()方法依次修改每个单词的顺序(创建双指针进行遍历字符串,start记录单词的头部，end记录单词的尾部) 提取reverse()方法用于交换单词内的字符代码实现：class Solution { public String reverseWords(String s) { //将String类型转化为StringBuilder StringBuilder sb = trimSpaces(s); //翻转每一个单词 reverseWord(sb); //返回字符串 return sb.toString(); } //转化为新的字符串 public StringBuilder trimSpaces(String s){ StringBuilder sb = new StringBuilder(); for(int i = 0; i &lt; s.length(); i++){ char c = s.charAt(i);//获取每一个字符 sb.append(c); } return sb; } //翻转字符 public void reverse(StringBuilder sb, int left, int right){ while(left &lt; right){ char temp = sb.charAt(left); //进行翻转 同时修改left和right的值 sb.setCharAt(left++,sb.charAt(right)); sb.setCharAt(right--,temp); } } //翻转单词 public void reverseWord(StringBuilder sb){ int n = sb.length(); int start = 0, end = 0; while(start &lt; n){ //循环至单词的末尾 while(end &lt; n &amp;&amp; sb.charAt(end) != ' '){ end++; } //翻转单词 reverse(sb,start,end-1); //更新start start = end + 1; ++end; } }}四、KMP算法1、KMP算法简介 1.Knuth–Morris–Pratt（KMP）算法是一种改进的字符串匹配算法，它的核心是利用匹配失败后的信息，尽量减少模式串与主串的匹配次数以达到快速匹配的目的。它的时间复杂度是 O(m+n) 2.实现KMP算法的核心在于构建一个next数组用于记录字符匹配失败后要跳转的位置信息 构建next数组的过程是根据匹配串来实现，与原字符串无关，我们将这一过程称为寻找next点 在构建next数组的过程中要理解字符串前缀和字符串后缀两个概念 eg：以字符串abcba为例子 其前缀为 a、ab、abc、abcb 后缀为a、ba、cba、bcba 两者都不包括其本身 因此 next[i]的值等于其到对应下标字符串的最长公共前缀后缀的长度2、实现strStr() 给你两个字符串 haystack 和 needle ，请你在 haystack 字符串中找出 needle 字符串的第一个匹配项的下标（下标从 0 开始）。如果 needle 不是 haystack 的一部分，则返回 -1 。 1 &lt;= haystack.length, needle.length &lt;= 104 haystack 和 needle 仅由小写英文字符组成 解题思路：本题本质为字符串匹配问题，有暴力迭代和KMP算法实现两种解法，本文主要讲解KMP算法的应用 先为匹配串needle构建next数组，根据匹配串进行构建 利用next数组进行字符串匹配，当字符串不匹配时，利用next数组的信息进行跳转，当匹配成功时，返回第一个匹配项的下标 在字符串匹配过程，原字符串是一直向后遍历的，且在字符串匹配和构建next数组的过程中，都运用了复用已有的有效信息的方法代码实现：class Solution { public int strStr(String haystack, String needle) { //获取两个字符串的长度 int n = haystack.length(), m = needle.length(); //构建next数组 int[] next = new int[m]; //为next数组进行赋值 同时运用到了 找到已有信息进行复用 for(int i = 1, j = 0; i &lt; m; ++i){ while(j &gt; 0 &amp;&amp; needle.charAt(i) != needle.charAt(j)){//j&gt;0则代表前面的字符必定是相等的 j = next[j-1];//根据的前缀的信息进行重新定位 } if(needle.charAt(i) == needle.charAt(j)){ j++; } next[i] = j; } //匹配字符串 找到第一个匹配项的下标 for(int i = 0, j = 0; i &lt; n; ++i){ while(j &gt; 0 &amp;&amp; haystack.charAt(i) != needle.charAt(j)){ j = next[j-1];//根据的前缀的信息进行重新定位 } if(haystack.charAt(i) == needle.charAt(j)){ j++; } if(j == m){ return i-m+1; //返回第一个匹配项的下标 } } return -1; //当两个字符串不存在匹配串的情况 }}" }, { "title": "动态规划详解", "url": "/posts/DP/", "categories": "数据结构与算法", "tags": "学习", "date": "2022-10-02 05:04:00 +0000", "snippet": "动态规划详解一、问题引入​\t\t先来看看生活中经常遇到的事吧——假设您是个土豪，身上带了足够的1、5、10、20、50、100元面值的钞票。现在您的目标是凑出某个金额w，需要用到尽量少的钞票。　　依据生活经验，我们显然可以采取这样的策略：能用100的就尽量用100的，否则尽量用50的……依次类推。在这种策略下，666=6×100+1×50+1×10+1×5+1×1，共使用了10张钞票。　　这种...", "content": "动态规划详解一、问题引入​\t\t先来看看生活中经常遇到的事吧——假设您是个土豪，身上带了足够的1、5、10、20、50、100元面值的钞票。现在您的目标是凑出某个金额w，需要用到尽量少的钞票。　　依据生活经验，我们显然可以采取这样的策略：能用100的就尽量用100的，否则尽量用50的……依次类推。在这种策略下，666=6×100+1×50+1×10+1×5+1×1，共使用了10张钞票。　　这种策略称为“贪心”：假设我们面对的局面是“需要凑出w”，贪心策略会尽快让w变得更小。能让w少100就尽量让它少100，这样我们接下来面对的局面就是凑出w-100。长期的生活经验表明，贪心策略是正确的。　　但是，如果我们换一组钞票的面值，贪心策略就也许不成立了。如果一个奇葩国家的钞票面额分别是1、5、11，那么我们在凑出15的时候，贪心策略会出错：　　15=1×11+4×1 （贪心策略使用了5张钞票）　　15=3×5 （正确的策略，只用3张钞票）　　为什么会这样呢？贪心策略错在了哪里？鼠目寸光　　刚刚已经说过，贪心策略的纲领是：“尽量使接下来面对的w更小”。这样，贪心策略在w=15的局面时，会优先使用11来把w降到4；但是在这个问题中，凑出4的代价是很高的，必须使用4×1。如果使用了5，w会降为10，虽然没有4那么小，但是凑出10只需要两张5元。　　在这里我们发现，贪心是一种只考虑眼前情况**的策略。　　那么，现在我们怎样才能避免鼠目寸光呢？　　如果直接暴力枚举凑出w的方案，明显复杂度过高。太多种方法可以凑出w了，枚举它们的时间是不可承受的。我们现在来尝试找一下性质。　　重新分析刚刚的例子。w=15时，我们如果取11，接下来就面对w=4的情况；如果取5，则接下来面对w=10的情况。我们发现这些问题都有相同的形式：“给定w，凑出w所用的最少钞票是多少张？”接下来，我们用f(n)来表示“凑出n所需的最少钞票数量”。　　那么，如果我们取了11，最后的代价（用掉的钞票总数）是多少呢？　　明显cost = f(4) + 1 = 4 + 1 = 5，它的意义是：利用11来凑出15，付出的代价等于f(4)加上自己这一张钞票。现在我们暂时不管f(4)怎么求出来。　　依次类推，马上可以知道：如果我们用5来凑出15，cost就是 f(10) + 1 = 2 + 1 = 3 。　　那么，现在w=15的时候，我们该取那种钞票呢？当然是各种方案中，cost值最低的那一个！ 取11: cost = f(4) + 1 = 4 + 1 = 5 取5: cost = f(10) + 1 = 2 + 1 = 3 取1: cost = f(14) + 1 = 4 + 1 = 5 ​\t\t显而易见，cost值最低的是取5的方案。我们通过上面三个式子，做出了正确的决策！　　这给了我们一个至关重要的启示——f(n) 只与 f(n-1),f(n-5),f(n-11) 相关，更准确地说 f(n) = min{ f(n-1) ,f(n-5), f(n-11) } + 1​\t\t这个式子是非常激动人心的。我们要求出f(n)，只需要求出几个更小的f值；既然如此，我们从小到大把所有的f(i)求出来不就好了？注意一下边界情况即可。总结原理​\t\t我们已 O(n) 的复杂度解决了这个问题，现在回过头来总结： f(n) 只与 f(n-1),f(n-5),f(n-11) 的值相关 我们只关心 f(w) 的值，不关心是怎么凑出来的 ​\t\t它与暴力的区别在哪里？我们的暴力枚举了“使用的硬币”，然而这属于冗余信息。我们要的是答案，根本不关心这个答案是怎么凑出来的。譬如，要求出f(15)，只需要知道f(14),f(10),f(4)的值。其他信息并不需要。我们舍弃了冗余信息。我们只记录了对解决问题有帮助的信息——f(n).　　我们能这样干，取决于问题的性质：求出f(n)，只需要知道几个更小的f(c)。我们将求解f(c)称作求解f(n)的“子问题”。　　这就是DP（动态规划，dynamic programming）.　　将一个问题拆成几个子问题，分别求解这些子问题，即可推断出大问题的解。二、相关概念1、无后效性 一旦f(n)确定，“我们如何凑出f(n)”就再也用不着了。 要求出f(15)，只需要知道f(14),f(10),f(4)的值，而f(14),f(10),f(4)是如何算出来的，对之后的问题没有影响。 “未来与过去无关”，这就是无后效性。 （严格定义：如果给定某一阶段的状态，则在这一阶段以后过程的发展不受这阶段以前各段状态的影响。）2、最优子结构 回顾我们对f(n)的定义：我们记“凑出n所需的最少钞票数量”为f(n). ​\t\tf(n)的定义就已经蕴含了“最优”。利用w=14,10,4的最优解，我们即可算出w=15的最优解。 ​\t\t大问题的最优解可以由小问题的最优解推出，这个性质叫做“最优子结构性质”。 引入这两个概念之后，我们如何判断一个问题能否使用DP解决呢？ 　　能将大问题拆成几个小问题，且满足无后效性、最优子结构性质。三、DP的应用最长回文子串 给你一个字符串 s，找到 s 中最长的回文子串 1 &lt;= s.length &lt;= 1000 s 仅由数字和英文字母组成 解题思路：​\t\t首先要了解回文串的定义：是一种特殊的字符串，从左往右读和从右往左读是一样的，本题的要求则是找出给定字符串中最长的回文串。​\t\t采用动态规划的方法进行求解，动态规划的本质就是：能将大问题拆成几个小问题，且满足无后效性、最优子结构性质，且分别求解这些子问题，即可以推断出大问题的解。在本题中，当回文子串的长度大于2时，那么将它首尾的两个字母去除之后，它仍然是个回文串。即我们可以得出：当一个子串的长度大于2时，它本身是否为回文串，取决于它去掉首位字符后的子串是否为回文串。​\t\t我们用 dp(i,j) 表示字符串s的i到j个字母组成的串是否为回文串,Si表示字符串的第i个字符： dp(i,j) = true 如果子串Si…Sj是回文串 dp(i,j) = false 其他情况( 包括①dp(i,j) 本身不是一个回文串，②i&gt;j,此时dp(i,j) 本身不合法 )那么此时我们就可以写出本题的动态规划转移方程： dp(i,j) = (Si == Sj) and dp(i+1,j-1)上文的所有讨论是建立在子串长度大于 2 的前提之上的，我们还需要考虑动态规划中的边界条件，即子串的长度为 1 或 2，给出边界条件下的动态规划转移方程： dp(i,i) = true \t\t\t长度为1，它显示回文串 dp(i,i+1) = (Si == Si+1) \t 长度为2，只需要它的两个字符相同，它就是一个回文串根据这个思路，我们就可以完成动态规划了，最终的答案即为所有dp(i,j)= true 中 j-1+1（即子串长度）的最大值。注意：在状态转移方程中，我们是从长度较短的字符串向长度较长的字符串进行转移的，因此一定要注意动态规划的循环顺序。代码实现：class Solution { public String longestPalindrome(String s) { //获取字符串的长度 int len = s.length(); //判断当字符串长度等于1或0的情况 if(len &lt; 2){ return s; } int maxLen = 1; //回文子串的最长度 初始化为1 int begin = 0; //回文子串的初始下标 //创建一个二维数组用于记录是否为回文串 boolean[][] dp = new boolean[len][len]; //初始化:所有长度为1的子串都是回文串,即对角线上的元素都为true for(int i = 0; i &lt; len; ++i){ dp[i][i] = true; } //将字符串对象中的字符转换为一个字符数组 char[] charArray = s.toCharArray(); //递推开始,先枚举子串的长度 for(int L = 2; L &lt;= len; ++L){ //枚举左边界,左边界的上限可以设置的宽松一点 for(int i = 0; i &lt; len; i++){ //可以由 L 和 i 来确定右边界, j - i + 1 = L 得 int j = L + i - 1; //如何j超出字符串数组的长度,则跳出该循环 if(j &gt;= len){ break; } //进行判断,左右边界的值是否相等 if(charArray[i] != charArray[j]){//不相等的情况 dp[i][j] = false; } else{//相等的情况 if(L == 2){//回文串长度为2的情况，即L == 2 dp[i][j] = true; } else{//其他情况 dp[i][j] = dp[i+1][j-1]; } } //只要dp[i][L] == true 成立,就表示子串s[i...L]是回文，此时记录回文长度和起始位置 if(dp[i][j] &amp;&amp; j - i + 1 &gt; maxLen){ //重新更新maLen和begin的值 maxLen = j - i + 1; begin = i; } } } return s.substring(begin,begin+maxLen);//截取字符串 }}" }, { "title": "数据结构--数组", "url": "/posts/Array/", "categories": "数据结构与算法", "tags": "学习", "date": "2022-09-24 04:30:00 +0000", "snippet": "数据结构–数组一、数组简介 数组是存放在连续内存空间上的相同类型数据的集合 数组是非常基础的数据结构，在面试中，考察数组的题目一般在思维上都不难，主要是考察对代码的掌控能力 数组可以方便的通过下标索引的方式获取到下标下对应的数据二、数组的类型数组主要分为两种类型： 一维数组 二维数组二维数组的结构如下图所示：三、数组的存储1、一维数组 数组下标都是从0开始的 数组内...", "content": "数据结构–数组一、数组简介 数组是存放在连续内存空间上的相同类型数据的集合 数组是非常基础的数据结构，在面试中，考察数组的题目一般在思维上都不难，主要是考察对代码的掌控能力 数组可以方便的通过下标索引的方式获取到下标下对应的数据二、数组的类型数组主要分为两种类型： 一维数组 二维数组二维数组的结构如下图所示：三、数组的存储1、一维数组 数组下标都是从0开始的 数组内存空间的地址是连续的 数组的元素是不能删除的,只能覆盖 正是因为数组的在内存空间的地址是连续的，所以我们在删除或者增添元素的时候，就难免要移动其他元素的地址。 例如删除下标为3的元素，需要对下标为3的元素后面的所有元素都要做移动操作，如图所示：2、二维数组二维数组在内存的存储空间不一定是连续的，不同编程语言的内存管理是不一样的，如：在C++中二维数组是连续分布，而在Java中二维数组不一样的是连续的。 Java的二维数组可能是如下排列的方式：四、经典问题1、寻找数组的中心索引 给你一个整数数组 nums ，请计算数组的 中心下标 。 数组 中心下标 是数组的一个下标，其左侧所有元素相加的和等于右侧所有元素相加的和。 如果中心下标位于数组最左端，那么左侧数之和视为 0 ，因为在下标的左侧不存在元素。这一点对于中心下标位于数组最右端同样适用。 如果数组有多个中心下标，应该返回 最靠近左边 的那一个。如果数组不存在中心下标，返回 -1 。解题思路： 当数组存在中心下标时，由其性质我们可得：左侧元素之和 + 中心下标元素值 + 右侧元素之和 = 数组的总元素之和 获取数组之和的api： Arrays.stream(nums).sum; 以流的方式获取数组的元素之和代码实现：//采用前缀和的方法class Solution { public int pivotIndex(int[] nums) { //先获取数组值的总和 int total = Arrays.stream(nums).sum(); //定义变量用于存储数值的前缀和 int sum = 0; for(int i = 0; i &lt; nums.length; ++i){ if(2 * sum + nums[i] == total){ return i; } sum += nums[i]; } return -1; }}2、搜索插入位置 给定一个排序数组和一个目标值，在数组中找到目标值，并返回其索引。如果目标值不存在于数组中，返回它将会被按顺序插入的位置。 请必须使用时间复杂度为 O(log n) 的算法。 1 &lt;= nums.length &lt;= 104-104 &lt;= nums[i] &lt;= 104nums 为 无重复元素 的 升序 排列数组-104 &lt;= target &lt;= 104解题思路： 本题采用迭代法和二分查找法进行实现 迭代法： 根据题目的介绍，我们可以结果具体分为两种情况：①目标值存在数组中，返回其对应的下标;②目标值不存在数组中,返回其按顺序在数组中的位置(此时目标可能在三种位置：数组头部、数组中间、数组尾部) 对上述情况一一区分，使用编程的语言表述出来即可 &amp;&amp;运算符的特性：当第一个条件的值为false时，直接返回false，不对后面的值进行判断 二分查找法： 二分查找法的条件为：数组必须为升序、不重复，而本题刚好符合要求 比特运算：(right-left) » 1 等价于 (right-left) / 2，且能够防止出现溢出风险，且可以向下取整 二分查找的截止条件为：左下标left &gt; 右下标right 代码实现：//使用迭代法进查找class Solution { public int searchInsert(int[] nums, int target) { //判断目标值小于数组头元素的情况 if(target &lt; nums[0]){ return 0; } for(int i = 0; i &lt; nums.length; ++i){ if(nums[i] == target){ return i; } else{ //判断目标值在数组内的情况,记得&amp;&amp;运算符的特性，防止出现空指针异常 if(i+1 &lt; nums.length &amp;&amp; target &gt; nums[i] &amp;&amp; target &lt; nums[i+1]){ return i+1; } } } //如果以上情况都未成立,则目标值大于数组的最后一位元素 return nums.length; }}//使用二分法进行查找class Solution { public int searchInsert(int[] nums, int target) { int n = nums.length; int left = 0, right = n - 1, ans = n; while (left &lt;= right) { int mid = ((right - left) &gt;&gt; 1) + left;//求出中心点，如何为小数点，则向下取整 (left + right)//2 if (target &lt;= nums[mid]) { ans = mid; right = mid - 1; } else { left = mid + 1; } } return ans; }}3、合并区间 以数组 intervals 表示若干个区间的集合，其中单个区间为 intervals[i] = [starti, endi] 。请你合并所有重叠的区间，并返回 一个不重叠的区间数组，该数组需恰好覆盖输入中的所有区间 1 &lt;= intervals.length &lt;= 104intervals[i].length == 20 &lt;= starti &lt;= endi &lt;= 104解题思路： 创建一个List集合用于接受结果若干个区间数组 List&lt;int[]&gt; merged 将原先数组 intervals 的若干个区间数组按左端点的大小进行排序，可以合并的区间一定是连续的 将原先数组的第一个区间放进结果集合，同时遍历原先数组intervals，通过判断结果集合中最后一个数组的右元素与原先数组intervals当前数组的左元素大小，实现更新 Array.sort() 方法默认是进行升序不区分大小写排序的，当我们需要排列的集合或者数组不是单纯的数字型时，可以使用 **new Comparator(){}** 进行自定义排序 List.toArray()方法：将List数组转换为二维数组代码实现：class Solution { public int[][] merge(int[][] intervals) { //判断该数组为空的情况 if(intervals.length == 0){ return new int[0][2]; } //根据区间的左端点，对原先区间进行排序 Arrays.sort(intervals, new Comparator&lt;int[]&gt;(){ public int compare(int[] intervals1, int[] intervals2){ return intervals1[0] - intervals2[0]; } }); //创建用于返回结果的merged集合 List&lt;int[]&gt; merged = new ArrayList&lt;int[]&gt;(); //依次进行判断 for(int i = 0; i &lt; intervals.length; ++i){ //获取当前区间的左值和右值 int L = intervals[i][0], R = intervals[i][1]; //当 merged内最后一个区间的 小于 右值区间的左值时 if(merged.size() == 0 || merged.get(merged.size()-1)[1] &lt; L){ merged.add(new int[]{L,R}); } else{ //当 merged内最后一个区间的 大于 右值区间的左值 merged.get(merged.size() - 1)[1] = Math.max(merged.get(merged.size() - 1)[1], R); } } return merged.toArray(new int[merged.size()][]); //返回merged数组 }}4、旋转矩阵 给你一幅由 N × N 矩阵表示的图像，其中每个像素的大小为 4 字节。请你设计一种算法，将图像旋转 90 度。 不占用额外内存空间能否做到？解题思路： 本题可以采用①辅助数组进行翻转方法或者②先水平翻转后对角线翻转的方法 辅助数组进行翻转方法： 关键在于找到矩阵旋转的规律：旋转前(i,j) 旋转后(j,n-i-1) 同时使用一个辅助数组进行暂时存储旋转后的数据，最后在重新赋值给原先的数组 先水平翻转后对角线翻转的方法： 以先水平翻转后对角线翻转的方法达到旋转矩阵 水平翻转的规律：翻转前(i,j) 翻转后(n-i-1,j) 对角线翻转的规律： 翻转前(i,j) 翻转后(j,i) 代码实现：//①创建辅助数组进行反转class Solution { public void rotate(int[][] matrix) { //获取二维数组的长度 int n = matrix.length; //创建辅助的二维数组 int[][] matrix_new = new int[n][n]; //使用新的数组用于存储翻转后的数组 for(int i = 0; i &lt; n; i++){ for(int j = 0; j &lt; n; j++){ matrix_new[j][n-i-1] = matrix[i][j]; } } //将新的数组赋值给旧的数组 for(int i = 0; i &lt; n; i++){ for(int j = 0; j &lt; n; j++){ matrix[i][j] = matrix_new[i][j]; } } }}//②先水平翻转，在进行主对角线翻转class Solution { public void rotate(int[][] matrix) { //先让数组进行水平翻转，在进行对角线翻转，即可达到旋转后的效果 int n = matrix.length; //先进行水平翻转 for(int i = 0; i &lt; n/2; ++i){//注意行的截止条件 for(int j = 0; j &lt; n; ++j){ int temp = matrix[i][j]; //创建临时变量用于存储原先的值 matrix[i][j] = matrix[n-i-1][j]; //水平翻转的条件 matrix[n-i-1][j] = temp; } } //进行对角线翻转 for(int i = 0; i &lt; n; ++i){ for(int j = 0; j &lt; i; ++j){//注意列的截止条件 int temp = matrix[i][j]; matrix[i][j] = matrix[j][i]; //主对角线翻转的条件 matrix[j][i] = temp; } } }}5、零矩阵 编写一种算法，若M × N矩阵中某个元素为0，则将其所在的行与列清零解题思路： 本题可以创建两个标记数组进行求解 或者 创建两个标记变量进行求解 创建两个标记数组进行求解： 创建的数组用于记录原先二维数组的哪一行哪一列存在0 先遍历原先数组用于记录哪里存在0，在遍历并根据标记数组将所对应的行与列进行清零 创建两个标记变量进行求解： 此解法的核心是用原先数组的第一行与第一列代替第一个方法中的两个标记数组，但是会对原先的第一行与第一列造成修改，因此我们创建两个标记变量用于记录第一行和第一列是否存在0 代码实现：//使用两个标记数组class Solution { public void setZeroes(int[][] matrix) { //获取二维数组行和列的长度 int m = matrix.length, n = matrix[0].length; //创建两个二维数组用于记录每一行、每一列是否存在0 boolean[] row = new boolean[m]; boolean[] col = new boolean[n]; //判断数组的每一行 每一列是否存在0 for(int i = 0; i &lt; m; ++i){ for(int j = 0; j &lt; n; ++j){ if(matrix[i][j] == 0){ row[i] = col[j] = true; } } } //将所在的行与列进行清零 for(int i = 0; i &lt; m; ++i){ for(int j = 0; j &lt; n; ++j){ if(row[i] || col[j] ){//为true则代表某一行某一列为0 matrix[i][j] = 0; } } } }}//使用两个标记变量class Solution { public void setZeroes(int[][] matrix) { //获取二维数组行和列的长度 int m = matrix.length, n = matrix[0].length; //创建两个标记变量用于标记第一行第一列本身是否具有0 boolean flagCol0 = false, falgRow0 = false; //判断第一列本身是否具有0 for(int i = 0; i &lt; m; ++i){ if(matrix[i][0] == 0){ flagCol0 = true; } } //判断第一行本身是否具有0 for(int j = 0; j &lt; n; ++j){ if(matrix[0][j] == 0){ falgRow0 = true; } } //使用第一行和第一列来标记数组其他部分是否具有0 for(int i = 1; i &lt; m; ++i){//切记i j的初始值为1 for(int j = 1; j &lt; n; ++j){ if(matrix[i][j] == 0){ matrix[i][0] = matrix[0][j] = 0; } } } //根据第一行和第一列来实现行与列清零 for(int i = 1; i &lt; m; ++i){ for(int j = 1; j &lt; n; ++j){ //包括本身第一行和第一列具有的，也包括上一轮刚标记的 if(matrix[i][0] == 0 || matrix[0][j] == 0){ matrix[i][j] = 0; } } } //判断第一行自身是否具有零 有的话实现清零 if(falgRow0){ for(int j = 0; j &lt; n; ++j){ matrix[0][j] = 0; } } //判断第一列自身是否具有零 有的话实现清零 if(flagCol0){ for(int i = 0; i &lt; m; ++i){ matrix[i][0] = 0; } } }}6、对角线遍历 给你一个大小为 m x n 的矩阵 mat ，请以对角线遍历的顺序，用一个数组返回这个矩阵中的所有元素。解题思路： 先获取要遍历的对角线的个数：m+n-1 （m,n分别为二维数组的行与列的个数） 根据对角线的遍历方向来分情况讨论 i为奇数，对角线从上往下遍历 i为偶数，对角线从下往上遍历 创建一个一维数组用于来存储对角线遍历后的结果、 确实每一条边角线遍历的起点： i为奇数(从上往下): i&lt;n (0,i) i&gt;n (i-n-1,n-1) i为偶数(从下往上) i&lt;m (i,0) i&gt;m (m-1,i-m+1) 确定遍历的截止条件： i为奇数： x &lt; m &amp;&amp; y &gt;= 0 i为偶数： x &gt;= 0 &amp;&amp; y &lt; n 代码实现：class Solution { public int[] findDiagonalOrder(int[][] mat) { //获取二维数组行与列的长度 int m = mat.length; int n = mat[0].length; //创建一个新的数组用于存储遍历结果 int[] res = new int[m * n]; //创建变量用于记录结果数组的下标 int pos = 0; //对所有的对角线进行遍历 对角线的总数为 m+n-1 for(int i = 0; i &lt; m+n-1; ++i){ if(i % 2 == 1){//当i为奇数时,对角线从上往下遍历 //获取在该对角线上的初始位置 int x = i &lt; n ? 0 : i - n + 1; int y = i &lt; n ? i : n - 1; //遍历的截止条件 while(x &lt; m &amp;&amp; y &gt;= 0){ res[pos] = mat[x][y]; pos++; x++; y--; } } else{//当i为偶数时,对角线从下往上遍历 //获取在该对角线上的初始位置 int x = i &lt; m ? i : m-1;//判断该对角线是否超过原先 int y = i &lt; m ? 0 : i-m+1; //遍历的截止条件 while( x &gt;= 0 &amp;&amp; y &lt; n){ res[pos] = mat[x][y]; pos++; x--; y++; } } } return res; }}" }, { "title": "JWT介绍", "url": "/posts/JWT/", "categories": "技术科普", "tags": "学习", "date": "2022-09-18 11:30:00 +0000", "snippet": "JWT介绍一、JWT简介​\tJWT，全名：Json Web Token，是一种开放标准， 它定义了一种以紧凑和自包含的方法，用于在双方之间安全地传输编码为 JSON 对象的信息。​\t因此，简单来说，它是 JSON 格式的加密字符串，其中包含敏感信息，它使我们能够验证不同服务间的发送者。二、JWT的使用场景 授权： 这是使用 JWT 最常见的场景。JWT 用于授权而非身份验证。通过身份验证，...", "content": "JWT介绍一、JWT简介​\tJWT，全名：Json Web Token，是一种开放标准， 它定义了一种以紧凑和自包含的方法，用于在双方之间安全地传输编码为 JSON 对象的信息。​\t因此，简单来说，它是 JSON 格式的加密字符串，其中包含敏感信息，它使我们能够验证不同服务间的发送者。二、JWT的使用场景 授权： 这是使用 JWT 最常见的场景。JWT 用于授权而非身份验证。通过身份验证，我们验证用户名和密码是否有效，并将用户登录到系统中。通过授权，我们可以验证发送到服务器的请求是否属于通过身份验证登录的用户，从而可以授予该用户访问系统的权限，继而批准该用户使用获得的 token 访问路由、服务和资源 信息交换：SON Web Token 是在双方之间安全地传输信息的一种好方法。因为 JWT 可以被签名（例如，使用公钥/私钥对），所以使您能确保发送方是他们所声称的那一方。此外，由于签名是使用 Header 和 Payload 计算的，因此还使您能验证发送的内容没有被篡改三、JWT与Session Id的比较1、小型WEB应用程序1.Session Id实现​\t在传统的 Web 应用程序中，我们使用 Session 来授权用户，当用户登录到应用程序后，我们会为该用户分配一个唯一的 Session Id。我们将此 Session Id 保存在用户浏览器的安全 cookie 中和服务器的内存中。我们对每个请求都使用相同的会话，以便服务器知道该用户已通过身份验证。对于每个请求，cookie 中的 Session Id 都会与服务器内存中的 Session Id 作匹配，以验证用户是否被授权。2.JWT实现​\t在 JWT 实现中，我们使用 JWT 授权用户，当用户登录到应用程序后，就会为每个通过身份验证的用户生成一个唯一的 JWT。我们将该 token 保存在浏览器的 local storage 或者 cookie 中，而不会在服务器端保存任何内容。对于每个请求，该 token 都会被发送到服务器进行解密和验证，以核实该用户是否已授权，不管以何种方式篡改了 token 都会被拒绝。这种实现对于小型站点来说很好，仅仅因为我们不再存储 Session Id，从而通过减少服务器的负载，我们已经从 JWT 中看到了一些好处2、高级WEB应用程序1.Session Id实现​\t当我们的WEB应用逐渐火热，我们需要有一台连接到负载均衡器的新服务器，以便基于流量和可用性在 Web 服务器之间导航流量。这种实现给我们带来了一个新的问题，如下所示：如果用户 1 登录到了服务器 1，那么服务器 1 已经将 session 保存在其内存中，当用户 1 发出另一个请求并且负载均衡器将该请求重定向到了服务器 2，而服务器 2 没有保存该 session 信息，这时会发生什么情况？用户将被认为已退出应用程序并被要求再次登录，这不是一个好的用户体验。通常，我们解决这个问题的方法是引入缓存：​\t现在，所有的 Session 也将同时保存在缓存中，因此任何一台服务器都可以检查该 Session 是否存在，并可以利用它来验证用户并授予他们对应用程序的访问权限。尽管缓存解决了我们的问题，但是在生产环境中，这种解决方案有着昂贵的成本： 需要大量的 RAM、CPU、存储来跟踪所有这些会话和平稳地处理请求 需要维护缓存，以确保没有幽灵会话或无效会话 万一某台服务器崩溃，所有未与缓存同步的会话都会丢失 使用户无效更复杂 托管成本高2.JWT实现​\t让我们来看看如何通过 JWT 实现来处理相同的情况。​\t不同于在 Cookie 中使用 Session Id 与服务器内存中的 Session 作匹配；我们可以使用 JWT 来代替它。此时，当用户登录到我们的应用程序时，服务器将不会生成 Session Id 并将其保存在内存中，而是会创建一个 JWT token，并对其进行编码和序列化，然后使用自己的加密机制对其进行签名。通过这种方式，服务将知道一旦对它做了变更或篡改，便将其变为无效。由于通过服务器的加密机制对其进行了签名，所以这是可以被检验的。​\t使用 JWT 可以更容易地管理可伸缩性，因为我们不需要服务器来处理任何会话检查或缓存检查。请求可以转发到负载均衡器为其分配的任一服务器，而无需担心会话的可用性。万一某台服务器宕机，所有的 token 将仍然有效，因为所有服务器上的加密机制是一样的。3.JWT与Session Id的区别总结JWT： 服务器上不保存任何东西，JWT 存储于客户端中 由服务器加密和签名 token 包含用户的所有信息 所有信息都存储于 token 本身中 易于缩放Session Id Session Id 保存于服务器和客户端中 加密并签名 Session Id 是对用户的引用 服务器需要查找用户并进行必要的检查 难以缩放四、JWT结构JSON Web Token 由三部分组成，以点（.）分隔，分别是： Header（标头） Payload（有效负载） Signature（签名）因此，JWT 通常如下所示：xxxxxx.yyyyyyy.zzzzzzzz" }, { "title": "数据结构--二叉树", "url": "/posts/Tree/", "categories": "数据结构与算法", "tags": "学习", "date": "2022-09-18 03:30:00 +0000", "snippet": "数据结构–二叉树一、二叉树简介 树 是一种经常用到的数据结构，用来模拟具有树状结构性质的数据集合 树里的每一个节点有一个值和一个包含所有子节点的列表。从图的观点来看，树也可视为一个拥有N 个节点和N-1 条边的一个有向无环图 二叉树是一种更为典型的树状结构。如它名字所描述的那样，二叉树是每个节点最多有两个子树的树结构，通常子树被称作“左子树”和“右子树”二、二叉树的种类1、满二叉树 ...", "content": "数据结构–二叉树一、二叉树简介 树 是一种经常用到的数据结构，用来模拟具有树状结构性质的数据集合 树里的每一个节点有一个值和一个包含所有子节点的列表。从图的观点来看，树也可视为一个拥有N 个节点和N-1 条边的一个有向无环图 二叉树是一种更为典型的树状结构。如它名字所描述的那样，二叉树是每个节点最多有两个子树的树结构，通常子树被称作“左子树”和“右子树”二、二叉树的种类1、满二叉树 满二叉树：如果一棵二叉树只有度为0的结点和度为2的结点，并且度为0的结点在同一层上，则这棵二叉树为满二叉树 二叉树结点的度数指该结点所含子树的个数，二叉树结点子树个数最多的那个结点的度为二叉树的度 这棵二叉树为满二叉树，也可以说深度为k，有2^k-1个节点的二叉树2、完全二叉树 完全二叉树的定义如下：在完全二叉树中，除了最底层节点可能没填满外，其余每层节点数都达到最大值，并且最下面一层的节点都集中在该层最左边的若干位置。若最底层为第 h 层，则该层包含 1~ 2^(h-1) 个节点 满二叉树是完全二叉树的子集 了解完全二叉树的定义后对下面二叉树进行判断3、二叉搜索树 前面介绍的树，都没有数值的，而二叉搜索树是有数值的了，二叉搜索树是一个有序树 性质： 若它的左子树不空，则左子树上所有结点的值均小于它的根结点的值； 若它的右子树不空，则右子树上所有结点的值均大于它的根结点的值； 它的左、右子树也分别为二叉排序树 下面这两棵树都是二叉搜索树4、平衡二叉搜索树 平衡二叉搜索树：又被称为AVL（Adelson-Velsky and Landis）树，且具有以下性质：它是一棵空树或它的左右两个子树的高度差的绝对值不超过1，并且左右两个子树都是一棵平衡二叉树 可以理解为：在二叉搜索树的基础下，它的左右两个子树的高度差的绝对值不超过1 最后一棵 不是平衡二叉树，因为它的左右两个子树的高度差的绝对值超过了1三、二叉树的存储方式 二叉树可以链式存储，也可以顺序存储 链式存储方式由指针实现， 顺序存储的方式由数组实现 顺序存储的遍历：如果父节点的数组下标是 i，那么它的左孩子就是 i * 2 + 1，右孩子就是 i * 2 + 2 但是用链式表示的二叉树，更有利于我们理解，所以一般我们都是用链式存储二叉树 顾名思义就是顺序存储的元素在内存是连续分布的，而链式存储则是通过指针把分布在散落在各个地址的节点串联一起 如下图所示链式存储：顺序存储：四、二叉树的定义 我们在刷leetcode的时候，节点的定义默认都定义好了，真到面试的时候，需要自己写节点定义，简单的数据结构定义要牢记于心二叉树的定义如下：public class TreeNode { int val; TreeNode left; TreeNode right; TreeNode() {} TreeNode(int val) { this.val = val; } \tTreeNode(int val, TreeNode left, TreeNode right) { this.val = val; this.left = left; this.right = right; }}五、二叉树的遍历方式 二叉树主要有两种遍历方式： 深度优先遍历：先往深走，遇到叶子节点再往回走 广度优先遍历：一层一层的去遍历 而深度优先遍历和广度优先遍历可以进一步拓展，如下所示 深度优先遍历： 前序遍历 中序遍历 后序遍历 广度优先遍历： 层序遍历 在深度优先遍历中：有三个顺序，前中后序遍历，这里前中后，其实指的就是中间节点的遍历顺序，只要大家记住 前中后序指的就是中间节点的位置就可以了1、前序遍历 给你二叉树的根节点 root ，返回它节点值的 前序 遍历 树中节点数目在范围 [0, 100] 内 -100 &lt;= Node.val &lt;= 100 解题思路： 本题可以采用递归法、简单的迭代法、统一的迭代法进行实现 递归法的关键在于：①确定递归方法的返回值与参数、②截止条件和③单层逻辑 ①：根据题目的要求创建一个List集合用于接受前序遍历的值，而我们的递归方法可以将前序遍历的值依次放进List集合中，因而我们方法的参数为：二叉树的根节点、List集合 ②：通过递归方法，当遍历到当前节点为空，则直接返回上一层方法 ③：根据当前遍历方式的特点，在进行编写对应的递归逻辑 简单的迭代法：借助数据结构栈来遍历二叉树，依次将数据进行入栈和出栈，注意栈的特点是先进后出 判断依据是：当前节点的左右节点是否为空 统一的迭代法：借助数据结构栈遍历二叉树同时使用空节点指针进行标记，实现前中后序统一风格的迭代方法 判断依据是：当前节点是否为空代码实现： //递归法实现class Solution {    public List&lt;Integer&gt; preorderTraversal(TreeNode root) {        List&lt;Integer&gt; list= new ArrayList&lt;Integer&gt;();        preorder(root,list);        return list;    }    public void preorder(TreeNode root,List&lt;Integer&gt; list){        if(root == null){            return; //返回上一层方法        }        list.add(root.val); //将节点值添加到列表中        preorder(root.left,list);        preorder(root.right,list);    }}//简单的迭代法实现 //前序遍历顺序：中-左-右，入栈顺序：中-右-左class Solution { public List&lt;Integer&gt; preorderTraversal(TreeNode root) { List&lt;Integer&gt; result = new ArrayList&lt;&gt;(); if (root == null){ return result; } //创建数据结构栈来操作二叉树的节点值 Stack&lt;TreeNode&gt; stack = new Stack&lt;&gt;(); stack.push(root); while (!stack.isEmpty()){ TreeNode node = stack.pop(); result.add(node.val); if (node.right != null){ stack.push(node.right); } if (node.left != null){ stack.push(node.left); } } return result; }}//统一的迭代法实现class Solution {    public List&lt;Integer&gt; inorderTraversal(TreeNode root) {        List&lt;Integer&gt; result = new LinkedList&lt;&gt;();        Stack&lt;TreeNode&gt; stack = new Stack&lt;&gt;();        if(root != null) stack.push(root);        while(!stack.isEmpty()){            TreeNode node = stack.peek();            if(node != null){//栈顶节点不为空的情况 核心步骤                stack.pop();//弹出当前节点 重新将节点按遍历顺序压栈                //前序遍历：右 左 中                if(node.right != null) stack.push(node.right); if(node.left != null) stack.push(node.left);                stack.push(node);                stack.push(null);//中节点访问过，但是没有处理，加入空指针进行标记            }            else{//栈顶节点为空的情况                stack.pop();                node = stack.peek();                stack.pop();                result.add(node.val);            }        }        return result;    }}2、中序遍历 给定一个二叉树的根节点 root ，返回 它的 中序 遍历 。 树中节点数目在范围 [0, 100] 内 -100 &lt;= Node.val &lt;= 100 解题思路: 中序遍历同样具有三种解题方式 递归法：与前序遍历思路一致，不多赘述 普通的迭代法：由于中序遍历的特点，导致我们使用栈遍历时，访问节点与操作节点的顺序不一致，因而我们引入一个当前指针，共同遍历二叉树 ：while (cur != null   !stack.isEmpty())， 判断条件为：当前指针指向的节点是否为空 ，不为空则指向其左节点，为空则获取栈顶的元素并加入List集合，在而指向其右节点 统一的迭代法：与前序遍历思路一直，不多赘述代码实现：//递归法class Solution {    public List&lt;Integer&gt; inorderTraversal(TreeNode root) {        List&lt;Integer&gt; list= new ArrayList&lt;Integer&gt;();        inorder(root,list);        return list;    }    public void inorder(TreeNode root,List&lt;Integer&gt; list){        if(root == null){            return;        }        inorder(root.left,list);        list.add(root.val);  //将节点值添加到列表中        inorder(root.right,list);    }}//普通的迭代方法（同时使用栈遍历和指针遍历）// 中序遍历顺序: 左-中-右 入栈顺序： 左-右class Solution { public List&lt;Integer&gt; inorderTraversal(TreeNode root) { List&lt;Integer&gt; result = new ArrayList&lt;&gt;(); if (root == null){ return result; } Stack&lt;TreeNode&gt; stack = new Stack&lt;&gt;(); TreeNode cur = root; while (cur != null || !stack.isEmpty()){ if (cur != null){ stack.push(cur); cur = cur.left; }else{ cur = stack.pop(); result.add(cur.val); cur = cur.right; } } return result; }}//统一的迭代方法class Solution {    public List&lt;Integer&gt; inorderTraversal(TreeNode root) {        List&lt;Integer&gt; result = new LinkedList&lt;&gt;();        Stack&lt;TreeNode&gt; stack = new Stack&lt;&gt;();        if(root != null) stack.push(root);        while(!stack.isEmpty()){            TreeNode node = stack.peek();            if(node != null){//栈顶节点不为空的情况 核心步骤                stack.pop();//弹出当前节点 重新将节点按遍历顺序压栈                //中序遍历：右 中 左                if(node.right != null) stack.push(node.right);                stack.push(node);                stack.push(null);//中节点访问过，但是没有处理，加入空指针进行标记                if(node.left != null) stack.push(node.left);            }            else{//栈顶节点为空的情况                stack.pop();                node = stack.peek();                stack.pop();                result.add(node.val);            }        }        return result;    }}3、后序遍历 给你一棵二叉树的根节点 root ，返回其节点值的 后序遍历 树中节点的数目在范围 [0, 100] 内 -100 &lt;= Node.val &lt;= 100 解题思路： 后序遍历同样具有三种解题方式 递归法：与前序遍历思路一致，不多赘述 简单的迭代法：根据后序遍历的特点，我们可以对前序遍历的简单迭代法稍作修改，修改左右子节点的入栈顺序，得到：中右左的遍历结果 在对结果进行翻转即可得到：左右中的遍历结果 统一的迭代法：与前序遍历思路一直，不多赘述代码实现：//递归法实现class Solution {    public List&lt;Integer&gt; postorderTraversal(TreeNode root) {        List&lt;Integer&gt; list= new ArrayList&lt;Integer&gt;();        postorder(root,list);        return list;    }    public void postorder(TreeNode root,List&lt;Integer&gt; list){        if(root == null){            return;        }        postorder(root.left,list);        postorder(root.right,list);        list.add(root.val); //将节点值添加到列表中    }}//简单的迭代法// 后序遍历顺序 左-右-中 入栈顺序：中-左-右 出栈顺序：中-右-左， 最后翻转结果class Solution { public List&lt;Integer&gt; postorderTraversal(TreeNode root) { List&lt;Integer&gt; result = new ArrayList&lt;&gt;(); if (root == null){ return result; } //创建数据结构栈来操作二叉树的节点值 Stack&lt;TreeNode&gt; stack = new Stack&lt;&gt;(); stack.push(root); while (!stack.isEmpty()){ TreeNode node = stack.pop(); result.add(node.val); if (node.left != null){ stack.push(node.left); } if (node.right != null){ stack.push(node.right); } } Collections.reverse(result); return result; }}//统一的迭代法实现class Solution {    public List&lt;Integer&gt; inorderTraversal(TreeNode root) {        List&lt;Integer&gt; result = new LinkedList&lt;&gt;();        Stack&lt;TreeNode&gt; stack = new Stack&lt;&gt;();        if(root != null) stack.push(root);        while(!stack.isEmpty()){            TreeNode node = stack.peek();            if(node != null){//栈顶节点不为空的情况 核心步骤                stack.pop();//弹出当前节点 重新将节点按遍历顺序压栈                //后序遍历：中 右 左                stack.push(node);                stack.push(null);//中节点访问过，但是没有处理，加入空指针进行标记 if(node.right != null) stack.push(node.right);                if(node.left != null) stack.push(node.left);            }            else{//栈顶节点为空的情况                stack.pop();                node = stack.peek();                stack.pop();                result.add(node.val);            }        }        return result;    }}4、层序遍历 给你二叉树的根节点 root ，返回其节点值的 层序遍历 。 （即逐层地，从左到右访问所有节点） 树中节点数目在范围 [0, 2000] 内 -1000 &lt;= Node.val &lt;= 1000 解题思路： 本题有递归和迭代的解法，这里详细讲解迭代的思路 记得结果的返回类型是 List&lt;List&gt; 借助数据结构队列来遍历二叉树，实现广度优先遍历 获取当前层的节点个数，依次遍历每一个节点，使其加入List集合，同时让每一个节点的子节点入队代码实现：//迭代方法实现class Solution {    public List&lt;List&lt;Integer&gt;&gt; levelOrder(TreeNode root) {        //创建一个储存List列表的列表来返回结果        List&lt;List&lt;Integer&gt;&gt; result = new ArrayList&lt;List&lt;Integer&gt;&gt;();        //判断树节点为空的情况        if(root == null){            return result;        }        //创建辅助结构队列来层序遍历节点值        Queue&lt;TreeNode&gt; queue = new LinkedList&lt;TreeNode&gt;();        queue.offer(root);        //层序遍历        while(!queue.isEmpty()){            //创建一个List列表来获取每层的节点值            List&lt;Integer&gt; list = new ArrayList&lt;Integer&gt;();            int currentLevelSize = queue.size();//获取当前层的节点数            while(currentLevelSize &gt; 0){ //将每层的节点值放进List列表中，同时让下一层的节点值进入队列                TreeNode node = queue.poll();                list.add(node.val);                if(node.left != null){                    queue.offer(node.left);                }                if(node.right != null){                    queue.offer(node.right);                }                currentLevelSize--;             }            result.add(list);//将当前层的节点值放到结果结合中        }        return result;    }}//递归方法实现class Solution { //创建返回结果的list集合 public List&lt;List&lt;Integer&gt;&gt; resList = new ArrayList&lt;List&lt;Integer&gt;&gt;(); public List&lt;List&lt;Integer&gt;&gt; levelOrder(TreeNode root) { checkFun01(root,0); return resList; } //递归方法 public void checkFun01(TreeNode node, Integer deep) { //递归方法的终止条件 if (node == null) return; deep++; //层数增加 //如果list集合的元素个数小于当前层数，则为返回结果的list集合添加一个list集合 if (resList.size() &lt; deep) { //当层级增加时，list的Item也增加，利用list的索引值进行层级界定 List&lt;Integer&gt; item = new ArrayList&lt;Integer&gt;(); resList.add(item); } //将当前节点存进对应层的list集合中 resList.get(deep - 1).add(node.val); //添加左子树 checkFun01(node.left, deep); //添加右子树 checkFun01(node.right, deep); }六、二叉树的属性1、二叉树是否对称 给你一个二叉树的根节点 root ， 检查它是否轴对称 树中节点数目在范围 [1, 1000] 内 -100 &lt;= Node.val &lt;= 100 解题思路： 本题有递归和迭代的两种解法 对称二叉树&lt;==&gt;根节点的左子树和右子树呈镜像对称&lt;==&gt;二叉树与本身呈镜像对称，同时可以得出：①根节点具有相同的值 ②每个树的右子树与另一个树的左子树呈镜像对称 递归解法： 递归的返回值：boolean 参数：两个子树的根节点 TreeNode p，TreeNode q 递归的终止条件：①左右子树的当前节点有一个为空；②左右子树的当前节点都为空 递归的单层逻辑：当左右子树的当前节点都不为空，判断其值是否相等且判断它们的左右子树是否呈镜像对称 迭代解法： 借助数据结构队列进行遍历二叉树 每次入队左右子树当前节点的子节点（共四个） 每次出队两个元素，进行各种情况的判断：①都为空；②一个为空；③都不为空且数值是否相等 代码实现：//递归解法 遍历方式为后序遍历class Solution { public boolean isSymmetric(TreeNode root) { return check(root, root); } public boolean check(TreeNode p, TreeNode q) { if (p == null &amp;&amp; q == null) { return true; } if (p == null || q == null) { return false; } return p.val == q.val &amp;&amp; check(p.left, q.right) &amp;&amp; check(p.right, q.left); }}//迭代解法class Solution { public boolean isSymmetric(TreeNode root) { //借助队列 使用迭代的方式进行判断 return check(root,root); } public boolean check(TreeNode p,TreeNode q){ Queue&lt;TreeNode&gt; queue = new LinkedList&lt;TreeNode&gt;(); queue.offer(p); queue.offer(q); while(!queue.isEmpty()){ p = queue.poll(); q = queue.poll(); if(p == null &amp;&amp; q == null){ continue; } if((p == null || q == null) || (p.val != q.val)){ return false; } queue.offer(p.left); queue.offer(q.right); queue.offer(p.right); queue.offer(q.left); } return true; }2、二叉树的最大深度 给定一个二叉树，找出其最大深度。 二叉树的深度为根节点到最远叶子节点的最长路径上的节点数。 叶子节点是指没有子节点的节点解题思路： 本题有递归和迭代的两种解法 递归解法： 该递归是自底向上的解法 且根节点的高度就是二叉树的最大深度 方法的返回值 ： int ans(层数) 参数：TreeNode root(根节点) 终止条件：当前节点是否为空 为空则返回0 单层逻辑：获取当前节点的左子树高度(leftHeight)和右子树高度(rightHeight),返回结果：max(leftHeight,rightHeight)+1 迭代解法：借助数据结构队列实现二叉树的遍历，思路与二叉树的层序遍历相同，记录二叉树的深度即可代码实现：//使用递归法进行解决class Solution { public int maxDepth(TreeNode root) { if (root == null) { return 0; } else { int leftHeight = maxDepth(root.left); int rightHeight = maxDepth(root.right); return Math.max(leftHeight, rightHeight) + 1; } }}//使用迭代实现，使用数据结构队列class Solution { public int maxDepth(TreeNode root) { //使用迭代法(广度优先搜索) 求取二叉树的最大深度 if(root == null){ return 0; } Queue&lt;TreeNode&gt; queue = new LinkedList&lt;TreeNode&gt;(); queue.offer(root); int depth = 0; //创建变量用于接受最大深度 while(!queue.isEmpty()){ int size = queue.size(); //创建变量用于接受当前层的节点数 while(size &gt; 0){ TreeNode node = queue.poll(); if(node.left != null) queue.offer(node.left); if(node.right != null) queue.offer(node.right); size--; } depth++; } return depth; }}3、二叉树求路径总和 给你二叉树的根节点 root 和一个表示目标和的整数 targetSum 。判断该树中是否存在 根节点到叶子节点 的路径，这条路径上所有节点值相加等于目标和 targetSum 。如果存在，返回 true ；否则，返回 false 。 叶子节点 是指没有子节点的节点 树中节点的数目在范围 [0, 5000] 内 -1000 &lt;= Node.val &lt;= 1000 -1000 &lt;= targetSum &lt;= 1000 解题思路： 本题有递归和迭代两种解法 递归求解： 使用递归求解要具有将大问题化解为小问题的思想：假定从根节点到当前节点的路径为val，那么叶子节点的val是否为targetSum,那么每一个节点到叶子节点的路径是否等于targetSum-val 返回类型 ：boolean 参数：TreeNode root(当前节点) int targetSum(目标路径) 终止条件：①当前节点为空；②当前节点为叶子节点，判断 root.val == targetSum 单层逻辑：判断当前节点的左右子树是否包含一条路径等于targetSum 迭代求解： 创建两个数据结构队列：①一个用于存储树的节点 ②另一个用于存储树节点的值 创建一个临时变量tmep用于获取根节点到当前节点的路径之和 到达叶子节点时，判断temp == targetSum 代码实现：//递归求解：class Solution { public boolean hasPathSum(TreeNode root, int targetSum) { if(root == null){ return false; } if(root.left == null &amp;&amp; root.right == null){ return root.val == targetSum; } return hasPathSum(root.left,targetSum-root.val) || hasPathSum(root.right,targetSum-root.val); }}//迭代求解class Solution { public boolean hasPathSum(TreeNode root, int targetSum) { if(root == null){ return false; } //创建两个队列，一个用于存储树的节点，一个用于存储树节点的值 Queue&lt;TreeNode&gt; queNode = new LinkedList&lt;TreeNode&gt;(); Queue&lt;Integer&gt; queVal = new LinkedList&lt;Integer&gt;(); queNode.offer(root); queVal.offer(root.val); while(!queNode.isEmpty()){ TreeNode node = queNode.poll(); //将队列的节点依次出队 int temp = queVal.poll(); //获取根节点到当前节点的路径之和 if(node.left == null &amp;&amp; node.right == null){ if(temp == targetSum){ //进行判断 到叶子节点的值是否等于tatgetSum return true; } continue; } if(node.left != null){ queNode.offer(node.left); queVal.offer(node.left.val + temp); //将根节点到当前节点的总路径存进队列之中 } if(node.right != null){ queNode.offer(node.right); queVal.offer(node.right.val + temp); //将根节点到当前节点的总路径存进队列之中 } } return false; }}七、二叉树的修改与构造1、填充每个节点的下一个右侧指针节点指针 给定一个二叉树,填充它的每个 next 指针，让这个指针指向其下一个右侧节点。如果找不到下一个右侧节点，则将 next 指针设置为 NULL。 初始状态下，所有 next 指针都被设置为 NULL。 你只能使用常量级额外空间 使用递归解题也符合要求，本题中递归程序占用的栈空间不算做额外的空间复杂度 树中的节点数小于 6000 -100 &lt;= node.val &lt;= 100 解题思路： 本题有两种解题思路：①使用队列进行层序遍历解决；②使用已经建立的next指针 使用第①种方法解决： 区分队列的两种取元素方法：poll()方法与peek()方法 详细区别见本文第十小节 使用队列层序遍历二叉树，当当前节点还不是当前层的最后一个节点时，使其指向其右侧的元素： node.next = queue.peek(); 同时判断当前节点是否具有子节点，有的话则将其加入队列 使用第②种方法解决：（此方法只能解决完美二叉树的情况） 在完美二叉树中，存在两种情况的相邻节点：①连接相同父节点的相邻子节点；②连接不同父节点的相邻子节点 用next指针遍历至本层结束，每一层都从最左边开始遍历 代码实现：//层序遍历解决class Solution { public Node connect(Node root) { if (root == null) { return root; } // 初始化队列同时将第一层节点加入队列中，即根节点 Queue&lt;Node&gt; queue = new LinkedList&lt;Node&gt;(); queue.add(root); // 外层的 while 循环迭代的是层数 while (!queue.isEmpty()) { // 记录当前队列大小 int size = queue.size(); // 遍历这一层的所有节点 for (int i = 0; i &lt; size; i++) { // 从队首取出元素 Node node = queue.poll(); // 连接 if (i &lt; size - 1) { node.next = queue.peek(); } // 拓展下一层节点 if (node.left != null) { queue.add(node.left); } if (node.right != null) { queue.add(node.right); } } } // 返回根节点 return root; }}//使用已经建立的next指针//一颗树中存在两种类型的指针：//第一种情况是连接同一个父节点的两个子节点。它们可以通过同一个节点直接访问到，因此执行下面操作即可完成连接//第二种情况在不同父亲的子节点之间建立连接，这种情况不能直接连接class Solution { public Node connect(Node root) { if (root == null) { return root; } // 从根节点开始 Node leftmost = root; while (leftmost.left != null) { // 遍历这一层节点组织成的链表，为下一层的节点更新 next 指针 Node head = leftmost; while (head != null) { // CONNECTION 1 第一种情况 连接相同父节点的相邻子节点 head.left.next = head.right; // CONNECTION 2 第二种情况 连接不同父节点的相邻子节点 if (head.next != null) { head.right.next = head.next.left; } // 指针向后移动 head = head.next; } // 去下一层的最左的节点 leftmost = leftmost.left; } return root; }}2、根据中序和后序遍来构造二叉树 给定两个整数数组 inorder 和 postorder ，其中 inorder 是二叉树的中序遍历， postorder 是同一棵树的后序遍历，请你构造并返回这颗 二叉树 1 &lt;= inorder.length &lt;= 3000postorder.length == inorder.length-3000 &lt;= inorder[i], postorder[i] &lt;= 3000inorder 和 postorder 都由 不同 的值组成postorder 中每一个值都在 inorder 中inorder 保证是树的中序遍历postorder 保证是树的后序遍历解题思路： 本题首先要清楚中序遍历和后序遍历的特征，后序遍历数组的最后一个元素就是根节点，我们根据此来切割中序遍历数组，将其分成左子树和右子树的中序遍历数组 正由于中序遍历数组和后序遍历数组的长度是必然相同，我们可以在根据分割后的左中数组长度来分割后序遍历数组，将其分成左子树和右子树的后序遍历数组 依次不断递归处理各个区间，便将其连接起来 我们需要创建一个map集合用于存储中序遍历数组的节点值以及在数组中的对应下标，便于在各个递归区间内获取准确的根节点值的下标位置 值得注意的是在递归方法的参数传递时中，我们要保持左闭右开的原则代码实现：class Solution {    Map&lt;Integer,Integer&gt; map;    public TreeNode buildTree(int[] inorder, int[] postorder) {        //创建Map集合用于记录中序遍历数组的节点值以及在数组中的对应下标        map = new HashMap&lt;&gt;();        for(int i = 0; i &lt; inorder.length; i++){            map.put(inorder[i],i); //key为节点值,value为下标        }        //返回构造的二叉树        return findNode(inorder,0,inorder.length,postorder,0,postorder.length); //左闭右开    }    //创建递归方法用于构造二叉树    public TreeNode findNode(int[] inorder, int inBegin, int inEnd, int[] postorder, int postBegin, int postEnd){        //不满足左闭右开，说明没有元素，返回空数 即截止条件        if(inBegin &gt;= inEnd || postBegin &gt;= postEnd){            return null;        }        //在当前区间 获取后序遍历数组的最后一位节点的值        int node = postorder[postEnd-1];        //在当前区间 获取根节点在中序数组中的下标        int rootIndex = map.get(node);        //在当前区间 将中序遍历数组进行划分，获取左中数组的长度        int lenOfLeft = rootIndex - inBegin;        //构造二叉树节点        TreeNode root = new TreeNode(inorder[rootIndex]);        //由于两个数组的长度是必然相等,因此可以在根据左中数组的长度再来划分后序数组        root.left = findNode(inorder,inBegin,rootIndex,                        postorder,postBegin,postBegin+lenOfLeft); //保持左闭右开的原则        root.right = findNode(inorder,rootIndex+1,inEnd, //根节点不算入其中                        postorder,postBegin+lenOfLeft,postEnd-1);        return root;    }}3、根据前序和中序遍来构造二叉树 给定两个整数数组 preorder 和 inorder ，其中 preorder 是二叉树的先序遍历， inorder 是同一棵树的中序遍历，请构造二叉树并返回其根节点 1 &lt;= preorder.length &lt;= 3000inorder.length == preorder.length-3000 &lt;= preorder[i], inorder[i] &lt;= 3000preorder 和 inorder 均 无重复 元素inorder 均出现在 preorderpreorder 保证 为二叉树的前序遍历序列inorder 保证 为二叉树的中序遍历序列解题思路： 与根据中序和后序遍来构造二叉树思路类似，这里不过多赘述代码实现：class Solution {    Map&lt;Integer, Integer&gt; map;    public TreeNode buildTree(int[] preorder, int[] inorder) {        //创建Map集合用于记录中序遍历数组的节点值以及在数组中的对应下标        map = new HashMap&lt;&gt;();        for (int i = 0; i &lt; inorder.length; i++) { // 用map保存中序序列的数值对应位置            map.put(inorder[i], i);        }        //返回构造的二叉树        return findNode(preorder, 0, preorder.length, inorder,  0, inorder.length);  // 前闭后开    }    //创建递归方法用于构造二叉树    public TreeNode findNode(int[] preorder, int preBegin, int preEnd, int[] inorder, int inBegin, int inEnd) {        // 参数里的范围都是前闭后开        if (preBegin &gt;= preEnd || inBegin &gt;= inEnd) {  // 不满足左闭右开，说明没有元素，返回空树            return null;        }        //在当前区间 获取前序遍历数组的第一位节点的值        int node = preorder[preBegin];        //在当前区间 获取根节点在中序数组中的下标        int rootIndex = map.get(node);        //在当前区间 保存中序左子树个数，用来确定前序数列的个数        int lenOfLeft = rootIndex - inBegin;        // 构造二叉树结点        TreeNode root = new TreeNode(inorder[rootIndex]);        //由于两个数组的长度是必然相等,因此可以在根据左中数组的长度再来划分前序数组        root.left = findNode(preorder, preBegin + 1, preBegin + lenOfLeft + 1,   //保持左闭右开的原则                            inorder, inBegin, rootIndex);        root.right = findNode(preorder, preBegin + lenOfLeft + 1, preEnd,                            inorder, rootIndex + 1, inEnd);        return root;    }}八、二叉树的最近公共祖先 给定一个二叉树, 找到该树中两个指定节点的最近公共祖先。 百度百科中最近公共祖先的定义为：“对于有根树 T 的两个节点 p、q，最近公共祖先表示为一个节点 x，满足 x 是 p、q 的祖先且 x 的深度尽可能大（一个节点也可以是它自己的祖先）。” 树中节点数目在范围 [2, 105] 内。-109 &lt;= Node.val &lt;= 109所有 Node.val 互不相同 。p != qp 和 q 均存在于给定的二叉树中解题思路： 本题采用递归的方法进行实现 返回类型：boolean 参数：TreeNode root(当前节点) ， TreeNode p，TreeNode q(目标节点) 终止条件：当前节点为空 返回false； 单层逻辑： lson：判断当前节点的左子树是否包含p或q节点 dfs(root.left,p,q) rson：判断当前节点的右子树是否包含p或q节点 dfs(root.right,p,q) 两种情况可以得出当前节点为最近共同祖先：①左右子树都包含p或q节点 ②该节点本身就为p或q节点，且两个节点在同一侧 判断是否包含p或q节点 可能为：①左子树或者右子树包含 ②本身就是p节点或者q节点 代码实现：//递归解法class Solution {    private TreeNode ans; //定义结果        public Solution(){            this.ans = null; //初始化结果值        }    public TreeNode lowestCommonAncestor(TreeNode root, TreeNode p, TreeNode q) {        this.dfs(root,p,q);        return ans;    }    private boolean dfs(TreeNode root,TreeNode p,TreeNode q){        //出口条件        if (root == null) return false;        //单层逻辑        boolean lson = dfs(root.left,p,q); //左子树是否包含p或q节点        boolean rson = dfs(root.right,p,q); //右子树是否包含p或q节点        //两种情况 可以得出当前节点为最近共同祖先 ①左右子树都包含p或q节点 ②该节点本身就为p或q节点，且两个节点在同一侧        if( (lson&amp;&amp;rson) || ((root.val == p.val || root.val == q.val) &amp;&amp; (lson || rson)) ){            ans = root;        }        //判断是否包含p或q节点 可能为：①左子树或者右子树包含  ②本身就是p节点或者q节点        return lson || rson || (root.val == p.val || root.val == q.val);    }}九、二叉树的序列化与反序列化 序列化是将一个数据结构或者对象转换为连续的比特位的操作，进而可以将转换后的数据存储在一个文件或者内存中，同时也可以通过网络传输到另一个计算机环境，采取相反方式重构得到原数据。 请设计一个算法来实现二叉树的序列化与反序列化。这里不限定你的序列 / 反序列化算法执行逻辑，你只需要保证一个二叉树可以被序列化为一个字符串并且将这个字符串反序列化为原始的树结构 树中结点数在范围 [0, 104] 内 -1000 &lt;= Node.val &lt;= 1000 解题思路： 序列化：将二叉树转为字符串 反序列化：将字符串反转为二叉树 由于题目没有要求以何种方式进行序列化，我们这里采用前序遍历+递归的方式进行序列化 注意解题中需要使用到的方法： String.valueOf(a)：将a强制转换为字符串类型 String.split(‘ ‘)：使用特定的切割符将字符串分割成字符串数组 Arrays.asList( )：将数组转换为List集合 利用递归三步骤依次构造序列化和反序列化的递归方法代码实现：//前序遍历+dfs方法public class Codec { // Encodes a tree to a single string.    public String serialize(TreeNode root) {        return rserialize(root,\"\");    }    // Decodes your encoded data to tree.    public TreeNode deserialize(String data) {        String[] dataArray = data.split(\",\"); //将字符串分割为字符串数组        //将字符串数组转化为List集合        List&lt;String&gt; dataList = new LinkedList&lt;String&gt;(Arrays.asList(dataArray));         return rdeserialize(dataList);       }    //序列化递归函数     public String rserialize(TreeNode root, String str){        if(root == null){//递归的终止条件            str += \"None,\";        }        else{ //单层逻辑 前序遍历            str += str.valueOf(root.val) + \",\";            str = rserialize(root.left,str);            str = rserialize(root.right,str);        }        return str; //返回序列化后的字符串    }    //反序列化递归函数    public TreeNode rdeserialize(List&lt;String&gt; dataList){        if(dataList.get(0).equals(\"None\")){ //递归终止条件 判断当前节点是否为空            dataList.remove(0);            return null;        }        //递归单层逻辑        TreeNode root = new TreeNode(Integer.valueOf(dataList.get(0)));        dataList.remove(0);        root.left = rdeserialize(dataList);        root.right = rdeserialize(dataList);        return root;    }}十、相关知识1、递归的思想思想：将一个大问题分解成多个规模更小、具有与原来问题相同解法的问题递归求解问题需要满足的条件： 可以把要解决的问题转化为一个子问题，而这个子问题的解决方法仍与原来的解决方法相同，只是问题的规模变小了 原问题可以通过子问题的解决而组合解决 存在一种简单的情景，使问题在简单情境下退出实现递归的步骤： 确实递归方法的参数与返回类型 确定递归的终止条件 确定递归的单层逻辑2、栈的使用在二叉树问题下栈的初始化：Stack stack = new Stack&lt;&gt;();出栈： stack.pop()：获得栈顶元素并弹出 stack.peek()：获得栈顶元素但不弹出入栈： stack.push()：将元素入栈使用场景：作为辅助数据结构实现二叉树的深度优先遍历3、队列的使用在二叉树问题下队列的初始化：Queue&lt;TreeNode&gt; queue = new LinkList&lt;TreeNode&gt;();出队： queue.poll()：获得队头元素并移出队列 queue.peek()：获得队头元素但不移出队列入队： queue.offer()：往队列尾部插入元素，当超出队列界限的时候，该方法会返回false queue.add()：往队列尾部插入元素，当超出队列界限的时候，该方法会抛出异常使用场景：作为辅助数据结构实现二叉树的广度优先遍历" }, { "title": "Git&GitHub使用", "url": "/posts/Git&GitHub/", "categories": "实用工具", "tags": "学习", "date": "2022-09-09 10:19:00 +0000", "snippet": "Git&amp;GitHub使用本文将带你快速入门Git&amp;GitHubGit是一个版本控制软件，而GitHub是基于Git这一个版本控制软件的远程仓库网站。一、Git使用1、Git的历史和现状 Git 是 Linux 作者 Linus 的另一个作品。2002 年他还在使用 Bitkeeper 作为 Linux 内核的版本管理，但因为它是 Copyright 有版权的软件备受质疑，然...", "content": "Git&amp;GitHub使用本文将带你快速入门Git&amp;GitHubGit是一个版本控制软件，而GitHub是基于Git这一个版本控制软件的远程仓库网站。一、Git使用1、Git的历史和现状 Git 是 Linux 作者 Linus 的另一个作品。2002 年他还在使用 Bitkeeper 作为 Linux 内核的版本管理，但因为它是 Copyright 有版权的软件备受质疑，然后 Andrew Tridgell 对 Bitkeeper 进行逆向工程，导致 BitMover 要回收 Linux 开发者的 Bitkeeper 的免费使用权，Linus 一怒之下花了 10 天写出了 Git。 名字的意思是：egotistical bastard 如今 Git 已经成为绝大多数开发者的选择， Tom Preston-Werner、Chris Wanstrath 和 PJ Hyett 在 2007 年 10 月推出的 Github 已经成为了全球最大的开发者网站。2、集中式版本管理和分布式版本管理的区别 Git 和 SVN 是从设计理念上就不一样的版本工具，SVN 将代码进行中心化管理，拥有更好的稳定性和安全性，但是去中心化的 Git 却是从 Linux 操作系统的开发需求而来，更加适合多人协作的开源项目，可以以任何一个点为 remote 将他的代码与本地代码合并，随着时间发展，还衍生出了更多强大功能和一整套操纵流程，让它也可以适应了商业软件的开发。3、Git基础命令1.Git 按照场景可以分为以下场景 Workspace：当前工作区，修改的的最初状态。 Staging：修改后，添加到准备提交的缓存状态，即暂存区。 Local repository：本地的代码仓库，只对自己的代码生效。这也是和 svn 区别之一，svn commit 之后就直接提交到远程服务器了，git commit 之后只是到本地代码库。 Remote repository：远程代码库，将自己的本地代码库同步到远程代码库上，这样可以供别的开发者分享自己的成果；典型的远程仓库有：GitHub、Gitee….2.常用的Git命令 git add：添加到暂存区 git commit：提交到本地仓库 git push：提交到远程仓库 git fetch：将项目拉取到本地仓库 git clone：添加到本地工作区二、GitHub使用1、GitHub简介 GitHub 是一个面向开源及私有软件项目的托管平台，因为只支持 Git 作为唯一的版本库格式进行托管，故名 GitHub。GitHub 于 2008 年 4 月 10 日正式上线，除了 Git 代码仓库托管及基本的 Web 管理界面以外，还提供了订阅、讨论组、文本渲染、在线文件编辑器、协作图谱（报表）、代码片段分享（Gist）等功能。目前，在 GitHub 上托管的版本数量非常之多，其中不乏知名开源项目 Ruby on Rails、jQuery、python 等。2、GitHub的软收藏与硬收藏 硬收藏：git clone(远程拉取到文件夹) 或者 Download zip(下载成本地压缩包) 软收藏： star:收藏 fork:复制到自己的本地仓库3、GitHub基本功能板块 Pull reques：是一种通知机制。 你修改了他人的代码，将你的修改通知原来的作者，希望他合并你的修改，让项目进一步完善 Issues：“评论区”，你可以理解为在一个项目下面提出自己的想法告诉作者哪里可以改进 Explore：”发现页”，你可以在这一模块探索有趣的项目，其中Trending模块可以让你通过调整国家、编程语言、最近时间来获取最近最热门的项目4、GitHub关键词搜索 搜索前缀： awesome xxxx(技术名称) 用于搜索该技术的权威资料 搜索后缀： xxxx(技术名称) sample 用于搜索该技术的相关项目 搜索后缀： xxxx(技术名称) start 用于搜索该技术的初始化配置项目5、优秀资源推荐 HelloGitHub： 分享 GitHub 上有趣、入门级的开源项目。每月 28 号以月刊的形式更新发布，内容包括：有趣、入门级的开源项目、开源书籍、实战项目、企业级项目等，让你用很短时间感受到开源的魅力，爱上开源 地址：https://github.com/521xueweihan/HelloGitHub 阮一峰老师的科技爱好者周刊：记录每周值得分享的科技内容，周五发布。 地址：https://github.com/ruanyf/weekly" }, { "title": "数据结构--链表", "url": "/posts/List/", "categories": "数据结构与算法", "tags": "学习", "date": "2022-09-09 03:30:00 +0000", "snippet": "数据结构–链表一、链表简介 什么是链表，链表是一种通过指针串联在一起的线性结构，每一个节点由两部分组成，一个是数据域，一个是指针域（存放指向下一个节点的指针），最后一个节点的指针域指向null（空指针的意思） 链接的入口节点称为链表的头结点也就是head 链表的两种操作方法：①直接使用原来的链表进行操作 ②设置一个虚拟头结点进行操作 一般设置一个虚拟头结点来操作链表会简单许多二、类型...", "content": "数据结构–链表一、链表简介 什么是链表，链表是一种通过指针串联在一起的线性结构，每一个节点由两部分组成，一个是数据域，一个是指针域（存放指向下一个节点的指针），最后一个节点的指针域指向null（空指针的意思） 链接的入口节点称为链表的头结点也就是head 链表的两种操作方法：①直接使用原来的链表进行操作 ②设置一个虚拟头结点进行操作 一般设置一个虚拟头结点来操作链表会简单许多二、类型链表的几种类型： 单链表 双链表 循环链表(也有单双之分) 还有一些面试题会出现的特殊链表 如：环形链表、随机链表、扁平化双链表、相交链表、奇偶链表、回文链表..三、链表的存储方式 数组是在内存中是连续分布的，但是链表在内存中可不是连续分布的 链表是通过指针域的指针链接在内存中各个节点 所以链表中的节点在内存中不是连续分布的 ，而是散乱分布在内存中的某地址上，分配机制取决于操作系统的内存管理四、单链表1、单链表结构单链表是最基础的链表结构,其指针域只能指向节点的下一个节点2、单链表的设计 在单链表中实现这些功能： get(index)：获取链表中第 index 个节点的值。如果索引无效，则返回-1。 addAtHead(val)：在链表的第一个元素之前添加一个值为 val 的节点。插入后，新节点将成为链表的第一个节点。 addAtTail(val)：将值为 val 的节点追加到链表的最后一个元素。 addAtIndex(index,val)：在链表中的第 index 个节点之前添加值为 val 的节点。如果 index 等于链表的长度，则该节点将附加到链表的末尾。如果 index 大于链表长度，则不会插入节点。如果index小于0，则在头部插入节点。 deleteAtIndex(index)：如果索引 index 有效，则删除链表中的第 index 个节点。 解题思路： 单链表的定义：单链表中的节点应该具有两个属性：val 和 next。val 是当前节点的值，next 是指向下一个节点的指针/引用 链表的删除操作：将C节点的next指针指向E节点即可 即C.next = C.next.next; （关键在于要找到删除节点的前驱） 链表的添加操作：将C节点的next指针指向F节点，同时将F节点的next指针指向E节点 F.next = C.next; C.next = F; (关键在于要找到插入节点的前驱) 代码实现：//链表节点的定义class ListNode { int val; ListNode next; ListNode(){} ListNode(int val) { this.val=val; }}//链表的定义class MyLinkedList { //size存储链表元素的个数 int size; //虚拟头结点 ListNode head; //链表初始化,同时创建一个虚拟头结点 public MyLinkedList() { size = 0; head = new ListNode(0); } //获取第index个节点的数值 public int get(int index) { //如果index非法，返回-1 if (index &lt; 0 || index &gt;= size) { return -1; } ListNode currentNode = head; //包含一个虚拟头节点，所以查找第 index+1 个节点 for (int i = 0; i &lt; index+1; i++) { currentNode = currentNode.next; } return currentNode.val; } //在链表最前面插入一个节点 public void addAtHead(int val) { addAtIndex(0, val); } //在链表的最后插入一个节点 public void addAtTail(int val) { addAtIndex(size, val); } // 在第 index 个节点之前插入一个新节点，例如index为0，那么新插入的节点为链表的新头节点。 // 如果 index 等于链表的长度，则说明是新插入的节点为链表的尾结点 // 如果 index 大于链表的长度，则返回空 public void addAtIndex(int index, int val) { if (index &gt; size) {//index大于链表长度的情况 return; } if (index &lt; 0) {//index小于0的情况 index = 0; } size++; //找到要插入节点的前驱 ListNode pred = head; for (int i = 0; i &lt; index; i++) { pred = pred.next; } ListNode toAdd = new ListNode(val); toAdd.next = pred.next; pred.next = toAdd; } //删除第index个节点 public void deleteAtIndex(int index) { if (index &lt; 0 || index &gt;= size) { return; } size--; ListNode pred = head; for (int i = 0; i &lt; index; i++) { pred = pred.next; } pred.next = pred.next.next; }}五、双链表1、双链表结构双链表：每一个节点有两个指针域，一个指向下一个节点，一个指向上一个节点双链表 既可以向前查询也可以向后查询2、双链表的设计 在双链表中实现以下功能： get(index)：获取链表中第 index 个节点的值。如果索引无效，则返回-1。addAtHead(val)：在链表的第一个元素之前添加一个值为 val 的节点。插入后，新节点将成为链表的第一个节点。addAtTail(val)：将值为 val 的节点追加到链表的最后一个元素。addAtIndex(index,val)：在链表中的第 index 个节点之前添加值为 val 的节点。如果 index 等于链表的长度，则该节点将附加到链表的末尾。如果 index 大于链表长度，则不会插入节点。如果index小于0，则在头部插入节点。deleteAtIndex(index)：如果索引 index 有效，则删除链表中的第 index 个节点。解题思路： 双链表的定义：双链表中的节点应该具有三个属性：val 、next和prev。val 是当前节点的值，next 是指向下一个节点的指针/引用，prev是指向上一个节点的指针/引用 双链表的删除操作：如果我们想从双链表中删除一个现有的结点 cur，我们可以简单地将它的前一个结点 prev 与下一个结点 next 链接起来 即prev.next = cur.next next.prev = cur.prev 双链表的添加操作：如果我们想在现有的结点 prev 之后插入一个新的结点 cur，我们可以将此过程分为两个步骤： (先操作要添加的节点，在操作前后节点)代码实现：//双链表节点的定义public class ListNode{ int val; ListNode next; ListNode prev; ListNode(int x){ val = x; }}class MyLinkedList { int size; //创建虚拟头节点和虚拟尾节点 ListNode head, tail; public MyLinkedList() { size = 0; head = new ListNode(0); tail = new ListNode(0); head.next = tail; tail.prev = head; } // public int get(int index) { //判断index的值是否有效 if(index &lt; 0 || index &gt;= size){ return -1; } ListNode curr = head; if(index + 1 &lt; size - index){//判断从哪一边开始的效率高 for(int i = 0; i &lt; index +1; ++i) curr = curr.next; } else{ curr = tail; for(int i = 0; i &lt; size-index; ++i) curr = curr.prev; } return curr.val; } public void addAtHead(int val) { //创建两个指针等于虚拟头节点和头节点 ListNode pred = head; ListNode succ = head.next; ++size; ListNode toAdd = new ListNode(val); //添加操作 toAdd.prev = pred; toAdd.next = succ; pred.next = toAdd; succ.prev = toAdd; } public void addAtTail(int val) { //创建两个指针等于虚拟尾结点和尾结点 ListNode succ = tail; ListNode pred = tail.prev; ++size; //添加操作 ListNode toAdd = new ListNode(val); toAdd.prev = pred; toAdd.next = succ; pred.next = toAdd; succ.prev = toAdd; } public void addAtIndex(int index, int val) { //判断index是否有效 if(index &gt; size) return; if(index &lt; 0) index = 0; //创建两个指针用于指向index-1节点和index节点 ListNode pred,succ; if(index &lt; size - index){ pred = head; for(int i = 0; i &lt; index; ++i) pred = pred.next; succ = pred.next; } else{ succ = tail; for(int i = 0; i &lt; size-index; ++i) succ = succ.prev; pred = succ.prev; } ++size; ListNode toAdd = new ListNode(val); toAdd.prev = pred; toAdd.next = succ; pred.next = toAdd; succ.prev = toAdd; } public void deleteAtIndex(int index) { //判断index是否有效 if (index &lt; 0 || index &gt;= size) return; //创建两个指针用于指向要删除节点的前一个节点和后一个节点 ListNode pred, succ; if (index &lt; size - index) { pred = head; for(int i = 0; i &lt; index; ++i) pred = pred.next; succ = pred.next.next; } else { succ = tail; for (int i = 0; i &lt; size - index - 1; ++i) succ = succ.prev; pred = succ.prev.prev; } //删除操作 --size; pred.next = succ; succ.prev = pred; }}六、经典问题1、反转链表 给你单链表的头节点 head ，请你反转链表，并返回反转后的链表。 链表中节点的数目范围是 [0, 5000] -5000 &lt;= Node.val &lt;= 5000 解题思路： 本题可以采用迭代或者递归的方法进行实现，本文采用迭代的解法 迭代即暴力解法，核心思想就是将链表从头到依次反转 我们可以创建三个指针分别用于接受当前的节点(cur)、前一位节点(prev)、下一位节点(next)代码实现：/** * Definition for singly-linked list. * public class ListNode { * int val; * ListNode next; * ListNode() {} * ListNode(int val) { this.val = val; } * ListNode(int val, ListNode next) { this.val = val; this.next = next; } * } */class Solution { public ListNode reverseList(ListNode head) { ListNode prev = null; ListNode cur = head; while(cur != null){ ListNode temp = cur.next;//创建一个temp指针用于接受下一位节点 //反转固定操作 cur.next =prev; prev = cur; cur = temp; } return prev; }}2、判断链表是否为环形链表 给你一个链表的头节点 head ，判断链表中是否有环。 如果链表中有某个节点，可以通过连续跟踪 next 指针再次到达，则链表中存在环。 为了表示给定链表中的环，评测系统内部使用整数 pos 来表示链表尾连接到链表中的位置（索引从 0 开始）。注意：pos 不作为参数进行传递 。仅仅是为了标识链表的实际情况。 如果链表中存在环 ，则返回 true 。 否则，返回 false 。 链表中节点的数目范围是 [0, 104] -105 &lt;= Node.val &lt;= 105 pos 为 -1 或者链表中的一个 有效索引 解题思路： 本题可以采用双指针法或者哈希表法进行解决，哈希表法：使用哈希表进行存储，如果储存到相同的节点则代表为环形链表，本文采用更加灵活的双指针法进行解决 双指针法：使用快慢指针：slow、fast，slow指针每次走一步，fast指针每次走两次，如果fast指针最终追上slow指针，则代表为环形链表 记得考虑链表节点数为0或1的情况 使用双指针法必须注意快指针fast的临界条件，防止出现空指针异常代码实现：/** * Definition for singly-linked list. * class ListNode { * int val; * ListNode next; * ListNode(int x) { * val = x; * next = null; * } * } */public class Solution { public boolean hasCycle(ListNode head) { if (head == null || head.next == null) {//当链表的节点数为0或者1时，则不可能为环形链表 return false; } ListNode slow = head; ListNode fast = head.next; while (slow != fast) { if (fast == null || fast.next == null) {//因为快指针fast一次走两步，则必须判断其的下下一个节点是否为空，防止出现空指针异常 return false; } slow = slow.next; fast = fast.next.next; } return true; }}3、判断链表是否为相交链表 给你两个单链表的头节点 headA 和 headB ，请你找出并返回两个单链表相交的起始节点。如果两个链表不存在相交节点，返回 null 。 题目数据 保证 整个链式结构中不存在环。 注意，函数返回结果后，链表必须 保持其原始结构 。解题思路： 本题同样可以采用双指针法和哈希表法进行解决，哈希表法的原理即先将一条链表存进哈希表中，在遍历另一条链表查看是否有相同的节点，这里便不多赘述。本题同样采用双指针进行解决 使用双指针法：创建两个指针为pA和pB分别指向链表A和链表B的头节点，每次同时更新pA和pB,不为空的时候就指向下一个节点，pA为空时则指向链表B的头节点，pB为空时则指向链表A的头节点，若最终两个指针访问到相同的节点(a+c+b=b+c+a)则代表为相交链表。 a代表链表A从头节点到相交节点的个数，b代表链表B从头节点到相交节点的个数，c代表两条链表共同部分的节点个数 如果链表AB出现一条为空链表，则不可能为相交链表代码实现：/** * Definition for singly-linked list. * public class ListNode { * int val; * ListNode next; * ListNode(int x) { * val = x; * next = null; * } * } */public class Solution { public ListNode getIntersectionNode(ListNode headA, ListNode headB) { if(headA == null || headB == null){ return null; } ListNode pA = headA; ListNode pB = headB; while(pA != pB){ pA = pA == null? headB: pA.next; //采用三元运算符进行简化代码 pB = pB == null? headA: pB.next; } return pA; }}4、删除链表的倒数第N个节点 给你一个链表，删除链表的倒数第 n 个结点，并且返回链表的头结点 链表中结点的数目为 sz 1 &lt;= sz &lt;= 30 0 &lt;= Node.val &lt;= 100 1 &lt;= n &lt;= sz 解题思路： 本题有三种解题方法：①先获取链表的长度，在根据n求出要删除的节点；②使用栈数据结构：先进后出；③使用双指针(快慢) 本文依然采用双指针进行解决 使用快慢指针进行删除：创建两个指针为first(指向头结点)和second(指向虚拟头结点)，first指针提前向前移动n个位置，当first指针到达null时，second到达要删除的节点的前一个节点 (下文代码中first、second指针均指向虚拟头节点，那此时做什么改动才能让程序正常实现功能？) 容易混淆的两个临界条件： ①while(first.next != null)：代表走到链表的最后一个节点②while(first != null)：代表走完整个链表 代码实现：/** * Definition for singly-linked list. * public class ListNode { * int val; * ListNode next; * ListNode() {} * ListNode(int val) { this.val = val; } * ListNode(int val, ListNode next) { this.val = val; this.next = next; } * } */class Solution { public ListNode removeNthFromEnd(ListNode head, int n) { ListNode dummy = new ListNode(0, head);//创建一个虚拟头结点 ListNode first = dummy; ListNode second = dummy; for (int i = 0; i &lt; n; ++i) {//将快指针提前移动n位 first = first.next; } while (first.next != null) {//思考这里的临界条件有什么变化？ first = first.next; second = second.next; } //进行删除操作 second.next = second.next.next; ListNode ans = dummy.next; return ans; }}5、判断链表是否为回文链表 给你一个单链表的头节点 head ，请你判断该链表是否为回文链表。如果是，返回 true ；否则，返回 false 链表中节点数目在范围[1, 105] 内 0 &lt;= Node.val &lt;= 9 解题思路： 本题有两种解题思路：①将链表存进数组中进行比较，从两边依次向中间进行比较；②使用快慢指针翻转列表进行比较，但最终需要还原链表 第①种方法的步骤： 复制链表值到数组列表中 使用双指针法判断是否为回文并返回结果 第②种方法的步骤： 找到前半部分链表的尾节点 //创建一个查找尾结点的方法 反转后半部分链表 //创建一个反转链表的方法 判断是否回文 恢复链表 返回结果 本文采用第②中方法解决问题 注意：若使用第①种方法，在判断是否为回文链表的时候，要使用.equals()方法代码实现：/** * Definition for singly-linked list. * public class ListNode { * int val; * ListNode next; * ListNode() {} * ListNode(int val) { this.val = val; } * ListNode(int val, ListNode next) { this.val = val; this.next = next; } * } */class Solution { public boolean isPalindrome(ListNode head) { if (head == null) { return true; } // 找到前半部分链表的尾节点并反转后半部分链表 ListNode firstHalfEnd = endOfFirstHalf(head); ListNode secondHalfStart = reverseList(firstHalfEnd.next); // 判断是否回文 ListNode p1 = head; ListNode p2 = secondHalfStart; boolean result = true; while (result &amp;&amp; p2 != null) { if (p1.val != p2.val) { result = false; } p1 = p1.next; p2 = p2.next; } // 还原链表并返回结果 firstHalfEnd.next = reverseList(secondHalfStart); return result; } //反转链表的方法 private ListNode reverseList(ListNode head) { ListNode prev = null; ListNode curr = head; while (curr != null) { ListNode nextTemp = curr.next; curr.next = prev; prev = curr; curr = nextTemp; } return prev; } //查找第一条链表尾结点的方法 private ListNode endOfFirstHalf(ListNode head) { ListNode fast = head; ListNode slow = head; while (fast.next != null &amp;&amp; fast.next.next != null) { fast = fast.next.next; slow = slow.next; } return slow; }}6、扁平化多级双向链表 你会得到一个双链表，其中包含的节点有一个下一个指针、一个前一个指针和一个额外的 子指针 。这个子指针可能指向一个单独的双向链表，也包含这些特殊的节点。这些子列表可以有一个或多个自己的子列表，以此类推，以生成如下面的示例所示的 多层数据结构 。 给定链表的头节点 head ，将链表 扁平化 ，以便所有节点都出现在单层双链表中。让 curr 是一个带有子列表的节点。子列表中的节点应该出现在扁平化列表中的 curr 之后 和 curr.next 之前 。 返回 扁平列表的 head 。列表中的节点必须将其 所有 子指针设置为 null 。 节点数目不超过 1000 1 &lt;= Node.val &lt;= 105 解题思路： 本题要采用深度优先搜索的思想 采用递归的方法 dfs()方法：扁平化链表 若节点拥有子节点，记得将节点的子节点置为空代码实现：/*// Definition for a Node.class Node { public int val; public Node prev; public Node next; public Node child;};*/class Solution { public Node flatten(Node head) { dfs(head); return head; } //创建扁平化函数 public Node dfs(Node node){ Node cur = node; //记录链表的最后一个节点 Node last = null; //进行操作 while(cur != null){ Node next = cur.next; //如果该节点存在子节点，则优先处理子节点 if(cur.child != null){ Node childLast = dfs(cur.child); //采用递归，自己调用自己 next = cur.next; //将node(当前节点)与child(子链表连接) cur.next = cur.child; cur.child.prev = cur; //如果next不为空，就将子链表的尾结点(childLast)与next先连 if(next != null){ childLast.next = next; next.prev = childLast; } //将child置为空 cur.child = null; last = childLast; //记录当前扁平化链表的最后一位 } else{ last = cur; } cur = next; } return last; }}7、复制带有随机指针的链表 给你一个长度为 n 的链表，每个节点包含一个额外增加的随机指针 random ，该指针可以指向链表中的任何节点或空节点。 构造这个链表的 深拷贝。 深拷贝应该正好由 n 个 全新 节点组成，其中每个新节点的值都设为其对应的原节点的值。新节点的 next 指针和 random 指针也都应指向复制链表中的新节点，并使原链表和复制链表中的这些指针能够表示相同的链表状态。复制链表中的指针都不应指向原链表中的节点 。 例如，如果原链表中有 X 和 Y 两个节点，其中 X.random –&gt; Y 。那么在复制链表中对应的两个节点 x 和 y ，同样有 x.random –&gt; y 。 返回复制链表的头节点。 用一个由 n 个节点组成的链表来表示输入/输出中的链表。每个节点用一个 [val, random_index] 表示： val：一个表示 Node.val 的整数。random_index：随机指针指向的节点索引（范围从 0 到 n-1）；如果不指向任何节点，则为 null 。你的代码 只 接受原链表的头节点 head 作为传入参数。 0 &lt;= n &lt;= 1000 -104 &lt;= Node.val &lt;= 104 Node.random 为 null 或指向链表中的节点。 解题思路： 本题要采用哈希表结构和回溯的算法思想来解决 如果是普通链表，我们可以直接按照遍历的顺序创建链表节点。而本题中因为随机指针的存在，当我们拷贝节点时，「当前节点的随机指针指向的节点」可能还没创建 利用回溯的方式，创建一个哈希表用于储存已经拷贝完的节点，如果「当前节点的后继节点」和「当前节点的随机指针指向的节点」还没有被创建，则马上进行创建，并添加进哈希表中，当我们拷贝完成，回溯到当前层时，我们即可完成当前节点的指针赋值。 回溯与递归的区别：\t①递归是一种算法结构，递归会出现在子程序中，形式上表现为直接或间接的自己调用自己\t②回溯是一种算法思想，它是用递归实现的，回溯的过程类似于穷举法，但回溯有“剪枝”功能，即自我判断过程。代码实现：/*// Definition for a Node.class Node { int val; Node next; Node random; public Node(int val) { this.val = val; this.next = null; this.random = null; }}*/class Solution { //创建哈希表进行储存值 Map&lt;Node,Node&gt; cacheNode = new HashMap&lt;Node,Node&gt;(); public Node copyRandomList(Node head) { if(head == null){ return null; } if(!cacheNode.containsKey(head)){ Node headNew = new Node(head.val); cacheNode.put(head,headNew); //此处体现了回溯的思想，即可实现当链表的节点要拷贝random域时所有节点已经被创建出来了 headNew.next = copyRandomList(head.next); headNew.random = copyRandomList(head.random); } return cacheNode.get(head); }}七、链表问题的常见解题思路 双指针法 暴力迭代法 递归法 回溯思想与深度优先搜索思想" }, { "title": "我的第一篇博客！", "url": "/posts/MyFirstBlog/", "categories": "随笔", "tags": "学习", "date": "2022-09-02 13:17:00 +0000", "snippet": "我的第一篇博客！这里可以放代码片段噢～//代码片段int main(){ hello world;}", "content": "我的第一篇博客！这里可以放代码片段噢～//代码片段int main(){ hello world;}" } ]
